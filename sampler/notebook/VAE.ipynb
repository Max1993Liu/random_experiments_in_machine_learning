{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will be using satimage data as in the Borderline-SMOTE paper\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "data = fetch_openml(name='satimage')\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# treat 4 as the target class\n",
    "y = (y=='4.').astype(int)\n",
    "y.mean()\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the baseline model does without any over-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7,\n",
    "                                                    stratify=y, shuffle=True, random_state=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf):\n",
    "    pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    pred = clf.predict(X_test)\n",
    "    metric = 'AUC: {}\\nRecall: {}\\nPrecision: {}\\nF1: {}\\n'.format(roc_auc_score(y_test, pred_proba),\n",
    "                                                              recall_score(y_test, pred),\n",
    "                                                              precision_score(y_test, pred),\n",
    "                                                              f1_score(y_test, pred))\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9455910029696799\n",
      "Recall: 0.4946808510638298\n",
      "Precision: 0.8378378378378378\n",
      "F1: 0.6220735785953178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=1024)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "evaluate(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9583755972967358\n",
      "Recall: 0.6329787234042553\n",
      "Precision: 0.6432432432432432\n",
      "F1: 0.6380697050938338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "clf.fit(*SMOTE().fit_resample(X_train, y_train))\n",
    "\n",
    "evaluate(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T19:17:29.886938Z",
     "start_time": "2019-05-10T19:17:26.656978Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuzhehao\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl\\lib\\site-packages\\tqdm\\autonotebook\\__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from tensorflow import keras\n",
    "\n",
    "# the nightly build of tensorflow_probability is required as of the time of writing this \n",
    "import tensorflow_probability as tfp\n",
    "ds = tfp.distributions\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    mean_, var_ = tf.nn.moments(x, axes=[0], keepdims=True)\n",
    "    return (x - mean_) / tf.sqrt(var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURE = X_train.shape[1]\n",
    "BATCH_SIZE = 124\n",
    "\n",
    "train_dataset =tf.data.Dataset.from_tensor_slices(X_train.values.astype('float32')) \\\n",
    "    .shuffle(10000).batch(BATCH_SIZE).map(standardize)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(X_test.values.astype('float32')) \\\n",
    "    .shuffle(10000).batch(10000).map(standardize)\n",
    "\n",
    "# all positive samples\n",
    "train_pos_dataset = tf.data.Dataset.from_tensor_slices(X_train[y_train==1].values.astype('float32')) \\\n",
    "    .shuffle(10000).batch(BATCH_SIZE).map(standardize)\n",
    "test_pos_dataset = tf.data.Dataset.from_tensor_slices(X_test[y_test==1].values.astype('float32')) \\\n",
    "    .shuffle(10000).batch(100).map(standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([124, 36])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_pos_dataset))\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encoder_decoder(hidden_size=4, share_hidden=False):\n",
    "    \"\"\" If share hidden is set to False, then the actual hidden_size will be 2 * hidden_size\n",
    "        and it will split in half into the mean and std vector\n",
    "    \"\"\"\n",
    "    if share_hidden is False:\n",
    "        hidden_size = hidden_size * 2\n",
    "        \n",
    "    encoder = tf.keras.models.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(N_FEATURE,)),\n",
    "        keras.layers.Dense(36, activation=None),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(18, activation=None),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(hidden_size)\n",
    "    ])\n",
    "\n",
    "    decoder = tf.keras.models.Sequential([\n",
    "        keras.layers.Dense(18, activation=None),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(36, activation=None),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(N_FEATURE)\n",
    "    ])\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 18)                666       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 152       \n",
      "=================================================================\n",
      "Total params: 2,150\n",
      "Trainable params: 2,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# take a look at the encoder and decoders\n",
    "e_, d_ = make_encoder_decoder()\n",
    "e_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(gradients):\n",
    "    \"\"\" Handy function to check the gradients, in case we get nans from gradient explotion \"\"\"\n",
    "    grad = [i.numpy() for i in gradients]\n",
    "    if all([np.isfinite(g).all() for g in grad]):\n",
    "        avg_grad = [np.mean(g) for g in grad]\n",
    "        mean_, std_ = np.mean(avg_grad), np.std(avg_grad)\n",
    "        print('Gradients stats: mean={}, std={}'.format(mean_, std_))\n",
    "    else:\n",
    "        print('Gradient exploded: {}'.format(grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-3, hidden_size=8, share_hidden=False, recon_loss_div=10.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.share_hidden = share_hidden\n",
    "        self.encoder, self.decoder = make_encoder_decoder(hidden_size, share_hidden)\n",
    "        self.recon_loss_div = recon_loss_div\n",
    "        \n",
    "        # only used when share hidden is True\n",
    "        self.dense_mean = keras.layers.Dense(hidden_size)\n",
    "        self.dense_std = keras.layers.Dense(hidden_size)\n",
    "        \n",
    "        # use for the weighted sum of latent loss and reconstruction loss\n",
    "        self.latent_weight = tf.Variable(0.5, dtype=tf.float32, name='recon_loss_weight')\n",
    "        \n",
    "        self.learning_rate = tf.Variable(learning_rate, dtype=tf.float32, trainable=False)\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "    def reduce_learning_rate(self, factor=2):\n",
    "        self.learning_rate = self.learning_rate.assign(self.learning_rate / factor)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        if self.share_hidden:\n",
    "            mu, sigma = self.dense_mean(encoded), self.dense_std(encoded)\n",
    "        else:\n",
    "            mu, sigma = tf.split(encoded, num_or_size_splits=2, axis=1)\n",
    "        return mu, sigma, ds.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def compute_loss(self, x):\n",
    "        mu, sigma, q_z = self.encode(x)\n",
    "#         z = sigma * q_z.sample() + mu\n",
    "        z = q_z.sample()\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        # standard normal distribution\n",
    "        p_z = ds.MultivariateNormalDiag(loc=[0.] * z.shape[-1], scale_diag=[1.] * z.shape[-1])\n",
    "        kl_div = ds.kl_divergence(q_z, p_z)\n",
    "        latent_loss = tf.reduce_mean(tf.maximum(kl_div, 0))\n",
    "        recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_recon), axis=0))\n",
    "#         return latent_loss, recon_loss, recon_loss / self.recon_loss_div + latent_loss\n",
    "\n",
    "        latent_weight = tf.clip_by_value(self.latent_weight, 0.1, 0.9)\n",
    "        recon_weight = 1 - latent_weight\n",
    "        return latent_loss, recon_loss, recon_weight * recon_loss + latent_weight * latent_loss\n",
    "    \n",
    "    def compute_gradients(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            latent_loss, recon_loss, loss = self.compute_loss(x)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        return gradients\n",
    "#         grad = [g.numpy() for g in gradients]\n",
    "#         if all([np.isfinite(g).all() for g in grad]):\n",
    "#             return gradients\n",
    "#         else:\n",
    "#             print('Gradient exploded.')\n",
    "#             return None\n",
    "        \n",
    "    @tf.function\n",
    "    def train(self, train_x):\n",
    "        gradients = self.compute_gradients(train_x)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf92cbe37644fa1ab7139db2cbe3053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 17:48:21.992195  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x0000000020846818> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x0000000020846818> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.014197  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x0000000020AAB908> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x0000000020AAB908> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.036199  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x0000000020AABEA8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x0000000020AABEA8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.057201  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x0000000020AABE58> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x0000000020AABE58> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.085204  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x0000000020ADBBD8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x0000000020ADBBD8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.109207  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x0000000020ADBF98> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x0000000020ADBF98> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.124208  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x00000000208465E8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x00000000208465E8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.147210  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x0000000020E2C408> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x0000000020E2C408> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.162212  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x0000000020E2C9A8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x0000000020E2C9A8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.180214  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x0000000020E2CCC8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x0000000020E2CCC8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.193215  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0098> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0098> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.208216  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0408> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0408> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.221218  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x00000000214D08B8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x00000000214D08B8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.235219  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0A48> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0A48> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.254221  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0B38> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0B38> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.270223  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0C28> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0C28> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.283224  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0D18> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0D18> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.295225  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0E08> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0E08> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.309226  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0EA8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x00000000214D0EA8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.323228  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153B098> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153B098> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.338229  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153B3B8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153B3B8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.355231  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153B228> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153B228> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.368232  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153B5E8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153B5E8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.382234  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153B368> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153B368> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.397235  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153B638> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153B638> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.409237  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153B7C8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153B7C8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.421238  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153B778> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153B778> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.432239  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153B908> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153B908> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.445240  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153B9F8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153B9F8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.457242  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153BAE8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153BAE8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.469243  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153BBD8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153BBD8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.484244  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153BC78> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153BC78> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.497245  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153BE08> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153BE08> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.510247  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x000000002153BEF8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x000000002153BEF8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.523248  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x0000000021549098> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x0000000021549098> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.533249  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x00000000215493B8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x00000000215493B8> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.546250  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x0000000021549228> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x0000000021549228> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 17:48:22.559252  6436 tf_logging.py:161] Entity <method-wrapper '__call__' of weakref object at 0x0000000021549408> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <method-wrapper '__call__' of weakref object at 0x0000000021549408> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Object conversion is not yet supported. If you are trying to convert code that uses an existing object, try including the creation of that object in the conversion. For example, instead of converting the method of a class, try converting the entire class instead. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/README.md#using-the-functional-api for more information.\n",
      "Epoch: 0 | recon_loss: 105.60470581054688 | latent_loss: 8.829839706420898 | total_loss: 57.2172737121582\n",
      "Epoch: 1 | recon_loss: 103.01872253417969 | latent_loss: 8.631074905395508 | total_loss: 55.82489776611328\n",
      "Epoch: 2 | recon_loss: 101.33224487304688 | latent_loss: 8.343873977661133 | total_loss: 54.83805847167969\n",
      "Epoch: 3 | recon_loss: 100.78919219970703 | latent_loss: 8.165118217468262 | total_loss: 54.47715377807617\n",
      "Epoch: 4 | recon_loss: 98.81007385253906 | latent_loss: 7.827826499938965 | total_loss: 53.31895065307617\n",
      "Epoch: 5 | recon_loss: 98.27079010009766 | latent_loss: 7.681879043579102 | total_loss: 52.97633361816406\n",
      "Epoch: 6 | recon_loss: 98.13070678710938 | latent_loss: 7.736872673034668 | total_loss: 52.93378829956055\n",
      "Epoch: 7 | recon_loss: 96.70618438720703 | latent_loss: 7.65146017074585 | total_loss: 52.1788215637207\n",
      "Epoch: 8 | recon_loss: 94.1205825805664 | latent_loss: 7.6160173416137695 | total_loss: 50.86830139160156\n",
      "Epoch: 9 | recon_loss: 94.46253204345703 | latent_loss: 7.601114273071289 | total_loss: 51.031822204589844\n",
      "Epoch: 10 | recon_loss: 92.24295043945312 | latent_loss: 7.6564130783081055 | total_loss: 49.94968032836914\n",
      "Epoch: 11 | recon_loss: 90.7287368774414 | latent_loss: 7.678255081176758 | total_loss: 49.203495025634766\n",
      "Epoch: 12 | recon_loss: 85.99967193603516 | latent_loss: 7.666346549987793 | total_loss: 46.8330078125\n",
      "Epoch: 13 | recon_loss: 85.11399841308594 | latent_loss: 7.716053485870361 | total_loss: 46.4150276184082\n",
      "Epoch: 14 | recon_loss: 81.77643585205078 | latent_loss: 7.835935592651367 | total_loss: 44.80618667602539\n",
      "Epoch: 15 | recon_loss: 80.15190887451172 | latent_loss: 8.030789375305176 | total_loss: 44.09135055541992\n",
      "Epoch: 16 | recon_loss: 76.61016845703125 | latent_loss: 8.13523006439209 | total_loss: 42.37269973754883\n",
      "Epoch: 17 | recon_loss: 74.34466552734375 | latent_loss: 8.09645938873291 | total_loss: 41.22056198120117\n",
      "Epoch: 18 | recon_loss: 68.95565795898438 | latent_loss: 8.14306354522705 | total_loss: 38.54936218261719\n",
      "Epoch: 19 | recon_loss: 68.64091491699219 | latent_loss: 8.188387870788574 | total_loss: 38.414649963378906\n",
      "Epoch: 20 | recon_loss: 68.3963394165039 | latent_loss: 8.237926483154297 | total_loss: 38.31713104248047\n",
      "Epoch: 21 | recon_loss: 63.43014907836914 | latent_loss: 8.307524681091309 | total_loss: 35.86883544921875\n",
      "Epoch: 22 | recon_loss: 61.84967803955078 | latent_loss: 8.326078414916992 | total_loss: 35.0878791809082\n",
      "Epoch: 23 | recon_loss: 59.82347106933594 | latent_loss: 8.442126274108887 | total_loss: 34.13279724121094\n",
      "Epoch: 24 | recon_loss: 57.82154846191406 | latent_loss: 8.397021293640137 | total_loss: 33.109283447265625\n",
      "Epoch: 25 | recon_loss: 55.124732971191406 | latent_loss: 8.42407512664795 | total_loss: 31.774404525756836\n",
      "Epoch: 26 | recon_loss: 55.28581619262695 | latent_loss: 8.42500114440918 | total_loss: 31.85540771484375\n",
      "Epoch: 27 | recon_loss: 54.39957809448242 | latent_loss: 8.462394714355469 | total_loss: 31.430986404418945\n",
      "Epoch: 28 | recon_loss: 52.30929946899414 | latent_loss: 8.44951057434082 | total_loss: 30.379405975341797\n",
      "Epoch: 29 | recon_loss: 51.96200180053711 | latent_loss: 8.484527587890625 | total_loss: 30.223264694213867\n",
      "Epoch: 30 | recon_loss: 50.82832336425781 | latent_loss: 8.558794021606445 | total_loss: 29.693557739257812\n",
      "Epoch: 31 | recon_loss: 49.85453414916992 | latent_loss: 8.51368522644043 | total_loss: 29.18410873413086\n",
      "Epoch: 32 | recon_loss: 50.15547180175781 | latent_loss: 8.507582664489746 | total_loss: 29.331527709960938\n",
      "Epoch: 33 | recon_loss: 48.04047393798828 | latent_loss: 8.5252103805542 | total_loss: 28.2828426361084\n",
      "Epoch: 34 | recon_loss: 47.35136795043945 | latent_loss: 8.612998008728027 | total_loss: 27.9821834564209\n",
      "Epoch: 35 | recon_loss: 46.99380111694336 | latent_loss: 8.635351181030273 | total_loss: 27.8145751953125\n",
      "Epoch: 36 | recon_loss: 46.53157043457031 | latent_loss: 8.621230125427246 | total_loss: 27.576400756835938\n",
      "Epoch: 37 | recon_loss: 47.60343933105469 | latent_loss: 8.686444282531738 | total_loss: 28.144941329956055\n",
      "Epoch: 38 | recon_loss: 47.51931381225586 | latent_loss: 8.695940017700195 | total_loss: 28.107627868652344\n",
      "Epoch: 39 | recon_loss: 46.674713134765625 | latent_loss: 8.693185806274414 | total_loss: 27.683948516845703\n",
      "Epoch: 40 | recon_loss: 44.34938430786133 | latent_loss: 8.71269702911377 | total_loss: 26.53104019165039\n",
      "Epoch: 41 | recon_loss: 45.1021728515625 | latent_loss: 8.738700866699219 | total_loss: 26.92043685913086\n",
      "Epoch: 42 | recon_loss: 44.84947204589844 | latent_loss: 8.737009048461914 | total_loss: 26.79323959350586\n",
      "Epoch: 43 | recon_loss: 45.97513198852539 | latent_loss: 8.755053520202637 | total_loss: 27.365093231201172\n",
      "Epoch: 44 | recon_loss: 45.4580192565918 | latent_loss: 8.759326934814453 | total_loss: 27.108673095703125\n",
      "Epoch: 45 | recon_loss: 45.29275131225586 | latent_loss: 8.780438423156738 | total_loss: 27.03659439086914\n",
      "Epoch: 46 | recon_loss: 45.7647590637207 | latent_loss: 8.723798751831055 | total_loss: 27.244277954101562\n",
      "Epoch: 47 | recon_loss: 46.567134857177734 | latent_loss: 8.744482040405273 | total_loss: 27.655807495117188\n",
      "Epoch: 48 | recon_loss: 44.51329040527344 | latent_loss: 8.665514945983887 | total_loss: 26.58940315246582\n",
      "Epoch: 49 | recon_loss: 44.75607681274414 | latent_loss: 8.649657249450684 | total_loss: 26.70286750793457\n",
      "Epoch: 50 | recon_loss: 45.120452880859375 | latent_loss: 8.637469291687012 | total_loss: 26.87896156311035\n",
      "Epoch: 51 | recon_loss: 42.76285171508789 | latent_loss: 8.62914752960205 | total_loss: 25.695999145507812\n",
      "Epoch: 52 | recon_loss: 42.533451080322266 | latent_loss: 8.673413276672363 | total_loss: 25.603431701660156\n",
      "Epoch: 53 | recon_loss: 42.97044372558594 | latent_loss: 8.683084487915039 | total_loss: 25.826763153076172\n",
      "Epoch: 54 | recon_loss: 42.46297836303711 | latent_loss: 8.670980453491211 | total_loss: 25.566978454589844\n",
      "Epoch: 55 | recon_loss: 42.56806564331055 | latent_loss: 8.636739730834961 | total_loss: 25.602401733398438\n",
      "Epoch: 56 | recon_loss: 41.804447174072266 | latent_loss: 8.6251220703125 | total_loss: 25.214784622192383\n",
      "Epoch: 57 | recon_loss: 42.79304885864258 | latent_loss: 8.627237319946289 | total_loss: 25.71014404296875\n",
      "Epoch: 58 | recon_loss: 42.16082763671875 | latent_loss: 8.64080810546875 | total_loss: 25.40081787109375\n",
      "Epoch: 59 | recon_loss: 40.60190200805664 | latent_loss: 8.61983871459961 | total_loss: 24.610870361328125\n",
      "Epoch: 60 | recon_loss: 40.2706298828125 | latent_loss: 8.609089851379395 | total_loss: 24.43985939025879\n",
      "Epoch: 61 | recon_loss: 41.967655181884766 | latent_loss: 8.60767936706543 | total_loss: 25.28766632080078\n",
      "Epoch: 62 | recon_loss: 41.29051208496094 | latent_loss: 8.604698181152344 | total_loss: 24.94760513305664\n",
      "Epoch: 63 | recon_loss: 41.20925521850586 | latent_loss: 8.604788780212402 | total_loss: 24.90702247619629\n",
      "Epoch: 64 | recon_loss: 40.13222122192383 | latent_loss: 8.614145278930664 | total_loss: 24.373184204101562\n",
      "Epoch: 65 | recon_loss: 40.72929763793945 | latent_loss: 8.621859550476074 | total_loss: 24.675579071044922\n",
      "Epoch: 66 | recon_loss: 39.21332550048828 | latent_loss: 8.628117561340332 | total_loss: 23.92072105407715\n",
      "Epoch: 67 | recon_loss: 40.28440856933594 | latent_loss: 8.649059295654297 | total_loss: 24.466733932495117\n",
      "Epoch: 68 | recon_loss: 41.20085144042969 | latent_loss: 8.650579452514648 | total_loss: 24.925716400146484\n",
      "Epoch: 69 | recon_loss: 40.29821014404297 | latent_loss: 8.64501953125 | total_loss: 24.471614837646484\n",
      "Epoch: 70 | recon_loss: 39.11014175415039 | latent_loss: 8.646183013916016 | total_loss: 23.878162384033203\n",
      "Epoch: 71 | recon_loss: 39.53409194946289 | latent_loss: 8.635786056518555 | total_loss: 24.084938049316406\n",
      "Epoch: 72 | recon_loss: 40.465389251708984 | latent_loss: 8.634576797485352 | total_loss: 24.549983978271484\n",
      "Epoch: 73 | recon_loss: 39.2930908203125 | latent_loss: 8.647588729858398 | total_loss: 23.970340728759766\n",
      "Epoch: 74 | recon_loss: 39.26415252685547 | latent_loss: 8.66935920715332 | total_loss: 23.966754913330078\n",
      "Epoch: 75 | recon_loss: 38.783756256103516 | latent_loss: 8.640256881713867 | total_loss: 23.712005615234375\n",
      "Epoch: 76 | recon_loss: 40.81492614746094 | latent_loss: 8.648177146911621 | total_loss: 24.731552124023438\n",
      "Epoch: 77 | recon_loss: 38.18154525756836 | latent_loss: 8.65446662902832 | total_loss: 23.418006896972656\n",
      "Epoch: 78 | recon_loss: 39.93438720703125 | latent_loss: 8.654202461242676 | total_loss: 24.294294357299805\n",
      "Epoch: 79 | recon_loss: 39.79286575317383 | latent_loss: 8.703217506408691 | total_loss: 24.2480411529541\n",
      "Epoch: 80 | recon_loss: 37.932762145996094 | latent_loss: 8.680532455444336 | total_loss: 23.30664825439453\n",
      "Epoch: 81 | recon_loss: 38.09751892089844 | latent_loss: 8.672273635864258 | total_loss: 23.38489532470703\n",
      "Epoch: 82 | recon_loss: 40.17171096801758 | latent_loss: 8.705467224121094 | total_loss: 24.438589096069336\n",
      "Epoch: 83 | recon_loss: 38.76748275756836 | latent_loss: 8.683004379272461 | total_loss: 23.725242614746094\n",
      "Epoch: 84 | recon_loss: 40.59144973754883 | latent_loss: 8.65678596496582 | total_loss: 24.62411880493164\n",
      "Epoch: 85 | recon_loss: 40.57381820678711 | latent_loss: 8.637969017028809 | total_loss: 24.605894088745117\n",
      "Epoch: 86 | recon_loss: 38.241172790527344 | latent_loss: 8.635414123535156 | total_loss: 23.43829345703125\n",
      "Epoch: 87 | recon_loss: 40.06700134277344 | latent_loss: 8.621827125549316 | total_loss: 24.34441375732422\n",
      "Epoch: 88 | recon_loss: 38.086181640625 | latent_loss: 8.603492736816406 | total_loss: 23.344837188720703\n",
      "Epoch: 89 | recon_loss: 38.07883834838867 | latent_loss: 8.588942527770996 | total_loss: 23.333890914916992\n",
      "Epoch: 90 | recon_loss: 39.0715446472168 | latent_loss: 8.589010238647461 | total_loss: 23.830276489257812\n",
      "Epoch: 91 | recon_loss: 38.46809387207031 | latent_loss: 8.60545539855957 | total_loss: 23.536773681640625\n",
      "Epoch: 92 | recon_loss: 38.580406188964844 | latent_loss: 8.575992584228516 | total_loss: 23.57819938659668\n",
      "Epoch: 93 | recon_loss: 38.275081634521484 | latent_loss: 8.57095718383789 | total_loss: 23.423019409179688\n",
      "Epoch: 94 | recon_loss: 37.57438659667969 | latent_loss: 8.58141803741455 | total_loss: 23.07790184020996\n",
      "Epoch: 95 | recon_loss: 38.58467102050781 | latent_loss: 8.599088668823242 | total_loss: 23.591880798339844\n",
      "Epoch: 96 | recon_loss: 38.720542907714844 | latent_loss: 8.562544822692871 | total_loss: 23.641544342041016\n",
      "Epoch: 97 | recon_loss: 37.600982666015625 | latent_loss: 8.536382675170898 | total_loss: 23.068683624267578\n",
      "Epoch: 98 | recon_loss: 37.18985366821289 | latent_loss: 8.52381706237793 | total_loss: 22.856834411621094\n",
      "Epoch: 99 | recon_loss: 38.32734680175781 | latent_loss: 8.53342056274414 | total_loss: 23.430383682250977\n",
      "Epoch: 100 | recon_loss: 37.19289779663086 | latent_loss: 8.501291275024414 | total_loss: 22.847095489501953\n",
      "Epoch: 101 | recon_loss: 36.79875183105469 | latent_loss: 8.481603622436523 | total_loss: 22.640178680419922\n",
      "Epoch: 102 | recon_loss: 37.9397087097168 | latent_loss: 8.46892261505127 | total_loss: 23.204315185546875\n",
      "Epoch: 103 | recon_loss: 37.56644058227539 | latent_loss: 8.461163520812988 | total_loss: 23.01380157470703\n",
      "Epoch: 104 | recon_loss: 36.664459228515625 | latent_loss: 8.447017669677734 | total_loss: 22.55573844909668\n",
      "Epoch: 105 | recon_loss: 37.26185607910156 | latent_loss: 8.428272247314453 | total_loss: 22.845064163208008\n",
      "Epoch: 106 | recon_loss: 36.86104965209961 | latent_loss: 8.407966613769531 | total_loss: 22.63450813293457\n",
      "Epoch: 107 | recon_loss: 39.782371520996094 | latent_loss: 8.389213562011719 | total_loss: 24.085792541503906\n",
      "Epoch: 108 | recon_loss: 38.352256774902344 | latent_loss: 8.377326011657715 | total_loss: 23.364791870117188\n",
      "Epoch: 109 | recon_loss: 35.549598693847656 | latent_loss: 8.377657890319824 | total_loss: 21.9636287689209\n",
      "Epoch: 110 | recon_loss: 37.37740707397461 | latent_loss: 8.38222599029541 | total_loss: 22.87981605529785\n",
      "Epoch: 111 | recon_loss: 37.47370910644531 | latent_loss: 8.383040428161621 | total_loss: 22.928375244140625\n",
      "Epoch: 112 | recon_loss: 36.13518524169922 | latent_loss: 8.373861312866211 | total_loss: 22.25452423095703\n",
      "Epoch: 113 | recon_loss: 37.36879348754883 | latent_loss: 8.374417304992676 | total_loss: 22.871604919433594\n",
      "Epoch: 114 | recon_loss: 35.53970718383789 | latent_loss: 8.386276245117188 | total_loss: 21.96299171447754\n",
      "Epoch: 115 | recon_loss: 35.85197830200195 | latent_loss: 8.33144760131836 | total_loss: 22.091712951660156\n",
      "Epoch: 116 | recon_loss: 36.45577621459961 | latent_loss: 8.315276145935059 | total_loss: 22.385526657104492\n",
      "Epoch: 117 | recon_loss: 36.398643493652344 | latent_loss: 8.302607536315918 | total_loss: 22.35062599182129\n",
      "Epoch: 118 | recon_loss: 37.01419448852539 | latent_loss: 8.286441802978516 | total_loss: 22.650318145751953\n",
      "Epoch: 119 | recon_loss: 35.69215393066406 | latent_loss: 8.271313667297363 | total_loss: 21.981733322143555\n",
      "Epoch: 120 | recon_loss: 35.47111892700195 | latent_loss: 8.255961418151855 | total_loss: 21.863540649414062\n",
      "Epoch: 121 | recon_loss: 36.54766082763672 | latent_loss: 8.254539489746094 | total_loss: 22.401100158691406\n",
      "Epoch: 122 | recon_loss: 37.795841217041016 | latent_loss: 8.237524032592773 | total_loss: 23.016681671142578\n",
      "Epoch: 123 | recon_loss: 36.092899322509766 | latent_loss: 8.213047981262207 | total_loss: 22.152973175048828\n",
      "Epoch: 124 | recon_loss: 35.22779846191406 | latent_loss: 8.185649871826172 | total_loss: 21.706724166870117\n",
      "Epoch: 125 | recon_loss: 35.61333465576172 | latent_loss: 8.165111541748047 | total_loss: 21.889223098754883\n",
      "Epoch: 126 | recon_loss: 35.96726989746094 | latent_loss: 8.150022506713867 | total_loss: 22.05864715576172\n",
      "Epoch: 127 | recon_loss: 36.290557861328125 | latent_loss: 8.145096778869629 | total_loss: 22.21782684326172\n",
      "Epoch: 128 | recon_loss: 37.47517395019531 | latent_loss: 8.149888038635254 | total_loss: 22.812530517578125\n",
      "Epoch: 129 | recon_loss: 36.051029205322266 | latent_loss: 8.169923782348633 | total_loss: 22.110477447509766\n",
      "Epoch: 130 | recon_loss: 37.25831604003906 | latent_loss: 8.152295112609863 | total_loss: 22.705305099487305\n",
      "Epoch: 131 | recon_loss: 35.55376434326172 | latent_loss: 8.133922576904297 | total_loss: 21.843843460083008\n",
      "Epoch: 132 | recon_loss: 35.445465087890625 | latent_loss: 8.125564575195312 | total_loss: 21.78551483154297\n",
      "Epoch: 133 | recon_loss: 34.72622299194336 | latent_loss: 8.113661766052246 | total_loss: 21.41994285583496\n",
      "Epoch: 134 | recon_loss: 37.611637115478516 | latent_loss: 8.098230361938477 | total_loss: 22.854934692382812\n",
      "Epoch: 135 | recon_loss: 35.912227630615234 | latent_loss: 8.08517074584961 | total_loss: 21.998699188232422\n",
      "Epoch: 136 | recon_loss: 37.00897979736328 | latent_loss: 8.074618339538574 | total_loss: 22.541799545288086\n",
      "Epoch: 137 | recon_loss: 36.99349594116211 | latent_loss: 8.058860778808594 | total_loss: 22.52617835998535\n",
      "Epoch: 138 | recon_loss: 35.85318374633789 | latent_loss: 8.04423999786377 | total_loss: 21.948711395263672\n",
      "Epoch: 139 | recon_loss: 34.66343307495117 | latent_loss: 8.037123680114746 | total_loss: 21.350278854370117\n",
      "Epoch: 140 | recon_loss: 35.35307312011719 | latent_loss: 8.039204597473145 | total_loss: 21.696138381958008\n",
      "Epoch: 141 | recon_loss: 37.23610305786133 | latent_loss: 8.038673400878906 | total_loss: 22.637388229370117\n",
      "Epoch: 142 | recon_loss: 34.269569396972656 | latent_loss: 8.02869701385498 | total_loss: 21.149133682250977\n",
      "Epoch: 143 | recon_loss: 35.07168197631836 | latent_loss: 8.040633201599121 | total_loss: 21.5561580657959\n",
      "Epoch: 144 | recon_loss: 35.32166290283203 | latent_loss: 8.026090621948242 | total_loss: 21.673877716064453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 145 | recon_loss: 35.294044494628906 | latent_loss: 8.006861686706543 | total_loss: 21.650453567504883\n",
      "Epoch: 146 | recon_loss: 35.664093017578125 | latent_loss: 7.999433517456055 | total_loss: 21.831764221191406\n",
      "Epoch: 147 | recon_loss: 34.16059112548828 | latent_loss: 7.977292537689209 | total_loss: 21.068941116333008\n",
      "Epoch: 148 | recon_loss: 34.298397064208984 | latent_loss: 7.9890241622924805 | total_loss: 21.14371109008789\n",
      "Epoch: 149 | recon_loss: 33.08659362792969 | latent_loss: 7.991910934448242 | total_loss: 20.53925323486328\n",
      "Epoch: 150 | recon_loss: 36.26482391357422 | latent_loss: 8.002205848693848 | total_loss: 22.133514404296875\n",
      "Epoch: 151 | recon_loss: 36.41569519042969 | latent_loss: 7.987678050994873 | total_loss: 22.20168685913086\n",
      "Epoch: 152 | recon_loss: 35.33517837524414 | latent_loss: 7.9332194328308105 | total_loss: 21.634199142456055\n",
      "Epoch: 153 | recon_loss: 34.666709899902344 | latent_loss: 7.898845195770264 | total_loss: 21.282777786254883\n",
      "Epoch: 154 | recon_loss: 37.77396774291992 | latent_loss: 7.871810436248779 | total_loss: 22.82288932800293\n",
      "Epoch: 155 | recon_loss: 36.57973098754883 | latent_loss: 7.868708610534668 | total_loss: 22.224220275878906\n",
      "Epoch: 156 | recon_loss: 34.38197326660156 | latent_loss: 7.838693141937256 | total_loss: 21.110332489013672\n",
      "Epoch: 157 | recon_loss: 35.29203414916992 | latent_loss: 7.827608585357666 | total_loss: 21.55982208251953\n",
      "Epoch: 158 | recon_loss: 35.543338775634766 | latent_loss: 7.831634521484375 | total_loss: 21.68748664855957\n",
      "Epoch: 159 | recon_loss: 34.28245162963867 | latent_loss: 7.849884510040283 | total_loss: 21.0661678314209\n",
      "Epoch: 160 | recon_loss: 34.437828063964844 | latent_loss: 7.839479446411133 | total_loss: 21.138652801513672\n",
      "Epoch: 161 | recon_loss: 33.837074279785156 | latent_loss: 7.7967329025268555 | total_loss: 20.816904067993164\n",
      "Epoch: 162 | recon_loss: 34.02523422241211 | latent_loss: 7.764303207397461 | total_loss: 20.89476776123047\n",
      "Epoch: 163 | recon_loss: 33.63859558105469 | latent_loss: 7.737666130065918 | total_loss: 20.68813133239746\n",
      "Epoch: 164 | recon_loss: 34.4952507019043 | latent_loss: 7.7128586769104 | total_loss: 21.104055404663086\n",
      "Epoch: 165 | recon_loss: 35.84980010986328 | latent_loss: 7.687485218048096 | total_loss: 21.76864242553711\n",
      "Epoch: 166 | recon_loss: 34.23857116699219 | latent_loss: 7.658587455749512 | total_loss: 20.948579788208008\n",
      "Epoch: 167 | recon_loss: 34.72895050048828 | latent_loss: 7.643356800079346 | total_loss: 21.186153411865234\n",
      "Epoch: 168 | recon_loss: 36.80453109741211 | latent_loss: 7.638776779174805 | total_loss: 22.22165298461914\n",
      "Epoch: 169 | recon_loss: 36.288597106933594 | latent_loss: 7.635092258453369 | total_loss: 21.96184539794922\n",
      "Epoch: 170 | recon_loss: 35.210628509521484 | latent_loss: 7.628385066986084 | total_loss: 21.419506072998047\n",
      "Epoch: 171 | recon_loss: 36.96055603027344 | latent_loss: 7.603076934814453 | total_loss: 22.281816482543945\n",
      "Epoch: 172 | recon_loss: 35.76858901977539 | latent_loss: 7.595077991485596 | total_loss: 21.681833267211914\n",
      "Epoch: 173 | recon_loss: 35.3778076171875 | latent_loss: 7.598958969116211 | total_loss: 21.488384246826172\n",
      "Epoch: 174 | recon_loss: 35.15007019042969 | latent_loss: 7.58326530456543 | total_loss: 21.366668701171875\n",
      "Epoch: 175 | recon_loss: 35.34968948364258 | latent_loss: 7.558687686920166 | total_loss: 21.45418930053711\n",
      "Epoch: 176 | recon_loss: 39.06025314331055 | latent_loss: 7.501347064971924 | total_loss: 23.280799865722656\n",
      "Epoch: 177 | recon_loss: 35.21626281738281 | latent_loss: 7.452695369720459 | total_loss: 21.3344783782959\n",
      "Epoch: 178 | recon_loss: 34.14908218383789 | latent_loss: 7.420806884765625 | total_loss: 20.784944534301758\n",
      "Epoch: 179 | recon_loss: 36.12874221801758 | latent_loss: 7.411154747009277 | total_loss: 21.769948959350586\n",
      "Epoch: 180 | recon_loss: 36.43766784667969 | latent_loss: 7.372713088989258 | total_loss: 21.905189514160156\n",
      "Epoch: 181 | recon_loss: 37.85347366333008 | latent_loss: 7.3628058433532715 | total_loss: 22.608139038085938\n",
      "Epoch: 182 | recon_loss: 35.30885314941406 | latent_loss: 7.376752853393555 | total_loss: 21.342803955078125\n",
      "Epoch: 183 | recon_loss: 36.02635192871094 | latent_loss: 7.3289794921875 | total_loss: 21.67766571044922\n",
      "Epoch: 184 | recon_loss: 34.62422561645508 | latent_loss: 7.3038177490234375 | total_loss: 20.964021682739258\n",
      "Epoch: 185 | recon_loss: 37.89834213256836 | latent_loss: 7.278838634490967 | total_loss: 22.588590621948242\n",
      "Epoch: 186 | recon_loss: 37.76711654663086 | latent_loss: 7.2513532638549805 | total_loss: 22.509235382080078\n",
      "Epoch: 187 | recon_loss: 36.91920852661133 | latent_loss: 7.2247161865234375 | total_loss: 22.071962356567383\n",
      "Epoch: 188 | recon_loss: 35.20541000366211 | latent_loss: 7.2013468742370605 | total_loss: 21.203378677368164\n",
      "Epoch: 189 | recon_loss: 34.934322357177734 | latent_loss: 7.1797356605529785 | total_loss: 21.057029724121094\n",
      "Epoch: 190 | recon_loss: 36.06700134277344 | latent_loss: 7.156628608703613 | total_loss: 21.611814498901367\n",
      "Epoch: 191 | recon_loss: 37.58301544189453 | latent_loss: 7.141801834106445 | total_loss: 22.362407684326172\n",
      "Epoch: 192 | recon_loss: 37.264793395996094 | latent_loss: 7.143816947937012 | total_loss: 22.20430564880371\n",
      "Epoch: 193 | recon_loss: 35.55474090576172 | latent_loss: 7.112187385559082 | total_loss: 21.333463668823242\n",
      "Epoch: 194 | recon_loss: 37.84843444824219 | latent_loss: 7.141459941864014 | total_loss: 22.49494743347168\n",
      "Epoch: 195 | recon_loss: 35.95412826538086 | latent_loss: 7.09022331237793 | total_loss: 21.522174835205078\n",
      "Epoch: 196 | recon_loss: 38.01936340332031 | latent_loss: 7.060303211212158 | total_loss: 22.539833068847656\n",
      "Epoch: 197 | recon_loss: 36.58314895629883 | latent_loss: 7.046149253845215 | total_loss: 21.81464958190918\n",
      "Epoch: 198 | recon_loss: 36.570892333984375 | latent_loss: 7.011305332183838 | total_loss: 21.791099548339844\n",
      "Epoch: 199 | recon_loss: 36.17652893066406 | latent_loss: 6.982367515563965 | total_loss: 21.579448699951172\n",
      "Epoch: 200 | recon_loss: 35.75856399536133 | latent_loss: 6.9633893966674805 | total_loss: 21.360977172851562\n",
      "Epoch: 201 | recon_loss: 35.91457748413086 | latent_loss: 6.90650749206543 | total_loss: 21.410541534423828\n",
      "Epoch: 202 | recon_loss: 38.771793365478516 | latent_loss: 6.86550760269165 | total_loss: 22.81865119934082\n",
      "Epoch: 203 | recon_loss: 36.80570983886719 | latent_loss: 6.829792022705078 | total_loss: 21.817750930786133\n",
      "Epoch: 204 | recon_loss: 35.38382339477539 | latent_loss: 6.796135425567627 | total_loss: 21.08997917175293\n",
      "Epoch: 205 | recon_loss: 40.25831604003906 | latent_loss: 6.76683235168457 | total_loss: 23.5125732421875\n",
      "Epoch: 206 | recon_loss: 38.796268463134766 | latent_loss: 6.735945224761963 | total_loss: 22.7661075592041\n",
      "Epoch: 207 | recon_loss: 37.05573272705078 | latent_loss: 6.705150127410889 | total_loss: 21.880441665649414\n",
      "Epoch: 208 | recon_loss: 37.60541915893555 | latent_loss: 6.682987213134766 | total_loss: 22.144203186035156\n",
      "Epoch: 209 | recon_loss: 35.67106246948242 | latent_loss: 6.670027256011963 | total_loss: 21.17054557800293\n",
      "Epoch: 210 | recon_loss: 38.24811553955078 | latent_loss: 6.653941631317139 | total_loss: 22.45102882385254\n",
      "Epoch: 211 | recon_loss: 37.45985794067383 | latent_loss: 6.634812831878662 | total_loss: 22.047334671020508\n",
      "Epoch: 212 | recon_loss: 39.75953674316406 | latent_loss: 6.616212844848633 | total_loss: 23.18787384033203\n",
      "Epoch: 213 | recon_loss: 39.99760437011719 | latent_loss: 6.600011825561523 | total_loss: 23.298809051513672\n",
      "Epoch: 214 | recon_loss: 36.65135192871094 | latent_loss: 6.582746505737305 | total_loss: 21.617050170898438\n",
      "Epoch: 215 | recon_loss: 41.720027923583984 | latent_loss: 6.574714183807373 | total_loss: 24.147371292114258\n",
      "Epoch: 216 | recon_loss: 36.139991760253906 | latent_loss: 6.566083908081055 | total_loss: 21.353038787841797\n",
      "Epoch: 217 | recon_loss: 38.85414123535156 | latent_loss: 6.557249069213867 | total_loss: 22.70569610595703\n",
      "Epoch: 218 | recon_loss: 39.29888916015625 | latent_loss: 6.5483598709106445 | total_loss: 22.92362403869629\n",
      "Epoch: 219 | recon_loss: 38.553558349609375 | latent_loss: 6.5391082763671875 | total_loss: 22.54633331298828\n",
      "Epoch: 220 | recon_loss: 37.6958122253418 | latent_loss: 6.526400089263916 | total_loss: 22.111106872558594\n",
      "Epoch: 221 | recon_loss: 40.522735595703125 | latent_loss: 6.519559860229492 | total_loss: 23.521148681640625\n",
      "Epoch: 222 | recon_loss: 37.88993835449219 | latent_loss: 6.510867118835449 | total_loss: 22.200403213500977\n",
      "Epoch: 223 | recon_loss: 41.62998580932617 | latent_loss: 6.498304843902588 | total_loss: 24.064146041870117\n",
      "Epoch: 224 | recon_loss: 35.7482795715332 | latent_loss: 6.4838056564331055 | total_loss: 21.116043090820312\n",
      "Epoch: 225 | recon_loss: 37.977386474609375 | latent_loss: 6.469226837158203 | total_loss: 22.22330665588379\n",
      "Epoch: 226 | recon_loss: 37.03263854980469 | latent_loss: 6.4579548835754395 | total_loss: 21.745296478271484\n",
      "Epoch: 227 | recon_loss: 37.563411712646484 | latent_loss: 6.451108455657959 | total_loss: 22.007259368896484\n",
      "Epoch: 228 | recon_loss: 43.487510681152344 | latent_loss: 6.4437360763549805 | total_loss: 24.96562385559082\n",
      "Epoch: 229 | recon_loss: 37.56190490722656 | latent_loss: 6.44057559967041 | total_loss: 22.001239776611328\n",
      "Epoch: 230 | recon_loss: 36.43464279174805 | latent_loss: 6.442432403564453 | total_loss: 21.43853759765625\n",
      "Epoch: 231 | recon_loss: 38.49875259399414 | latent_loss: 6.459503650665283 | total_loss: 22.479127883911133\n",
      "Epoch: 232 | recon_loss: 41.064178466796875 | latent_loss: 6.467352867126465 | total_loss: 23.765766143798828\n",
      "Epoch: 233 | recon_loss: 34.875160217285156 | latent_loss: 6.459012031555176 | total_loss: 20.667085647583008\n",
      "Epoch: 234 | recon_loss: 37.48986053466797 | latent_loss: 6.445085525512695 | total_loss: 21.967472076416016\n",
      "Epoch: 235 | recon_loss: 36.01224136352539 | latent_loss: 6.434725761413574 | total_loss: 21.22348403930664\n",
      "Epoch: 236 | recon_loss: 38.82216262817383 | latent_loss: 6.416833877563477 | total_loss: 22.61949920654297\n",
      "Epoch: 237 | recon_loss: 35.75292205810547 | latent_loss: 6.397681713104248 | total_loss: 21.075302124023438\n",
      "Epoch: 238 | recon_loss: 42.307491302490234 | latent_loss: 6.380504131317139 | total_loss: 24.343997955322266\n",
      "Epoch: 239 | recon_loss: 38.21406936645508 | latent_loss: 6.369020462036133 | total_loss: 22.291545867919922\n",
      "Epoch: 240 | recon_loss: 38.94841384887695 | latent_loss: 6.371061325073242 | total_loss: 22.65973663330078\n",
      "Epoch: 241 | recon_loss: 36.31611633300781 | latent_loss: 6.370579242706299 | total_loss: 21.343347549438477\n",
      "Epoch: 242 | recon_loss: 34.30611801147461 | latent_loss: 6.363056659698486 | total_loss: 20.33458709716797\n",
      "Epoch: 243 | recon_loss: 37.26295852661133 | latent_loss: 6.361624717712402 | total_loss: 21.812292098999023\n",
      "Epoch: 244 | recon_loss: 39.46747589111328 | latent_loss: 6.372610092163086 | total_loss: 22.9200439453125\n",
      "Epoch: 245 | recon_loss: 37.447654724121094 | latent_loss: 6.398343086242676 | total_loss: 21.922998428344727\n",
      "Epoch: 246 | recon_loss: 40.16807556152344 | latent_loss: 6.390406608581543 | total_loss: 23.27924156188965\n",
      "Epoch: 247 | recon_loss: 42.342002868652344 | latent_loss: 6.346722602844238 | total_loss: 24.344362258911133\n",
      "Epoch: 248 | recon_loss: 39.9816780090332 | latent_loss: 6.332432270050049 | total_loss: 23.157054901123047\n",
      "Epoch: 249 | recon_loss: 36.83403015136719 | latent_loss: 6.315402030944824 | total_loss: 21.574716567993164\n",
      "Epoch: 250 | recon_loss: 37.22908020019531 | latent_loss: 6.297247886657715 | total_loss: 21.763164520263672\n",
      "Epoch: 251 | recon_loss: 40.526912689208984 | latent_loss: 6.294427394866943 | total_loss: 23.410669326782227\n",
      "Epoch: 252 | recon_loss: 36.365150451660156 | latent_loss: 6.308696746826172 | total_loss: 21.336923599243164\n",
      "Epoch: 253 | recon_loss: 36.14427947998047 | latent_loss: 6.293972015380859 | total_loss: 21.219125747680664\n",
      "Epoch: 254 | recon_loss: 36.702110290527344 | latent_loss: 6.289739608764648 | total_loss: 21.495925903320312\n",
      "Epoch: 255 | recon_loss: 38.67179870605469 | latent_loss: 6.284142017364502 | total_loss: 22.477970123291016\n",
      "Epoch: 256 | recon_loss: 38.911128997802734 | latent_loss: 6.276279926300049 | total_loss: 22.593704223632812\n",
      "Epoch: 257 | recon_loss: 36.23679733276367 | latent_loss: 6.271572113037109 | total_loss: 21.25418472290039\n",
      "Epoch: 258 | recon_loss: 38.04136276245117 | latent_loss: 6.2726945877075195 | total_loss: 22.157028198242188\n",
      "Epoch: 259 | recon_loss: 42.16920852661133 | latent_loss: 6.272100448608398 | total_loss: 24.220653533935547\n",
      "Epoch: 260 | recon_loss: 39.15614318847656 | latent_loss: 6.2708964347839355 | total_loss: 22.713520050048828\n",
      "Epoch: 261 | recon_loss: 35.58729934692383 | latent_loss: 6.2712626457214355 | total_loss: 20.92928123474121\n",
      "Epoch: 262 | recon_loss: 39.40998840332031 | latent_loss: 6.2809906005859375 | total_loss: 22.845489501953125\n",
      "Epoch: 263 | recon_loss: 39.20454788208008 | latent_loss: 6.274003505706787 | total_loss: 22.739274978637695\n",
      "Epoch: 264 | recon_loss: 40.14265441894531 | latent_loss: 6.253694534301758 | total_loss: 23.19817352294922\n",
      "Epoch: 265 | recon_loss: 37.119205474853516 | latent_loss: 6.248936653137207 | total_loss: 21.684070587158203\n",
      "Epoch: 266 | recon_loss: 39.61137390136719 | latent_loss: 6.249292373657227 | total_loss: 22.93033218383789\n",
      "Epoch: 267 | recon_loss: 39.47139358520508 | latent_loss: 6.246682643890381 | total_loss: 22.859037399291992\n",
      "Epoch: 268 | recon_loss: 38.36369323730469 | latent_loss: 6.2447710037231445 | total_loss: 22.304231643676758\n",
      "Epoch: 269 | recon_loss: 39.1735725402832 | latent_loss: 6.246748447418213 | total_loss: 22.710161209106445\n",
      "Epoch: 270 | recon_loss: 39.760780334472656 | latent_loss: 6.251678466796875 | total_loss: 23.006229400634766\n",
      "Epoch: 271 | recon_loss: 38.230960845947266 | latent_loss: 6.248868465423584 | total_loss: 22.239913940429688\n",
      "Epoch: 272 | recon_loss: 40.375823974609375 | latent_loss: 6.2387566566467285 | total_loss: 23.30729103088379\n",
      "Epoch: 273 | recon_loss: 39.977081298828125 | latent_loss: 6.245260238647461 | total_loss: 23.11117172241211\n",
      "Epoch: 274 | recon_loss: 38.93185043334961 | latent_loss: 6.236299991607666 | total_loss: 22.584075927734375\n",
      "Epoch: 275 | recon_loss: 37.171016693115234 | latent_loss: 6.228291034698486 | total_loss: 21.69965362548828\n",
      "Epoch: 276 | recon_loss: 40.34907531738281 | latent_loss: 6.226193428039551 | total_loss: 23.287633895874023\n",
      "Epoch: 277 | recon_loss: 40.76799011230469 | latent_loss: 6.2218098640441895 | total_loss: 23.49489974975586\n",
      "Epoch: 278 | recon_loss: 36.80137634277344 | latent_loss: 6.220131874084473 | total_loss: 21.510753631591797\n",
      "Epoch: 279 | recon_loss: 38.69758605957031 | latent_loss: 6.2189836502075195 | total_loss: 22.458284378051758\n",
      "Epoch: 280 | recon_loss: 40.16213607788086 | latent_loss: 6.214843273162842 | total_loss: 23.18848991394043\n",
      "Epoch: 281 | recon_loss: 37.146602630615234 | latent_loss: 6.205693244934082 | total_loss: 21.6761474609375\n",
      "Epoch: 282 | recon_loss: 35.60114288330078 | latent_loss: 6.207967758178711 | total_loss: 20.904556274414062\n",
      "Epoch: 283 | recon_loss: 36.33173370361328 | latent_loss: 6.207512378692627 | total_loss: 21.269622802734375\n",
      "Epoch: 284 | recon_loss: 37.92334747314453 | latent_loss: 6.2028937339782715 | total_loss: 22.063119888305664\n",
      "Epoch: 285 | recon_loss: 39.46449661254883 | latent_loss: 6.1978654861450195 | total_loss: 22.831180572509766\n",
      "Epoch: 286 | recon_loss: 39.40313720703125 | latent_loss: 6.206756591796875 | total_loss: 22.804946899414062\n",
      "Epoch: 287 | recon_loss: 34.73190689086914 | latent_loss: 6.216980457305908 | total_loss: 20.474443435668945\n",
      "Epoch: 288 | recon_loss: 40.53038024902344 | latent_loss: 6.220768928527832 | total_loss: 23.375574111938477\n",
      "Epoch: 289 | recon_loss: 38.45302200317383 | latent_loss: 6.233281135559082 | total_loss: 22.343151092529297\n",
      "Epoch: 290 | recon_loss: 37.80063247680664 | latent_loss: 6.2570481300354 | total_loss: 22.028841018676758\n",
      "Epoch: 291 | recon_loss: 35.26075744628906 | latent_loss: 6.2541399002075195 | total_loss: 20.757448196411133\n",
      "Epoch: 292 | recon_loss: 35.4152946472168 | latent_loss: 6.25564432144165 | total_loss: 20.83547019958496\n",
      "Epoch: 293 | recon_loss: 38.53126907348633 | latent_loss: 6.2328596115112305 | total_loss: 22.382064819335938\n",
      "Epoch: 294 | recon_loss: 39.27153015136719 | latent_loss: 6.227257251739502 | total_loss: 22.749393463134766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 295 | recon_loss: 39.578529357910156 | latent_loss: 6.229670524597168 | total_loss: 22.90410041809082\n",
      "Epoch: 296 | recon_loss: 38.903099060058594 | latent_loss: 6.223409652709961 | total_loss: 22.563255310058594\n",
      "Epoch: 297 | recon_loss: 36.851715087890625 | latent_loss: 6.210999965667725 | total_loss: 21.531356811523438\n",
      "Epoch: 298 | recon_loss: 36.349578857421875 | latent_loss: 6.212860584259033 | total_loss: 21.281219482421875\n",
      "Epoch: 299 | recon_loss: 39.85163879394531 | latent_loss: 6.219303607940674 | total_loss: 23.035470962524414\n",
      "Epoch: 300 | recon_loss: 37.413368225097656 | latent_loss: 6.229689121246338 | total_loss: 21.821529388427734\n",
      "Epoch: 301 | recon_loss: 38.28263854980469 | latent_loss: 6.206340312957764 | total_loss: 22.244489669799805\n",
      "Epoch: 302 | recon_loss: 38.04290008544922 | latent_loss: 6.213239669799805 | total_loss: 22.128070831298828\n",
      "Epoch: 303 | recon_loss: 38.82169723510742 | latent_loss: 6.229011058807373 | total_loss: 22.525354385375977\n",
      "Epoch: 304 | recon_loss: 38.99814224243164 | latent_loss: 6.238714218139648 | total_loss: 22.618427276611328\n",
      "Epoch: 305 | recon_loss: 38.913692474365234 | latent_loss: 6.2194061279296875 | total_loss: 22.56654930114746\n",
      "Epoch: 306 | recon_loss: 40.45323181152344 | latent_loss: 6.215685844421387 | total_loss: 23.33445930480957\n",
      "Epoch: 307 | recon_loss: 36.329166412353516 | latent_loss: 6.170894622802734 | total_loss: 21.250030517578125\n",
      "Epoch: 308 | recon_loss: 36.43289566040039 | latent_loss: 6.150738716125488 | total_loss: 21.29181671142578\n",
      "Epoch: 309 | recon_loss: 36.062625885009766 | latent_loss: 6.131873607635498 | total_loss: 21.09724998474121\n",
      "Epoch: 310 | recon_loss: 37.47648620605469 | latent_loss: 6.124600887298584 | total_loss: 21.8005428314209\n",
      "Epoch: 311 | recon_loss: 43.092620849609375 | latent_loss: 6.133080959320068 | total_loss: 24.612850189208984\n",
      "Epoch: 312 | recon_loss: 39.93359375 | latent_loss: 6.157414436340332 | total_loss: 23.045503616333008\n",
      "Epoch: 313 | recon_loss: 39.36976623535156 | latent_loss: 6.155646800994873 | total_loss: 22.762706756591797\n",
      "Epoch: 314 | recon_loss: 36.91368103027344 | latent_loss: 6.139969348907471 | total_loss: 21.526824951171875\n",
      "Epoch: 315 | recon_loss: 39.22590255737305 | latent_loss: 6.145337104797363 | total_loss: 22.685619354248047\n",
      "Epoch: 316 | recon_loss: 37.55070114135742 | latent_loss: 6.152135848999023 | total_loss: 21.851417541503906\n",
      "Epoch: 317 | recon_loss: 37.44917678833008 | latent_loss: 6.183618545532227 | total_loss: 21.81639862060547\n",
      "Epoch: 318 | recon_loss: 39.08291244506836 | latent_loss: 6.195500373840332 | total_loss: 22.639205932617188\n",
      "Epoch: 319 | recon_loss: 36.50676345825195 | latent_loss: 6.155196666717529 | total_loss: 21.33098030090332\n",
      "Epoch: 320 | recon_loss: 36.84567642211914 | latent_loss: 6.138963222503662 | total_loss: 21.492319107055664\n",
      "Epoch: 321 | recon_loss: 39.4722785949707 | latent_loss: 6.124805450439453 | total_loss: 22.798542022705078\n",
      "Epoch: 322 | recon_loss: 36.843353271484375 | latent_loss: 6.11020565032959 | total_loss: 21.47677993774414\n",
      "Epoch: 323 | recon_loss: 34.37593078613281 | latent_loss: 6.110909938812256 | total_loss: 20.243419647216797\n",
      "Epoch: 324 | recon_loss: 39.820003509521484 | latent_loss: 6.126257419586182 | total_loss: 22.97313117980957\n",
      "Epoch: 325 | recon_loss: 38.458763122558594 | latent_loss: 6.144221782684326 | total_loss: 22.30149269104004\n",
      "Epoch: 326 | recon_loss: 38.58259201049805 | latent_loss: 6.171698093414307 | total_loss: 22.377145767211914\n",
      "Epoch: 327 | recon_loss: 37.1304931640625 | latent_loss: 6.161372184753418 | total_loss: 21.645933151245117\n",
      "Epoch: 328 | recon_loss: 41.84522247314453 | latent_loss: 6.163809299468994 | total_loss: 24.0045166015625\n",
      "Epoch: 329 | recon_loss: 41.23097229003906 | latent_loss: 6.1951799392700195 | total_loss: 23.713075637817383\n",
      "Epoch: 330 | recon_loss: 36.32094955444336 | latent_loss: 6.142503261566162 | total_loss: 21.231725692749023\n",
      "Epoch: 331 | recon_loss: 39.8380126953125 | latent_loss: 6.112729549407959 | total_loss: 22.975370407104492\n",
      "Epoch: 332 | recon_loss: 37.83842468261719 | latent_loss: 6.090104579925537 | total_loss: 21.964263916015625\n",
      "Epoch: 333 | recon_loss: 36.69107437133789 | latent_loss: 6.0742573738098145 | total_loss: 21.382665634155273\n",
      "Epoch: 334 | recon_loss: 38.85000991821289 | latent_loss: 6.06374979019165 | total_loss: 22.456880569458008\n",
      "Epoch: 335 | recon_loss: 38.49177169799805 | latent_loss: 6.073564052581787 | total_loss: 22.28266716003418\n",
      "Epoch: 336 | recon_loss: 40.25279235839844 | latent_loss: 6.101093292236328 | total_loss: 23.176942825317383\n",
      "Epoch: 337 | recon_loss: 36.47898483276367 | latent_loss: 6.137875556945801 | total_loss: 21.308429718017578\n",
      "Epoch: 338 | recon_loss: 37.5488395690918 | latent_loss: 6.208375930786133 | total_loss: 21.87860870361328\n",
      "Epoch: 339 | recon_loss: 37.325069427490234 | latent_loss: 6.209261417388916 | total_loss: 21.767166137695312\n",
      "Epoch: 340 | recon_loss: 39.530723571777344 | latent_loss: 6.189416885375977 | total_loss: 22.860069274902344\n",
      "Epoch: 341 | recon_loss: 36.897945404052734 | latent_loss: 6.195722579956055 | total_loss: 21.546833038330078\n",
      "Epoch: 342 | recon_loss: 36.12403106689453 | latent_loss: 6.238479137420654 | total_loss: 21.181255340576172\n",
      "Epoch: 343 | recon_loss: 36.28959655761719 | latent_loss: 6.239652633666992 | total_loss: 21.264625549316406\n",
      "Epoch: 344 | recon_loss: 40.07151794433594 | latent_loss: 6.172297477722168 | total_loss: 23.12190818786621\n",
      "Epoch: 345 | recon_loss: 39.53129196166992 | latent_loss: 6.1449480056762695 | total_loss: 22.838119506835938\n",
      "Epoch: 346 | recon_loss: 38.90809631347656 | latent_loss: 6.135980606079102 | total_loss: 22.522037506103516\n",
      "Epoch: 347 | recon_loss: 39.28562545776367 | latent_loss: 6.100728988647461 | total_loss: 22.69317626953125\n",
      "Epoch: 348 | recon_loss: 36.01063919067383 | latent_loss: 6.077033042907715 | total_loss: 21.04383659362793\n",
      "Epoch: 349 | recon_loss: 37.89952087402344 | latent_loss: 6.069007396697998 | total_loss: 21.984264373779297\n",
      "Epoch: 350 | recon_loss: 36.29302978515625 | latent_loss: 6.036725044250488 | total_loss: 21.16487693786621\n",
      "Epoch: 351 | recon_loss: 37.946571350097656 | latent_loss: 6.0512166023254395 | total_loss: 21.99889373779297\n",
      "Epoch: 352 | recon_loss: 38.49381637573242 | latent_loss: 6.065303802490234 | total_loss: 22.279560089111328\n",
      "Epoch: 353 | recon_loss: 37.96708679199219 | latent_loss: 6.0632781982421875 | total_loss: 22.015182495117188\n",
      "Epoch: 354 | recon_loss: 39.66189956665039 | latent_loss: 6.069880485534668 | total_loss: 22.865890502929688\n",
      "Epoch: 355 | recon_loss: 38.11561965942383 | latent_loss: 6.067540168762207 | total_loss: 22.09157943725586\n",
      "Epoch: 356 | recon_loss: 40.49982833862305 | latent_loss: 6.0511932373046875 | total_loss: 23.275510787963867\n",
      "Epoch: 357 | recon_loss: 35.50978469848633 | latent_loss: 6.053325176239014 | total_loss: 20.78155517578125\n",
      "Epoch: 358 | recon_loss: 37.89059066772461 | latent_loss: 6.104428768157959 | total_loss: 21.997509002685547\n",
      "Epoch: 359 | recon_loss: 37.72951889038086 | latent_loss: 6.058004856109619 | total_loss: 21.893762588500977\n",
      "Epoch: 360 | recon_loss: 40.930389404296875 | latent_loss: 6.043238639831543 | total_loss: 23.486814498901367\n",
      "Epoch: 361 | recon_loss: 37.99380111694336 | latent_loss: 6.0276312828063965 | total_loss: 22.01071548461914\n",
      "Epoch: 362 | recon_loss: 33.78569030761719 | latent_loss: 6.019395351409912 | total_loss: 19.902542114257812\n",
      "Epoch: 363 | recon_loss: 39.092628479003906 | latent_loss: 6.016067028045654 | total_loss: 22.55434799194336\n",
      "Epoch: 364 | recon_loss: 36.68073272705078 | latent_loss: 6.0127410888671875 | total_loss: 21.346736907958984\n",
      "Epoch: 365 | recon_loss: 35.69549560546875 | latent_loss: 6.00297737121582 | total_loss: 20.84923553466797\n",
      "Epoch: 366 | recon_loss: 38.106571197509766 | latent_loss: 6.006937026977539 | total_loss: 22.05675506591797\n",
      "Epoch: 367 | recon_loss: 39.862117767333984 | latent_loss: 6.01806640625 | total_loss: 22.940092086791992\n",
      "Epoch: 368 | recon_loss: 35.77351379394531 | latent_loss: 6.036802291870117 | total_loss: 20.90515899658203\n",
      "Epoch: 369 | recon_loss: 40.69962692260742 | latent_loss: 6.019753456115723 | total_loss: 23.359689712524414\n",
      "Epoch: 370 | recon_loss: 41.881492614746094 | latent_loss: 6.022027015686035 | total_loss: 23.951759338378906\n",
      "Epoch: 371 | recon_loss: 39.21207046508789 | latent_loss: 6.012226581573486 | total_loss: 22.61214828491211\n",
      "Epoch: 372 | recon_loss: 36.7193603515625 | latent_loss: 5.990945339202881 | total_loss: 21.355152130126953\n",
      "Epoch: 373 | recon_loss: 34.01023864746094 | latent_loss: 5.995781421661377 | total_loss: 20.003009796142578\n",
      "Epoch: 374 | recon_loss: 38.897239685058594 | latent_loss: 6.0038018226623535 | total_loss: 22.45052146911621\n",
      "Epoch: 375 | recon_loss: 38.46636199951172 | latent_loss: 6.018303394317627 | total_loss: 22.242332458496094\n",
      "Epoch: 376 | recon_loss: 38.58576965332031 | latent_loss: 6.017902851104736 | total_loss: 22.301836013793945\n",
      "Epoch: 377 | recon_loss: 38.14559555053711 | latent_loss: 6.00503396987915 | total_loss: 22.075315475463867\n",
      "Epoch: 378 | recon_loss: 37.13963317871094 | latent_loss: 6.017335414886475 | total_loss: 21.57848358154297\n",
      "Epoch: 379 | recon_loss: 38.21958923339844 | latent_loss: 6.031782627105713 | total_loss: 22.125686645507812\n",
      "Epoch: 380 | recon_loss: 35.81180953979492 | latent_loss: 6.049678325653076 | total_loss: 20.930744171142578\n",
      "Epoch: 381 | recon_loss: 35.05145263671875 | latent_loss: 6.078467845916748 | total_loss: 20.564960479736328\n",
      "Epoch: 382 | recon_loss: 35.89974594116211 | latent_loss: 6.066473960876465 | total_loss: 20.983110427856445\n",
      "Epoch: 383 | recon_loss: 34.99197006225586 | latent_loss: 6.0762457847595215 | total_loss: 20.534107208251953\n",
      "Epoch: 384 | recon_loss: 36.595096588134766 | latent_loss: 6.0555596351623535 | total_loss: 21.325328826904297\n",
      "Epoch: 385 | recon_loss: 36.18606948852539 | latent_loss: 6.054287910461426 | total_loss: 21.12017822265625\n",
      "Epoch: 386 | recon_loss: 38.65576171875 | latent_loss: 6.104249477386475 | total_loss: 22.3800048828125\n",
      "Epoch: 387 | recon_loss: 38.72461700439453 | latent_loss: 6.07548713684082 | total_loss: 22.40005111694336\n",
      "Epoch: 388 | recon_loss: 37.40093994140625 | latent_loss: 6.060875415802002 | total_loss: 21.730907440185547\n",
      "Epoch: 389 | recon_loss: 37.59701156616211 | latent_loss: 6.0458083152771 | total_loss: 21.821409225463867\n",
      "Epoch: 390 | recon_loss: 36.83580780029297 | latent_loss: 6.029847621917725 | total_loss: 21.43282699584961\n",
      "Epoch: 391 | recon_loss: 38.95292663574219 | latent_loss: 6.010161876678467 | total_loss: 22.481544494628906\n",
      "Epoch: 392 | recon_loss: 35.950714111328125 | latent_loss: 6.007234573364258 | total_loss: 20.978973388671875\n",
      "Epoch: 393 | recon_loss: 36.837120056152344 | latent_loss: 6.016362190246582 | total_loss: 21.426740646362305\n",
      "Epoch: 394 | recon_loss: 37.65446090698242 | latent_loss: 6.033928394317627 | total_loss: 21.844194412231445\n",
      "Epoch: 395 | recon_loss: 38.80818176269531 | latent_loss: 6.040493011474609 | total_loss: 22.42433738708496\n",
      "Epoch: 396 | recon_loss: 38.13496780395508 | latent_loss: 6.031121730804443 | total_loss: 22.083044052124023\n",
      "Epoch: 397 | recon_loss: 38.80250930786133 | latent_loss: 6.038044452667236 | total_loss: 22.420276641845703\n",
      "Epoch: 398 | recon_loss: 35.51346969604492 | latent_loss: 6.071186542510986 | total_loss: 20.792327880859375\n",
      "The learning rate now is: <tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=0.0005>\n",
      "Epoch: 399 | recon_loss: 36.12701416015625 | latent_loss: 6.064724922180176 | total_loss: 21.095869064331055\n",
      "Epoch: 400 | recon_loss: 39.18571472167969 | latent_loss: 6.039542198181152 | total_loss: 22.612628936767578\n",
      "Epoch: 401 | recon_loss: 37.76719665527344 | latent_loss: 6.016104698181152 | total_loss: 21.891651153564453\n",
      "Epoch: 402 | recon_loss: 39.2772331237793 | latent_loss: 6.024747371673584 | total_loss: 22.650989532470703\n",
      "Epoch: 403 | recon_loss: 39.647193908691406 | latent_loss: 5.978794097900391 | total_loss: 22.8129940032959\n",
      "Epoch: 404 | recon_loss: 38.77186965942383 | latent_loss: 5.9747724533081055 | total_loss: 22.373321533203125\n",
      "Epoch: 405 | recon_loss: 38.31928253173828 | latent_loss: 6.007415294647217 | total_loss: 22.163349151611328\n",
      "Epoch: 406 | recon_loss: 39.16817855834961 | latent_loss: 6.008386611938477 | total_loss: 22.58828353881836\n",
      "Epoch: 407 | recon_loss: 41.19169235229492 | latent_loss: 5.9647603034973145 | total_loss: 23.57822608947754\n",
      "Epoch: 408 | recon_loss: 41.89569854736328 | latent_loss: 5.963566303253174 | total_loss: 23.92963218688965\n",
      "Epoch: 409 | recon_loss: 39.12391662597656 | latent_loss: 5.985590934753418 | total_loss: 22.55475425720215\n",
      "Epoch: 410 | recon_loss: 40.08470916748047 | latent_loss: 5.9740071296691895 | total_loss: 23.02935791015625\n",
      "Epoch: 411 | recon_loss: 37.45899963378906 | latent_loss: 5.963324546813965 | total_loss: 21.711162567138672\n",
      "Epoch: 412 | recon_loss: 39.85020446777344 | latent_loss: 5.942407608032227 | total_loss: 22.896305084228516\n",
      "Epoch: 413 | recon_loss: 38.42062759399414 | latent_loss: 5.9315104484558105 | total_loss: 22.176069259643555\n",
      "Epoch: 414 | recon_loss: 36.68661880493164 | latent_loss: 5.926125526428223 | total_loss: 21.306371688842773\n",
      "Epoch: 415 | recon_loss: 35.9482536315918 | latent_loss: 5.92922830581665 | total_loss: 20.93874168395996\n",
      "Epoch: 416 | recon_loss: 37.81317901611328 | latent_loss: 5.937647819519043 | total_loss: 21.87541389465332\n",
      "Epoch: 417 | recon_loss: 40.98591613769531 | latent_loss: 5.939456939697266 | total_loss: 23.46268653869629\n",
      "Epoch: 418 | recon_loss: 37.23204803466797 | latent_loss: 5.954755783081055 | total_loss: 21.593402862548828\n",
      "Epoch: 419 | recon_loss: 36.56429672241211 | latent_loss: 5.970983505249023 | total_loss: 21.26763916015625\n",
      "Epoch: 420 | recon_loss: 38.35292053222656 | latent_loss: 5.984013080596924 | total_loss: 22.168466567993164\n",
      "Epoch: 421 | recon_loss: 38.286930084228516 | latent_loss: 5.966713905334473 | total_loss: 22.126821517944336\n",
      "Epoch: 422 | recon_loss: 36.14553451538086 | latent_loss: 5.958680629730225 | total_loss: 21.052106857299805\n",
      "Epoch: 423 | recon_loss: 35.93354034423828 | latent_loss: 5.948217868804932 | total_loss: 20.940879821777344\n",
      "Epoch: 424 | recon_loss: 37.93405532836914 | latent_loss: 5.923604965209961 | total_loss: 21.928829193115234\n",
      "Epoch: 425 | recon_loss: 37.91461944580078 | latent_loss: 5.902988433837891 | total_loss: 21.908803939819336\n",
      "Epoch: 426 | recon_loss: 36.20268249511719 | latent_loss: 5.930079936981201 | total_loss: 21.066381454467773\n",
      "Epoch: 427 | recon_loss: 40.007877349853516 | latent_loss: 5.909788608551025 | total_loss: 22.958833694458008\n",
      "Epoch: 428 | recon_loss: 39.778228759765625 | latent_loss: 5.9436211585998535 | total_loss: 22.860925674438477\n",
      "Epoch: 429 | recon_loss: 37.28522872924805 | latent_loss: 5.899012565612793 | total_loss: 21.592121124267578\n",
      "Epoch: 430 | recon_loss: 40.04186248779297 | latent_loss: 5.920959949493408 | total_loss: 22.98141098022461\n",
      "Epoch: 431 | recon_loss: 39.49606704711914 | latent_loss: 5.908510684967041 | total_loss: 22.702289581298828\n",
      "Epoch: 432 | recon_loss: 37.243194580078125 | latent_loss: 5.95671272277832 | total_loss: 21.599952697753906\n",
      "Epoch: 433 | recon_loss: 36.375152587890625 | latent_loss: 5.999982833862305 | total_loss: 21.18756866455078\n",
      "Epoch: 434 | recon_loss: 37.163795471191406 | latent_loss: 5.936460494995117 | total_loss: 21.550128936767578\n",
      "Epoch: 435 | recon_loss: 39.55021286010742 | latent_loss: 5.952755928039551 | total_loss: 22.751483917236328\n",
      "Epoch: 436 | recon_loss: 39.07701110839844 | latent_loss: 5.970693588256836 | total_loss: 22.523853302001953\n",
      "Epoch: 437 | recon_loss: 35.6076774597168 | latent_loss: 5.969879150390625 | total_loss: 20.78877830505371\n",
      "Epoch: 438 | recon_loss: 37.88298034667969 | latent_loss: 5.93958854675293 | total_loss: 21.911285400390625\n",
      "Epoch: 439 | recon_loss: 37.354244232177734 | latent_loss: 5.941474437713623 | total_loss: 21.647859573364258\n",
      "Epoch: 440 | recon_loss: 35.678680419921875 | latent_loss: 5.943088531494141 | total_loss: 20.810884475708008\n",
      "Epoch: 441 | recon_loss: 38.57060241699219 | latent_loss: 5.93687629699707 | total_loss: 22.253738403320312\n",
      "Epoch: 442 | recon_loss: 39.07463073730469 | latent_loss: 5.928719997406006 | total_loss: 22.50167465209961\n",
      "Epoch: 443 | recon_loss: 38.85153579711914 | latent_loss: 5.935839653015137 | total_loss: 22.393688201904297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 444 | recon_loss: 39.68938446044922 | latent_loss: 5.9572858810424805 | total_loss: 22.823335647583008\n",
      "Epoch: 445 | recon_loss: 38.43242263793945 | latent_loss: 5.971928596496582 | total_loss: 22.20217514038086\n",
      "Epoch: 446 | recon_loss: 37.559326171875 | latent_loss: 5.9778876304626465 | total_loss: 21.768606185913086\n",
      "Epoch: 447 | recon_loss: 38.26061248779297 | latent_loss: 5.992732524871826 | total_loss: 22.126672744750977\n",
      "Epoch: 448 | recon_loss: 35.59843444824219 | latent_loss: 5.955319404602051 | total_loss: 20.77687644958496\n",
      "Epoch: 449 | recon_loss: 37.77458953857422 | latent_loss: 5.932182788848877 | total_loss: 21.85338592529297\n",
      "Epoch: 450 | recon_loss: 34.874046325683594 | latent_loss: 5.8902668952941895 | total_loss: 20.382156372070312\n",
      "Epoch: 451 | recon_loss: 41.193233489990234 | latent_loss: 5.870772838592529 | total_loss: 23.53200340270996\n",
      "Epoch: 452 | recon_loss: 39.09104537963867 | latent_loss: 5.866164684295654 | total_loss: 22.478605270385742\n",
      "Epoch: 453 | recon_loss: 36.72886657714844 | latent_loss: 5.875033378601074 | total_loss: 21.301950454711914\n",
      "Epoch: 454 | recon_loss: 37.669368743896484 | latent_loss: 5.891589164733887 | total_loss: 21.780479431152344\n",
      "Epoch: 455 | recon_loss: 38.874305725097656 | latent_loss: 5.859444618225098 | total_loss: 22.36687469482422\n",
      "Epoch: 456 | recon_loss: 38.724853515625 | latent_loss: 5.841720104217529 | total_loss: 22.283287048339844\n",
      "Epoch: 457 | recon_loss: 36.79930114746094 | latent_loss: 5.866412162780762 | total_loss: 21.332857131958008\n",
      "Epoch: 458 | recon_loss: 38.056732177734375 | latent_loss: 5.848138809204102 | total_loss: 21.952434539794922\n",
      "Epoch: 459 | recon_loss: 38.14146041870117 | latent_loss: 5.855923652648926 | total_loss: 21.99869155883789\n",
      "Epoch: 460 | recon_loss: 34.56220245361328 | latent_loss: 5.896972179412842 | total_loss: 20.22958755493164\n",
      "Epoch: 461 | recon_loss: 40.52663040161133 | latent_loss: 5.896784782409668 | total_loss: 23.211708068847656\n",
      "Epoch: 462 | recon_loss: 37.831329345703125 | latent_loss: 5.915076732635498 | total_loss: 21.87320327758789\n",
      "Epoch: 463 | recon_loss: 40.85502243041992 | latent_loss: 5.891940116882324 | total_loss: 23.37348175048828\n",
      "Epoch: 464 | recon_loss: 37.000335693359375 | latent_loss: 5.901096820831299 | total_loss: 21.450716018676758\n",
      "Epoch: 465 | recon_loss: 36.00047302246094 | latent_loss: 5.948085784912109 | total_loss: 20.974279403686523\n",
      "Epoch: 466 | recon_loss: 34.457847595214844 | latent_loss: 5.904290199279785 | total_loss: 20.181068420410156\n",
      "Epoch: 467 | recon_loss: 35.157508850097656 | latent_loss: 5.873628616333008 | total_loss: 20.515567779541016\n",
      "Epoch: 468 | recon_loss: 36.3433952331543 | latent_loss: 5.851468563079834 | total_loss: 21.097431182861328\n",
      "Epoch: 469 | recon_loss: 35.927181243896484 | latent_loss: 5.87101936340332 | total_loss: 20.89910125732422\n",
      "Epoch: 470 | recon_loss: 37.80143356323242 | latent_loss: 5.892650604248047 | total_loss: 21.847042083740234\n",
      "Epoch: 471 | recon_loss: 39.18967819213867 | latent_loss: 5.923605918884277 | total_loss: 22.556642532348633\n",
      "Epoch: 472 | recon_loss: 36.99454116821289 | latent_loss: 5.9814887046813965 | total_loss: 21.488014221191406\n",
      "Epoch: 473 | recon_loss: 36.529605865478516 | latent_loss: 5.980128765106201 | total_loss: 21.254867553710938\n",
      "Epoch: 474 | recon_loss: 41.009281158447266 | latent_loss: 5.9557414054870605 | total_loss: 23.482511520385742\n",
      "Epoch: 475 | recon_loss: 37.70049285888672 | latent_loss: 5.944520473480225 | total_loss: 21.822505950927734\n",
      "Epoch: 476 | recon_loss: 36.42238998413086 | latent_loss: 5.954166889190674 | total_loss: 21.188278198242188\n",
      "Epoch: 477 | recon_loss: 34.91331481933594 | latent_loss: 5.929947376251221 | total_loss: 20.421630859375\n",
      "Epoch: 478 | recon_loss: 39.45478820800781 | latent_loss: 5.93023681640625 | total_loss: 22.69251251220703\n",
      "Epoch: 479 | recon_loss: 38.39972686767578 | latent_loss: 5.910701274871826 | total_loss: 22.155214309692383\n",
      "Epoch: 480 | recon_loss: 37.289493560791016 | latent_loss: 5.923348903656006 | total_loss: 21.606420516967773\n",
      "Epoch: 481 | recon_loss: 36.98353958129883 | latent_loss: 5.929593086242676 | total_loss: 21.456565856933594\n",
      "Epoch: 482 | recon_loss: 37.14259719848633 | latent_loss: 5.936769485473633 | total_loss: 21.539684295654297\n",
      "Epoch: 483 | recon_loss: 37.18444061279297 | latent_loss: 5.972695827484131 | total_loss: 21.578567504882812\n",
      "Epoch: 484 | recon_loss: 35.258792877197266 | latent_loss: 5.9411115646362305 | total_loss: 20.599952697753906\n",
      "Epoch: 485 | recon_loss: 38.427452087402344 | latent_loss: 5.906243324279785 | total_loss: 22.166847229003906\n",
      "Epoch: 486 | recon_loss: 35.46523666381836 | latent_loss: 5.898754119873047 | total_loss: 20.681995391845703\n",
      "Epoch: 487 | recon_loss: 34.81660461425781 | latent_loss: 5.905439853668213 | total_loss: 20.36102294921875\n",
      "Epoch: 488 | recon_loss: 33.77378845214844 | latent_loss: 5.998748779296875 | total_loss: 19.886268615722656\n",
      "Epoch: 489 | recon_loss: 39.62722396850586 | latent_loss: 5.9174909591674805 | total_loss: 22.772357940673828\n",
      "Epoch: 490 | recon_loss: 39.75556945800781 | latent_loss: 5.881694316864014 | total_loss: 22.818632125854492\n",
      "Epoch: 491 | recon_loss: 34.524871826171875 | latent_loss: 5.886220932006836 | total_loss: 20.205547332763672\n",
      "Epoch: 492 | recon_loss: 36.59791946411133 | latent_loss: 5.884665489196777 | total_loss: 21.24129295349121\n",
      "Epoch: 493 | recon_loss: 35.10869216918945 | latent_loss: 5.870145797729492 | total_loss: 20.489418029785156\n",
      "Epoch: 494 | recon_loss: 39.80848693847656 | latent_loss: 5.8379058837890625 | total_loss: 22.823196411132812\n",
      "Epoch: 495 | recon_loss: 40.13365173339844 | latent_loss: 5.824530124664307 | total_loss: 22.97909164428711\n",
      "Epoch: 496 | recon_loss: 37.68655776977539 | latent_loss: 5.822699546813965 | total_loss: 21.754629135131836\n",
      "Epoch: 497 | recon_loss: 35.69894790649414 | latent_loss: 5.816773891448975 | total_loss: 20.75786018371582\n",
      "Epoch: 498 | recon_loss: 34.43220901489258 | latent_loss: 5.8089165687561035 | total_loss: 20.120563507080078\n",
      "Epoch: 499 | recon_loss: 35.363861083984375 | latent_loss: 5.792262554168701 | total_loss: 20.578062057495117\n",
      "Epoch: 500 | recon_loss: 37.2137451171875 | latent_loss: 5.768518447875977 | total_loss: 21.491130828857422\n",
      "Epoch: 501 | recon_loss: 39.38874816894531 | latent_loss: 5.735931396484375 | total_loss: 22.562339782714844\n",
      "Epoch: 502 | recon_loss: 39.851097106933594 | latent_loss: 5.741337299346924 | total_loss: 22.79621696472168\n",
      "Epoch: 503 | recon_loss: 38.2491340637207 | latent_loss: 5.7463555335998535 | total_loss: 21.997745513916016\n",
      "Epoch: 504 | recon_loss: 35.77669906616211 | latent_loss: 5.717479228973389 | total_loss: 20.747089385986328\n",
      "Epoch: 505 | recon_loss: 36.20686340332031 | latent_loss: 5.716536045074463 | total_loss: 20.961700439453125\n",
      "Epoch: 506 | recon_loss: 35.28011703491211 | latent_loss: 5.760525703430176 | total_loss: 20.520320892333984\n",
      "Epoch: 507 | recon_loss: 37.676822662353516 | latent_loss: 5.813777923583984 | total_loss: 21.74530029296875\n",
      "Epoch: 508 | recon_loss: 34.46650695800781 | latent_loss: 5.843371391296387 | total_loss: 20.154939651489258\n",
      "Epoch: 509 | recon_loss: 37.774044036865234 | latent_loss: 5.785770893096924 | total_loss: 21.7799072265625\n",
      "Epoch: 510 | recon_loss: 39.6338996887207 | latent_loss: 5.771149158477783 | total_loss: 22.702524185180664\n",
      "Epoch: 511 | recon_loss: 39.17781066894531 | latent_loss: 5.770350933074951 | total_loss: 22.47408103942871\n",
      "Epoch: 512 | recon_loss: 36.27660369873047 | latent_loss: 5.7541046142578125 | total_loss: 21.01535415649414\n",
      "Epoch: 513 | recon_loss: 35.779090881347656 | latent_loss: 5.699212551116943 | total_loss: 20.739151000976562\n",
      "Epoch: 514 | recon_loss: 35.45708465576172 | latent_loss: 5.692279815673828 | total_loss: 20.574682235717773\n",
      "Epoch: 515 | recon_loss: 35.82563781738281 | latent_loss: 5.721979141235352 | total_loss: 20.773807525634766\n",
      "Epoch: 516 | recon_loss: 36.61680603027344 | latent_loss: 5.699551582336426 | total_loss: 21.158178329467773\n",
      "Epoch: 517 | recon_loss: 37.84175491333008 | latent_loss: 5.702206611633301 | total_loss: 21.77198028564453\n",
      "Epoch: 518 | recon_loss: 35.1932373046875 | latent_loss: 5.710650444030762 | total_loss: 20.45194435119629\n",
      "Epoch: 519 | recon_loss: 35.72908020019531 | latent_loss: 5.713507175445557 | total_loss: 20.721294403076172\n",
      "Epoch: 520 | recon_loss: 35.421260833740234 | latent_loss: 5.717193603515625 | total_loss: 20.56922721862793\n",
      "Epoch: 521 | recon_loss: 35.89907455444336 | latent_loss: 5.722044467926025 | total_loss: 20.81056022644043\n",
      "Epoch: 522 | recon_loss: 34.93087387084961 | latent_loss: 5.721990585327148 | total_loss: 20.326431274414062\n",
      "Epoch: 523 | recon_loss: 37.15671920776367 | latent_loss: 5.718321323394775 | total_loss: 21.43752098083496\n",
      "Epoch: 524 | recon_loss: 34.95338439941406 | latent_loss: 5.712551116943359 | total_loss: 20.33296775817871\n",
      "Epoch: 525 | recon_loss: 36.295841217041016 | latent_loss: 5.704294204711914 | total_loss: 21.00006866455078\n",
      "Epoch: 526 | recon_loss: 38.38911056518555 | latent_loss: 5.72158145904541 | total_loss: 22.05534553527832\n",
      "Epoch: 527 | recon_loss: 35.16756820678711 | latent_loss: 5.734233379364014 | total_loss: 20.45090103149414\n",
      "Epoch: 528 | recon_loss: 39.49668884277344 | latent_loss: 5.745355606079102 | total_loss: 22.621021270751953\n",
      "Epoch: 529 | recon_loss: 34.32876205444336 | latent_loss: 5.719630241394043 | total_loss: 20.02419662475586\n",
      "Epoch: 530 | recon_loss: 37.967857360839844 | latent_loss: 5.747927665710449 | total_loss: 21.857892990112305\n",
      "Epoch: 531 | recon_loss: 38.5500373840332 | latent_loss: 5.6755523681640625 | total_loss: 22.112794876098633\n",
      "Epoch: 532 | recon_loss: 37.054805755615234 | latent_loss: 5.660362720489502 | total_loss: 21.35758399963379\n",
      "Epoch: 533 | recon_loss: 33.74433898925781 | latent_loss: 5.644258975982666 | total_loss: 19.694299697875977\n",
      "Epoch: 534 | recon_loss: 36.51087951660156 | latent_loss: 5.6436262130737305 | total_loss: 21.077253341674805\n",
      "Epoch: 535 | recon_loss: 37.53879165649414 | latent_loss: 5.658135414123535 | total_loss: 21.59846305847168\n",
      "Epoch: 536 | recon_loss: 36.8366813659668 | latent_loss: 5.695428848266602 | total_loss: 21.266056060791016\n",
      "Epoch: 537 | recon_loss: 36.928646087646484 | latent_loss: 5.681594371795654 | total_loss: 21.30512046813965\n",
      "Epoch: 538 | recon_loss: 38.89483642578125 | latent_loss: 5.6676225662231445 | total_loss: 22.28122901916504\n",
      "Epoch: 539 | recon_loss: 36.66722106933594 | latent_loss: 5.675258636474609 | total_loss: 21.171239852905273\n",
      "Epoch: 540 | recon_loss: 36.24932098388672 | latent_loss: 5.690141677856445 | total_loss: 20.969730377197266\n",
      "Epoch: 541 | recon_loss: 38.67940139770508 | latent_loss: 5.688472747802734 | total_loss: 22.183937072753906\n",
      "Epoch: 542 | recon_loss: 34.884254455566406 | latent_loss: 5.654301166534424 | total_loss: 20.269277572631836\n",
      "Epoch: 543 | recon_loss: 35.81276321411133 | latent_loss: 5.673374652862549 | total_loss: 20.74306869506836\n",
      "Epoch: 544 | recon_loss: 35.8425178527832 | latent_loss: 5.693114757537842 | total_loss: 20.7678165435791\n",
      "Epoch: 545 | recon_loss: 35.51408386230469 | latent_loss: 5.710605621337891 | total_loss: 20.61234474182129\n",
      "Epoch: 546 | recon_loss: 40.323951721191406 | latent_loss: 5.679147720336914 | total_loss: 23.001548767089844\n",
      "Epoch: 547 | recon_loss: 33.66866683959961 | latent_loss: 5.6416192054748535 | total_loss: 19.65514373779297\n",
      "Epoch: 548 | recon_loss: 36.67866516113281 | latent_loss: 5.628655910491943 | total_loss: 21.15365982055664\n",
      "Epoch: 549 | recon_loss: 37.228057861328125 | latent_loss: 5.624344348907471 | total_loss: 21.42620086669922\n",
      "Epoch: 550 | recon_loss: 36.281898498535156 | latent_loss: 5.615982532501221 | total_loss: 20.94894027709961\n",
      "Epoch: 551 | recon_loss: 37.94523620605469 | latent_loss: 5.604407787322998 | total_loss: 21.774822235107422\n",
      "Epoch: 552 | recon_loss: 37.88563537597656 | latent_loss: 5.60312032699585 | total_loss: 21.74437713623047\n",
      "Epoch: 553 | recon_loss: 35.3829460144043 | latent_loss: 5.60359001159668 | total_loss: 20.493267059326172\n",
      "Epoch: 554 | recon_loss: 38.869667053222656 | latent_loss: 5.612376689910889 | total_loss: 22.24102210998535\n",
      "Epoch: 555 | recon_loss: 35.67795944213867 | latent_loss: 5.6387434005737305 | total_loss: 20.65835189819336\n",
      "Epoch: 556 | recon_loss: 36.15018844604492 | latent_loss: 5.632198333740234 | total_loss: 20.891193389892578\n",
      "Epoch: 557 | recon_loss: 38.57808303833008 | latent_loss: 5.631689071655273 | total_loss: 22.10488510131836\n",
      "Epoch: 558 | recon_loss: 40.2702751159668 | latent_loss: 5.657648086547852 | total_loss: 22.96396255493164\n",
      "Epoch: 559 | recon_loss: 38.25175094604492 | latent_loss: 5.6563801765441895 | total_loss: 21.954065322875977\n",
      "Epoch: 560 | recon_loss: 35.155330657958984 | latent_loss: 5.616608142852783 | total_loss: 20.385969161987305\n",
      "Epoch: 561 | recon_loss: 35.42413330078125 | latent_loss: 5.617211818695068 | total_loss: 20.520671844482422\n",
      "Epoch: 562 | recon_loss: 36.2651252746582 | latent_loss: 5.636229038238525 | total_loss: 20.9506778717041\n",
      "Epoch: 563 | recon_loss: 35.63199234008789 | latent_loss: 5.605032920837402 | total_loss: 20.618513107299805\n",
      "Epoch: 564 | recon_loss: 36.88325119018555 | latent_loss: 5.593158721923828 | total_loss: 21.238204956054688\n",
      "Epoch: 565 | recon_loss: 35.86203384399414 | latent_loss: 5.584414482116699 | total_loss: 20.723224639892578\n",
      "Epoch: 566 | recon_loss: 36.6795539855957 | latent_loss: 5.557778358459473 | total_loss: 21.11866569519043\n",
      "Epoch: 567 | recon_loss: 37.22399139404297 | latent_loss: 5.554444789886475 | total_loss: 21.389217376708984\n",
      "Epoch: 568 | recon_loss: 36.4094123840332 | latent_loss: 5.53294563293457 | total_loss: 20.971179962158203\n",
      "Epoch: 569 | recon_loss: 38.388973236083984 | latent_loss: 5.544342517852783 | total_loss: 21.966657638549805\n",
      "Epoch: 570 | recon_loss: 37.3569221496582 | latent_loss: 5.559697151184082 | total_loss: 21.458309173583984\n",
      "Epoch: 571 | recon_loss: 37.19271469116211 | latent_loss: 5.573972225189209 | total_loss: 21.383342742919922\n",
      "Epoch: 572 | recon_loss: 36.1613883972168 | latent_loss: 5.591576099395752 | total_loss: 20.876482009887695\n",
      "Epoch: 573 | recon_loss: 35.879703521728516 | latent_loss: 5.5717010498046875 | total_loss: 20.7257022857666\n",
      "Epoch: 574 | recon_loss: 35.652565002441406 | latent_loss: 5.562501907348633 | total_loss: 20.607532501220703\n",
      "Epoch: 575 | recon_loss: 38.33436584472656 | latent_loss: 5.553598403930664 | total_loss: 21.943981170654297\n",
      "Epoch: 576 | recon_loss: 38.217613220214844 | latent_loss: 5.555235385894775 | total_loss: 21.886425018310547\n",
      "Epoch: 577 | recon_loss: 35.402099609375 | latent_loss: 5.542945384979248 | total_loss: 20.472522735595703\n",
      "Epoch: 578 | recon_loss: 36.06867218017578 | latent_loss: 5.532435894012451 | total_loss: 20.800554275512695\n",
      "Epoch: 579 | recon_loss: 35.33549499511719 | latent_loss: 5.525619029998779 | total_loss: 20.430557250976562\n",
      "Epoch: 580 | recon_loss: 34.63270950317383 | latent_loss: 5.5119757652282715 | total_loss: 20.072341918945312\n",
      "Epoch: 581 | recon_loss: 42.946693420410156 | latent_loss: 5.502079963684082 | total_loss: 24.22438621520996\n",
      "Epoch: 582 | recon_loss: 38.21076583862305 | latent_loss: 5.501124382019043 | total_loss: 21.855945587158203\n",
      "Epoch: 583 | recon_loss: 39.19601058959961 | latent_loss: 5.5276570320129395 | total_loss: 22.361833572387695\n",
      "Epoch: 584 | recon_loss: 34.49174118041992 | latent_loss: 5.510534763336182 | total_loss: 20.00113868713379\n",
      "Epoch: 585 | recon_loss: 38.17039489746094 | latent_loss: 5.504265785217285 | total_loss: 21.837329864501953\n",
      "Epoch: 586 | recon_loss: 36.12617492675781 | latent_loss: 5.5383687019348145 | total_loss: 20.832271575927734\n",
      "Epoch: 587 | recon_loss: 36.99143981933594 | latent_loss: 5.500769138336182 | total_loss: 21.246105194091797\n",
      "Epoch: 588 | recon_loss: 38.46148681640625 | latent_loss: 5.497100830078125 | total_loss: 21.979293823242188\n",
      "Epoch: 589 | recon_loss: 34.45503234863281 | latent_loss: 5.499700546264648 | total_loss: 19.977367401123047\n",
      "Epoch: 590 | recon_loss: 35.2868766784668 | latent_loss: 5.507519721984863 | total_loss: 20.397197723388672\n",
      "Epoch: 591 | recon_loss: 39.71373748779297 | latent_loss: 5.4625244140625 | total_loss: 22.588130950927734\n",
      "Epoch: 592 | recon_loss: 35.417686462402344 | latent_loss: 5.453749179840088 | total_loss: 20.435718536376953\n",
      "Epoch: 593 | recon_loss: 33.50413513183594 | latent_loss: 5.433804512023926 | total_loss: 19.468969345092773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 594 | recon_loss: 37.12522888183594 | latent_loss: 5.423421859741211 | total_loss: 21.27432632446289\n",
      "Epoch: 595 | recon_loss: 38.09125900268555 | latent_loss: 5.445416450500488 | total_loss: 21.76833724975586\n",
      "Epoch: 596 | recon_loss: 37.18135070800781 | latent_loss: 5.471043586730957 | total_loss: 21.326196670532227\n",
      "Epoch: 597 | recon_loss: 36.00945281982422 | latent_loss: 5.488447189331055 | total_loss: 20.748950958251953\n",
      "Epoch: 598 | recon_loss: 37.9837532043457 | latent_loss: 5.468301296234131 | total_loss: 21.72602653503418\n",
      "Epoch: 599 | recon_loss: 34.258636474609375 | latent_loss: 5.4367170333862305 | total_loss: 19.84767723083496\n",
      "Epoch: 600 | recon_loss: 36.11518096923828 | latent_loss: 5.411529541015625 | total_loss: 20.763355255126953\n",
      "Epoch: 601 | recon_loss: 35.60740661621094 | latent_loss: 5.384028911590576 | total_loss: 20.495718002319336\n",
      "Epoch: 602 | recon_loss: 38.34607696533203 | latent_loss: 5.360528945922852 | total_loss: 21.853302001953125\n",
      "Epoch: 603 | recon_loss: 36.428367614746094 | latent_loss: 5.344754219055176 | total_loss: 20.886560440063477\n",
      "Epoch: 604 | recon_loss: 39.96975326538086 | latent_loss: 5.3482208251953125 | total_loss: 22.658987045288086\n",
      "Epoch: 605 | recon_loss: 36.289913177490234 | latent_loss: 5.387254238128662 | total_loss: 20.83858299255371\n",
      "Epoch: 606 | recon_loss: 39.5387077331543 | latent_loss: 5.375148773193359 | total_loss: 22.456928253173828\n",
      "Epoch: 607 | recon_loss: 34.48792266845703 | latent_loss: 5.393150806427002 | total_loss: 19.940536499023438\n",
      "Epoch: 608 | recon_loss: 36.34621047973633 | latent_loss: 5.38906717300415 | total_loss: 20.867639541625977\n",
      "Epoch: 609 | recon_loss: 35.339290618896484 | latent_loss: 5.3794403076171875 | total_loss: 20.359365463256836\n",
      "Epoch: 610 | recon_loss: 37.56832504272461 | latent_loss: 5.3702263832092285 | total_loss: 21.469276428222656\n",
      "Epoch: 611 | recon_loss: 36.5699577331543 | latent_loss: 5.363959312438965 | total_loss: 20.96695899963379\n",
      "Epoch: 612 | recon_loss: 39.27043914794922 | latent_loss: 5.364388465881348 | total_loss: 22.317413330078125\n",
      "Epoch: 613 | recon_loss: 36.018585205078125 | latent_loss: 5.367883682250977 | total_loss: 20.693233489990234\n",
      "Epoch: 614 | recon_loss: 37.125179290771484 | latent_loss: 5.374084949493408 | total_loss: 21.249631881713867\n",
      "Epoch: 615 | recon_loss: 34.33783721923828 | latent_loss: 5.362943649291992 | total_loss: 19.850391387939453\n",
      "Epoch: 616 | recon_loss: 34.95867919921875 | latent_loss: 5.352499485015869 | total_loss: 20.155590057373047\n",
      "Epoch: 617 | recon_loss: 37.344078063964844 | latent_loss: 5.3356828689575195 | total_loss: 21.339879989624023\n",
      "Epoch: 618 | recon_loss: 38.007137298583984 | latent_loss: 5.318540096282959 | total_loss: 21.662837982177734\n",
      "Epoch: 619 | recon_loss: 36.35307312011719 | latent_loss: 5.3375420570373535 | total_loss: 20.845308303833008\n",
      "Epoch: 620 | recon_loss: 35.79100799560547 | latent_loss: 5.3547492027282715 | total_loss: 20.572877883911133\n",
      "Epoch: 621 | recon_loss: 34.51447677612305 | latent_loss: 5.3744940757751465 | total_loss: 19.94448471069336\n",
      "Epoch: 622 | recon_loss: 33.7437858581543 | latent_loss: 5.378337860107422 | total_loss: 19.56106185913086\n",
      "Epoch: 623 | recon_loss: 36.14398193359375 | latent_loss: 5.3800201416015625 | total_loss: 20.762001037597656\n",
      "Epoch: 624 | recon_loss: 37.31713104248047 | latent_loss: 5.356594085693359 | total_loss: 21.336862564086914\n",
      "Epoch: 625 | recon_loss: 37.17401885986328 | latent_loss: 5.352447032928467 | total_loss: 21.263233184814453\n",
      "Epoch: 626 | recon_loss: 37.11589050292969 | latent_loss: 5.365439414978027 | total_loss: 21.240665435791016\n",
      "Epoch: 627 | recon_loss: 38.30999755859375 | latent_loss: 5.361517429351807 | total_loss: 21.835758209228516\n",
      "Epoch: 628 | recon_loss: 36.962982177734375 | latent_loss: 5.3313398361206055 | total_loss: 21.14716148376465\n",
      "Epoch: 629 | recon_loss: 37.3647346496582 | latent_loss: 5.322808742523193 | total_loss: 21.34377098083496\n",
      "Epoch: 630 | recon_loss: 34.36464309692383 | latent_loss: 5.326693058013916 | total_loss: 19.84566879272461\n",
      "Epoch: 631 | recon_loss: 38.29449462890625 | latent_loss: 5.362004280090332 | total_loss: 21.828248977661133\n",
      "Epoch: 632 | recon_loss: 37.24113845825195 | latent_loss: 5.335516452789307 | total_loss: 21.288328170776367\n",
      "Epoch: 633 | recon_loss: 34.73279571533203 | latent_loss: 5.316168308258057 | total_loss: 20.02448272705078\n",
      "Epoch: 634 | recon_loss: 35.44178771972656 | latent_loss: 5.305917263031006 | total_loss: 20.373851776123047\n",
      "Epoch: 635 | recon_loss: 35.63007354736328 | latent_loss: 5.319746494293213 | total_loss: 20.474910736083984\n",
      "Epoch: 636 | recon_loss: 37.682472229003906 | latent_loss: 5.307045936584473 | total_loss: 21.49475860595703\n",
      "Epoch: 637 | recon_loss: 34.591224670410156 | latent_loss: 5.296800136566162 | total_loss: 19.944011688232422\n",
      "Epoch: 638 | recon_loss: 35.58601379394531 | latent_loss: 5.281563758850098 | total_loss: 20.433788299560547\n",
      "Epoch: 639 | recon_loss: 37.471923828125 | latent_loss: 5.280460357666016 | total_loss: 21.376192092895508\n",
      "Epoch: 640 | recon_loss: 37.917633056640625 | latent_loss: 5.279423236846924 | total_loss: 21.598527908325195\n",
      "Epoch: 641 | recon_loss: 36.15208053588867 | latent_loss: 5.275857925415039 | total_loss: 20.713970184326172\n",
      "Epoch: 642 | recon_loss: 35.236305236816406 | latent_loss: 5.267376899719238 | total_loss: 20.251840591430664\n",
      "Epoch: 643 | recon_loss: 35.58039855957031 | latent_loss: 5.260979175567627 | total_loss: 20.42068862915039\n",
      "Epoch: 644 | recon_loss: 35.07246780395508 | latent_loss: 5.257656097412109 | total_loss: 20.165061950683594\n",
      "Epoch: 645 | recon_loss: 35.118194580078125 | latent_loss: 5.258588790893555 | total_loss: 20.188392639160156\n",
      "Epoch: 646 | recon_loss: 34.70193099975586 | latent_loss: 5.266235828399658 | total_loss: 19.98408317565918\n",
      "Epoch: 647 | recon_loss: 37.884456634521484 | latent_loss: 5.267447471618652 | total_loss: 21.575952529907227\n",
      "Epoch: 648 | recon_loss: 35.74862289428711 | latent_loss: 5.258653163909912 | total_loss: 20.503637313842773\n",
      "Epoch: 649 | recon_loss: 38.52378845214844 | latent_loss: 5.257175445556641 | total_loss: 21.89048194885254\n",
      "Epoch: 650 | recon_loss: 32.477142333984375 | latent_loss: 5.266584396362305 | total_loss: 18.871864318847656\n",
      "Epoch: 651 | recon_loss: 35.15077590942383 | latent_loss: 5.272523403167725 | total_loss: 20.21164894104004\n",
      "Epoch: 652 | recon_loss: 38.51536560058594 | latent_loss: 5.267734527587891 | total_loss: 21.891550064086914\n",
      "Epoch: 653 | recon_loss: 37.48134994506836 | latent_loss: 5.266513824462891 | total_loss: 21.373931884765625\n",
      "Epoch: 654 | recon_loss: 35.98272705078125 | latent_loss: 5.279485702514648 | total_loss: 20.631107330322266\n",
      "Epoch: 655 | recon_loss: 35.67893981933594 | latent_loss: 5.27485466003418 | total_loss: 20.476898193359375\n",
      "Epoch: 656 | recon_loss: 34.63018798828125 | latent_loss: 5.315616607666016 | total_loss: 19.972902297973633\n",
      "Epoch: 657 | recon_loss: 36.45760726928711 | latent_loss: 5.347527027130127 | total_loss: 20.90256690979004\n",
      "Epoch: 658 | recon_loss: 35.34291458129883 | latent_loss: 5.333186626434326 | total_loss: 20.338050842285156\n",
      "Epoch: 659 | recon_loss: 35.52028274536133 | latent_loss: 5.300328254699707 | total_loss: 20.41030502319336\n",
      "Epoch: 660 | recon_loss: 35.66445541381836 | latent_loss: 5.2820820808410645 | total_loss: 20.473268508911133\n",
      "Epoch: 661 | recon_loss: 35.648433685302734 | latent_loss: 5.271681308746338 | total_loss: 20.460058212280273\n",
      "Epoch: 662 | recon_loss: 34.64936065673828 | latent_loss: 5.275053024291992 | total_loss: 19.962207794189453\n",
      "Epoch: 663 | recon_loss: 36.95022201538086 | latent_loss: 5.271414756774902 | total_loss: 21.11081886291504\n",
      "Epoch: 664 | recon_loss: 36.62619400024414 | latent_loss: 5.266061782836914 | total_loss: 20.946128845214844\n",
      "Epoch: 665 | recon_loss: 36.03588104248047 | latent_loss: 5.268582820892334 | total_loss: 20.652231216430664\n",
      "Epoch: 666 | recon_loss: 34.6723518371582 | latent_loss: 5.274896144866943 | total_loss: 19.973623275756836\n",
      "Epoch: 667 | recon_loss: 33.614559173583984 | latent_loss: 5.279139518737793 | total_loss: 19.446849822998047\n",
      "Epoch: 668 | recon_loss: 37.43116760253906 | latent_loss: 5.282663345336914 | total_loss: 21.356914520263672\n",
      "Epoch: 669 | recon_loss: 34.70216369628906 | latent_loss: 5.292212009429932 | total_loss: 19.997188568115234\n",
      "Epoch: 670 | recon_loss: 36.95058822631836 | latent_loss: 5.326130390167236 | total_loss: 21.13835906982422\n",
      "Epoch: 671 | recon_loss: 37.11030578613281 | latent_loss: 5.316473007202148 | total_loss: 21.213390350341797\n",
      "Epoch: 672 | recon_loss: 37.5261344909668 | latent_loss: 5.286376953125 | total_loss: 21.4062557220459\n",
      "Epoch: 673 | recon_loss: 36.358306884765625 | latent_loss: 5.282468318939209 | total_loss: 20.82038688659668\n",
      "Epoch: 674 | recon_loss: 33.51580047607422 | latent_loss: 5.273749828338623 | total_loss: 19.394775390625\n",
      "Epoch: 675 | recon_loss: 34.87052917480469 | latent_loss: 5.264379978179932 | total_loss: 20.067455291748047\n",
      "Epoch: 676 | recon_loss: 35.27766036987305 | latent_loss: 5.262878894805908 | total_loss: 20.2702693939209\n",
      "Epoch: 677 | recon_loss: 36.32490539550781 | latent_loss: 5.255363941192627 | total_loss: 20.79013442993164\n",
      "Epoch: 678 | recon_loss: 35.46408462524414 | latent_loss: 5.241219520568848 | total_loss: 20.352651596069336\n",
      "Epoch: 679 | recon_loss: 35.33637237548828 | latent_loss: 5.225154876708984 | total_loss: 20.280763626098633\n",
      "Epoch: 680 | recon_loss: 34.4013786315918 | latent_loss: 5.2091169357299805 | total_loss: 19.805248260498047\n",
      "Epoch: 681 | recon_loss: 35.425235748291016 | latent_loss: 5.202796459197998 | total_loss: 20.314016342163086\n",
      "Epoch: 682 | recon_loss: 38.816349029541016 | latent_loss: 5.194421768188477 | total_loss: 22.005386352539062\n",
      "Epoch: 683 | recon_loss: 37.96516799926758 | latent_loss: 5.189934253692627 | total_loss: 21.577550888061523\n",
      "Epoch: 684 | recon_loss: 35.2427978515625 | latent_loss: 5.198163032531738 | total_loss: 20.22047996520996\n",
      "Epoch: 685 | recon_loss: 37.50389862060547 | latent_loss: 5.1991424560546875 | total_loss: 21.351520538330078\n",
      "Epoch: 686 | recon_loss: 35.9185791015625 | latent_loss: 5.194045543670654 | total_loss: 20.556312561035156\n",
      "Epoch: 687 | recon_loss: 38.303558349609375 | latent_loss: 5.17399787902832 | total_loss: 21.73877716064453\n",
      "Epoch: 688 | recon_loss: 36.089298248291016 | latent_loss: 5.169783115386963 | total_loss: 20.629541397094727\n",
      "Epoch: 689 | recon_loss: 38.0749397277832 | latent_loss: 5.180037975311279 | total_loss: 21.62748908996582\n",
      "Epoch: 690 | recon_loss: 33.38572692871094 | latent_loss: 5.192266464233398 | total_loss: 19.288997650146484\n",
      "Epoch: 691 | recon_loss: 37.358482360839844 | latent_loss: 5.1829705238342285 | total_loss: 21.270727157592773\n",
      "Epoch: 692 | recon_loss: 37.05409622192383 | latent_loss: 5.182848930358887 | total_loss: 21.118473052978516\n",
      "Epoch: 693 | recon_loss: 37.71846389770508 | latent_loss: 5.193174362182617 | total_loss: 21.45581817626953\n",
      "Epoch: 694 | recon_loss: 36.03561782836914 | latent_loss: 5.1986918449401855 | total_loss: 20.617155075073242\n",
      "Epoch: 695 | recon_loss: 36.9317741394043 | latent_loss: 5.194009304046631 | total_loss: 21.062891006469727\n",
      "Epoch: 696 | recon_loss: 37.74624252319336 | latent_loss: 5.181056499481201 | total_loss: 21.46364974975586\n",
      "Epoch: 697 | recon_loss: 36.22631072998047 | latent_loss: 5.177047252655029 | total_loss: 20.701679229736328\n",
      "Epoch: 698 | recon_loss: 34.01357650756836 | latent_loss: 5.183287143707275 | total_loss: 19.598432540893555\n",
      "Epoch: 699 | recon_loss: 39.943092346191406 | latent_loss: 5.178484916687012 | total_loss: 22.560789108276367\n",
      "Epoch: 700 | recon_loss: 35.53569793701172 | latent_loss: 5.167250156402588 | total_loss: 20.35147476196289\n",
      "Epoch: 701 | recon_loss: 32.52250671386719 | latent_loss: 5.173289775848389 | total_loss: 18.847898483276367\n",
      "Epoch: 702 | recon_loss: 36.60074996948242 | latent_loss: 5.172708511352539 | total_loss: 20.886730194091797\n",
      "Epoch: 703 | recon_loss: 37.2564697265625 | latent_loss: 5.168211460113525 | total_loss: 21.21234130859375\n",
      "Epoch: 704 | recon_loss: 35.495704650878906 | latent_loss: 5.173497200012207 | total_loss: 20.3346004486084\n",
      "Epoch: 705 | recon_loss: 37.50300979614258 | latent_loss: 5.1456708908081055 | total_loss: 21.3243408203125\n",
      "Epoch: 706 | recon_loss: 34.71260070800781 | latent_loss: 5.1566619873046875 | total_loss: 19.93463134765625\n",
      "Epoch: 707 | recon_loss: 39.34794616699219 | latent_loss: 5.135631561279297 | total_loss: 22.241788864135742\n",
      "Epoch: 708 | recon_loss: 39.31782531738281 | latent_loss: 5.152887344360352 | total_loss: 22.235355377197266\n",
      "Epoch: 709 | recon_loss: 37.342628479003906 | latent_loss: 5.155332565307617 | total_loss: 21.248981475830078\n",
      "Epoch: 710 | recon_loss: 34.316368103027344 | latent_loss: 5.147220611572266 | total_loss: 19.731794357299805\n",
      "Epoch: 711 | recon_loss: 34.035037994384766 | latent_loss: 5.158985614776611 | total_loss: 19.59701156616211\n",
      "Epoch: 712 | recon_loss: 35.770904541015625 | latent_loss: 5.166660308837891 | total_loss: 20.468782424926758\n",
      "Epoch: 713 | recon_loss: 36.03354263305664 | latent_loss: 5.1612229347229 | total_loss: 20.597383499145508\n",
      "Epoch: 714 | recon_loss: 35.52196502685547 | latent_loss: 5.180631160736084 | total_loss: 20.35129737854004\n",
      "Epoch: 715 | recon_loss: 36.09217071533203 | latent_loss: 5.143581390380859 | total_loss: 20.617876052856445\n",
      "Epoch: 716 | recon_loss: 34.69151306152344 | latent_loss: 5.136488437652588 | total_loss: 19.91400146484375\n",
      "Epoch: 717 | recon_loss: 35.03294372558594 | latent_loss: 5.129621982574463 | total_loss: 20.081283569335938\n",
      "Epoch: 718 | recon_loss: 37.00531005859375 | latent_loss: 5.1360087394714355 | total_loss: 21.070659637451172\n",
      "Epoch: 719 | recon_loss: 38.62908172607422 | latent_loss: 5.129428863525391 | total_loss: 21.879255294799805\n",
      "Epoch: 720 | recon_loss: 36.63362121582031 | latent_loss: 5.141619682312012 | total_loss: 20.88762092590332\n",
      "Epoch: 721 | recon_loss: 34.78783416748047 | latent_loss: 5.134923934936523 | total_loss: 19.961380004882812\n",
      "Epoch: 722 | recon_loss: 34.77375793457031 | latent_loss: 5.142877101898193 | total_loss: 19.958316802978516\n",
      "Epoch: 723 | recon_loss: 37.00591278076172 | latent_loss: 5.160397529602051 | total_loss: 21.083154678344727\n",
      "Epoch: 724 | recon_loss: 37.33864212036133 | latent_loss: 5.184930801391602 | total_loss: 21.26178741455078\n",
      "Epoch: 725 | recon_loss: 37.07115936279297 | latent_loss: 5.158778667449951 | total_loss: 21.11496925354004\n",
      "Epoch: 726 | recon_loss: 36.01511764526367 | latent_loss: 5.134609222412109 | total_loss: 20.57486343383789\n",
      "Epoch: 727 | recon_loss: 34.95772933959961 | latent_loss: 5.1021928787231445 | total_loss: 20.02996063232422\n",
      "Epoch: 728 | recon_loss: 36.87133026123047 | latent_loss: 5.101491451263428 | total_loss: 20.98641014099121\n",
      "Epoch: 729 | recon_loss: 36.6917724609375 | latent_loss: 5.132522106170654 | total_loss: 20.912147521972656\n",
      "Epoch: 730 | recon_loss: 36.5211181640625 | latent_loss: 5.149506092071533 | total_loss: 20.835311889648438\n",
      "Epoch: 731 | recon_loss: 36.13810729980469 | latent_loss: 5.161420822143555 | total_loss: 20.649765014648438\n",
      "Epoch: 732 | recon_loss: 34.04610824584961 | latent_loss: 5.162984848022461 | total_loss: 19.60454559326172\n",
      "Epoch: 733 | recon_loss: 37.54962921142578 | latent_loss: 5.165503978729248 | total_loss: 21.357566833496094\n",
      "Epoch: 734 | recon_loss: 33.078460693359375 | latent_loss: 5.127868175506592 | total_loss: 19.103164672851562\n",
      "Epoch: 735 | recon_loss: 36.64623260498047 | latent_loss: 5.113218307495117 | total_loss: 20.87972640991211\n",
      "Epoch: 736 | recon_loss: 34.966888427734375 | latent_loss: 5.095514297485352 | total_loss: 20.031200408935547\n",
      "Epoch: 737 | recon_loss: 35.52631759643555 | latent_loss: 5.086221218109131 | total_loss: 20.3062686920166\n",
      "Epoch: 738 | recon_loss: 35.643218994140625 | latent_loss: 5.088973522186279 | total_loss: 20.36609649658203\n",
      "Epoch: 739 | recon_loss: 36.82837677001953 | latent_loss: 5.078503608703613 | total_loss: 20.953439712524414\n",
      "Epoch: 740 | recon_loss: 35.01126480102539 | latent_loss: 5.075884819030762 | total_loss: 20.043575286865234\n",
      "Epoch: 741 | recon_loss: 35.34742736816406 | latent_loss: 5.06787109375 | total_loss: 20.20764923095703\n",
      "Epoch: 742 | recon_loss: 36.46305847167969 | latent_loss: 5.055482387542725 | total_loss: 20.75926971435547\n",
      "Epoch: 743 | recon_loss: 35.69077682495117 | latent_loss: 5.049485206604004 | total_loss: 20.37013053894043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 744 | recon_loss: 35.50563049316406 | latent_loss: 5.058650970458984 | total_loss: 20.282140731811523\n",
      "Epoch: 745 | recon_loss: 36.02708435058594 | latent_loss: 5.051197052001953 | total_loss: 20.539140701293945\n",
      "Epoch: 746 | recon_loss: 32.98656463623047 | latent_loss: 5.046037673950195 | total_loss: 19.016300201416016\n",
      "Epoch: 747 | recon_loss: 34.89533996582031 | latent_loss: 5.045056343078613 | total_loss: 19.970197677612305\n",
      "Epoch: 748 | recon_loss: 35.19281005859375 | latent_loss: 5.059830188751221 | total_loss: 20.126319885253906\n",
      "Epoch: 749 | recon_loss: 35.837215423583984 | latent_loss: 5.066617012023926 | total_loss: 20.451915740966797\n",
      "Epoch: 750 | recon_loss: 35.78056335449219 | latent_loss: 5.060623645782471 | total_loss: 20.42059326171875\n",
      "Epoch: 751 | recon_loss: 37.351505279541016 | latent_loss: 5.038664817810059 | total_loss: 21.195085525512695\n",
      "Epoch: 752 | recon_loss: 37.84627151489258 | latent_loss: 5.022536754608154 | total_loss: 21.434404373168945\n",
      "Epoch: 753 | recon_loss: 36.34814453125 | latent_loss: 5.0133538246154785 | total_loss: 20.680749893188477\n",
      "Epoch: 754 | recon_loss: 35.89019012451172 | latent_loss: 5.013082981109619 | total_loss: 20.451637268066406\n",
      "Epoch: 755 | recon_loss: 37.11138916015625 | latent_loss: 5.029958724975586 | total_loss: 21.070674896240234\n",
      "Epoch: 756 | recon_loss: 34.47224807739258 | latent_loss: 5.060480117797852 | total_loss: 19.76636505126953\n",
      "Epoch: 757 | recon_loss: 34.61039733886719 | latent_loss: 5.078353404998779 | total_loss: 19.844375610351562\n",
      "Epoch: 758 | recon_loss: 35.04411697387695 | latent_loss: 5.0801825523376465 | total_loss: 20.062149047851562\n",
      "Epoch: 759 | recon_loss: 35.3701057434082 | latent_loss: 5.056909561157227 | total_loss: 20.21350860595703\n",
      "Epoch: 760 | recon_loss: 37.59959030151367 | latent_loss: 5.038539409637451 | total_loss: 21.31906509399414\n",
      "Epoch: 761 | recon_loss: 37.166404724121094 | latent_loss: 5.029557704925537 | total_loss: 21.097980499267578\n",
      "Epoch: 762 | recon_loss: 35.69808578491211 | latent_loss: 5.0256195068359375 | total_loss: 20.361852645874023\n",
      "Epoch: 763 | recon_loss: 35.68894958496094 | latent_loss: 5.034106254577637 | total_loss: 20.361528396606445\n",
      "Epoch: 764 | recon_loss: 33.292816162109375 | latent_loss: 5.050561428070068 | total_loss: 19.171688079833984\n",
      "Epoch: 765 | recon_loss: 34.60130310058594 | latent_loss: 5.063235282897949 | total_loss: 19.8322696685791\n",
      "Epoch: 766 | recon_loss: 36.36000061035156 | latent_loss: 5.057306289672852 | total_loss: 20.70865249633789\n",
      "Epoch: 767 | recon_loss: 35.48405456542969 | latent_loss: 5.044168472290039 | total_loss: 20.264110565185547\n",
      "Epoch: 768 | recon_loss: 34.45139694213867 | latent_loss: 5.0367751121521 | total_loss: 19.74408531188965\n",
      "Epoch: 769 | recon_loss: 34.44391632080078 | latent_loss: 5.030649662017822 | total_loss: 19.73728370666504\n",
      "Epoch: 770 | recon_loss: 34.52266311645508 | latent_loss: 5.0398640632629395 | total_loss: 19.78126335144043\n",
      "Epoch: 771 | recon_loss: 34.99958419799805 | latent_loss: 5.049081802368164 | total_loss: 20.024333953857422\n",
      "Epoch: 772 | recon_loss: 39.45298385620117 | latent_loss: 5.058618068695068 | total_loss: 22.255800247192383\n",
      "Epoch: 773 | recon_loss: 38.04751968383789 | latent_loss: 5.064690589904785 | total_loss: 21.55610466003418\n",
      "Epoch: 774 | recon_loss: 34.20661163330078 | latent_loss: 5.057697772979736 | total_loss: 19.63215446472168\n",
      "Epoch: 775 | recon_loss: 33.20954895019531 | latent_loss: 5.072202205657959 | total_loss: 19.1408748626709\n",
      "Epoch: 776 | recon_loss: 36.50383758544922 | latent_loss: 5.042130947113037 | total_loss: 20.77298355102539\n",
      "Epoch: 777 | recon_loss: 34.11272430419922 | latent_loss: 5.0367350578308105 | total_loss: 19.574729919433594\n",
      "Epoch: 778 | recon_loss: 36.944400787353516 | latent_loss: 5.038950443267822 | total_loss: 20.991676330566406\n",
      "Epoch: 779 | recon_loss: 36.132530212402344 | latent_loss: 5.047652721405029 | total_loss: 20.590091705322266\n",
      "Epoch: 780 | recon_loss: 35.804378509521484 | latent_loss: 5.054057598114014 | total_loss: 20.429218292236328\n",
      "Epoch: 781 | recon_loss: 40.318511962890625 | latent_loss: 5.059765338897705 | total_loss: 22.689138412475586\n",
      "Epoch: 782 | recon_loss: 38.56650161743164 | latent_loss: 5.063546180725098 | total_loss: 21.81502342224121\n",
      "Epoch: 783 | recon_loss: 36.81258010864258 | latent_loss: 5.059873580932617 | total_loss: 20.93622589111328\n",
      "Epoch: 784 | recon_loss: 35.240821838378906 | latent_loss: 5.055648326873779 | total_loss: 20.148235321044922\n",
      "Epoch: 785 | recon_loss: 35.21991729736328 | latent_loss: 5.053015232086182 | total_loss: 20.13646697998047\n",
      "Epoch: 786 | recon_loss: 38.77934265136719 | latent_loss: 5.048968315124512 | total_loss: 21.914155960083008\n",
      "Epoch: 787 | recon_loss: 35.715023040771484 | latent_loss: 5.038991451263428 | total_loss: 20.37700653076172\n",
      "Epoch: 788 | recon_loss: 38.03641891479492 | latent_loss: 5.024410247802734 | total_loss: 21.530414581298828\n",
      "Epoch: 789 | recon_loss: 35.335201263427734 | latent_loss: 5.010472297668457 | total_loss: 20.172836303710938\n",
      "Epoch: 790 | recon_loss: 33.805824279785156 | latent_loss: 5.014698028564453 | total_loss: 19.410261154174805\n",
      "Epoch: 791 | recon_loss: 40.753292083740234 | latent_loss: 5.000292778015137 | total_loss: 22.876792907714844\n",
      "Epoch: 792 | recon_loss: 37.780086517333984 | latent_loss: 5.010657787322998 | total_loss: 21.39537239074707\n",
      "Epoch: 793 | recon_loss: 36.83486557006836 | latent_loss: 5.0666279792785645 | total_loss: 20.950746536254883\n",
      "Epoch: 794 | recon_loss: 36.86917495727539 | latent_loss: 5.0581865310668945 | total_loss: 20.963680267333984\n",
      "Epoch: 795 | recon_loss: 35.617313385009766 | latent_loss: 5.064857006072998 | total_loss: 20.34108543395996\n",
      "Epoch: 796 | recon_loss: 34.87102127075195 | latent_loss: 5.074175834655762 | total_loss: 19.972599029541016\n",
      "Epoch: 797 | recon_loss: 34.34489822387695 | latent_loss: 5.079592704772949 | total_loss: 19.71224594116211\n",
      "Epoch: 798 | recon_loss: 35.95610046386719 | latent_loss: 5.072723388671875 | total_loss: 20.51441192626953\n",
      "The learning rate now is: <tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=0.00025>\n",
      "Epoch: 799 | recon_loss: 38.43173599243164 | latent_loss: 5.0651631355285645 | total_loss: 21.748449325561523\n",
      "Epoch: 800 | recon_loss: 35.23372268676758 | latent_loss: 5.072636604309082 | total_loss: 20.153179168701172\n",
      "Epoch: 801 | recon_loss: 34.58647918701172 | latent_loss: 5.081806659698486 | total_loss: 19.834142684936523\n",
      "Epoch: 802 | recon_loss: 36.04500961303711 | latent_loss: 5.086986064910889 | total_loss: 20.565998077392578\n",
      "Epoch: 803 | recon_loss: 34.74361038208008 | latent_loss: 5.107358932495117 | total_loss: 19.92548370361328\n",
      "Epoch: 804 | recon_loss: 36.157615661621094 | latent_loss: 5.057819843292236 | total_loss: 20.607717514038086\n",
      "Epoch: 805 | recon_loss: 32.76607131958008 | latent_loss: 5.063467025756836 | total_loss: 18.91476821899414\n",
      "Epoch: 806 | recon_loss: 34.365806579589844 | latent_loss: 5.078805446624756 | total_loss: 19.722305297851562\n",
      "Epoch: 807 | recon_loss: 35.23489761352539 | latent_loss: 5.084526538848877 | total_loss: 20.159711837768555\n",
      "Epoch: 808 | recon_loss: 36.66322326660156 | latent_loss: 5.086613655090332 | total_loss: 20.87491798400879\n",
      "Epoch: 809 | recon_loss: 35.63991928100586 | latent_loss: 5.076600551605225 | total_loss: 20.358259201049805\n",
      "Epoch: 810 | recon_loss: 36.52790832519531 | latent_loss: 5.052395820617676 | total_loss: 20.790151596069336\n",
      "Epoch: 811 | recon_loss: 34.8179931640625 | latent_loss: 5.039289951324463 | total_loss: 19.92864227294922\n",
      "Epoch: 812 | recon_loss: 34.14177322387695 | latent_loss: 5.044573783874512 | total_loss: 19.59317398071289\n",
      "Epoch: 813 | recon_loss: 35.56072235107422 | latent_loss: 5.0296950340271 | total_loss: 20.295207977294922\n",
      "Epoch: 814 | recon_loss: 36.88378143310547 | latent_loss: 5.0199079513549805 | total_loss: 20.951845169067383\n",
      "Epoch: 815 | recon_loss: 36.65341567993164 | latent_loss: 5.018895149230957 | total_loss: 20.83615493774414\n",
      "Epoch: 816 | recon_loss: 35.02360534667969 | latent_loss: 5.022570610046387 | total_loss: 20.023088455200195\n",
      "Epoch: 817 | recon_loss: 35.75657653808594 | latent_loss: 5.041227340698242 | total_loss: 20.398902893066406\n",
      "Epoch: 818 | recon_loss: 37.52488327026367 | latent_loss: 5.054467678070068 | total_loss: 21.289674758911133\n",
      "Epoch: 819 | recon_loss: 34.70814895629883 | latent_loss: 5.035926342010498 | total_loss: 19.872037887573242\n",
      "Epoch: 820 | recon_loss: 35.27675247192383 | latent_loss: 5.04852819442749 | total_loss: 20.162639617919922\n",
      "Epoch: 821 | recon_loss: 34.02296447753906 | latent_loss: 5.0745344161987305 | total_loss: 19.548749923706055\n",
      "Epoch: 822 | recon_loss: 37.36274719238281 | latent_loss: 5.0542521476745605 | total_loss: 21.208499908447266\n",
      "Epoch: 823 | recon_loss: 35.613773345947266 | latent_loss: 5.028189182281494 | total_loss: 20.320981979370117\n",
      "Epoch: 824 | recon_loss: 35.10139846801758 | latent_loss: 5.011097431182861 | total_loss: 20.05624771118164\n",
      "Epoch: 825 | recon_loss: 39.3900032043457 | latent_loss: 4.994471549987793 | total_loss: 22.192237854003906\n",
      "Epoch: 826 | recon_loss: 35.617679595947266 | latent_loss: 4.993844032287598 | total_loss: 20.305761337280273\n",
      "Epoch: 827 | recon_loss: 32.97312545776367 | latent_loss: 5.001791477203369 | total_loss: 18.987459182739258\n",
      "Epoch: 828 | recon_loss: 36.60917282104492 | latent_loss: 5.025424003601074 | total_loss: 20.817298889160156\n",
      "Epoch: 829 | recon_loss: 34.022953033447266 | latent_loss: 5.026482105255127 | total_loss: 19.524717330932617\n",
      "Epoch: 830 | recon_loss: 36.06825637817383 | latent_loss: 5.037838459014893 | total_loss: 20.55304718017578\n",
      "Epoch: 831 | recon_loss: 36.24226379394531 | latent_loss: 5.0475754737854 | total_loss: 20.644920349121094\n",
      "Epoch: 832 | recon_loss: 35.34294509887695 | latent_loss: 5.005709648132324 | total_loss: 20.174327850341797\n",
      "Epoch: 833 | recon_loss: 36.539615631103516 | latent_loss: 4.991330146789551 | total_loss: 20.765472412109375\n",
      "Epoch: 834 | recon_loss: 39.152366638183594 | latent_loss: 4.977887153625488 | total_loss: 22.065126419067383\n",
      "Epoch: 835 | recon_loss: 35.37184143066406 | latent_loss: 4.9926934242248535 | total_loss: 20.182268142700195\n",
      "Epoch: 836 | recon_loss: 35.95275115966797 | latent_loss: 4.965818405151367 | total_loss: 20.459285736083984\n",
      "Epoch: 837 | recon_loss: 35.768497467041016 | latent_loss: 4.978541374206543 | total_loss: 20.373519897460938\n",
      "Epoch: 838 | recon_loss: 35.48623275756836 | latent_loss: 4.985126495361328 | total_loss: 20.235679626464844\n",
      "Epoch: 839 | recon_loss: 34.64875030517578 | latent_loss: 4.98365592956543 | total_loss: 19.816204071044922\n",
      "Epoch: 840 | recon_loss: 37.9759407043457 | latent_loss: 4.9995646476745605 | total_loss: 21.48775291442871\n",
      "Epoch: 841 | recon_loss: 38.43321990966797 | latent_loss: 5.003676891326904 | total_loss: 21.718448638916016\n",
      "Epoch: 842 | recon_loss: 37.51731872558594 | latent_loss: 5.004382133483887 | total_loss: 21.26085090637207\n",
      "Epoch: 843 | recon_loss: 35.60072326660156 | latent_loss: 5.007636070251465 | total_loss: 20.304180145263672\n",
      "Epoch: 844 | recon_loss: 37.836490631103516 | latent_loss: 5.018033027648926 | total_loss: 21.427261352539062\n",
      "Epoch: 845 | recon_loss: 34.30027770996094 | latent_loss: 5.018993377685547 | total_loss: 19.659635543823242\n",
      "Epoch: 846 | recon_loss: 36.30735778808594 | latent_loss: 5.0109453201293945 | total_loss: 20.659151077270508\n",
      "Epoch: 847 | recon_loss: 36.69272994995117 | latent_loss: 5.013307571411133 | total_loss: 20.85301971435547\n",
      "Epoch: 848 | recon_loss: 35.19763946533203 | latent_loss: 4.99356746673584 | total_loss: 20.095603942871094\n",
      "Epoch: 849 | recon_loss: 34.13264846801758 | latent_loss: 4.996688365936279 | total_loss: 19.564668655395508\n",
      "Epoch: 850 | recon_loss: 36.77656555175781 | latent_loss: 4.978227615356445 | total_loss: 20.877395629882812\n",
      "Epoch: 851 | recon_loss: 33.0462760925293 | latent_loss: 4.975010395050049 | total_loss: 19.010643005371094\n",
      "Epoch: 852 | recon_loss: 34.44837951660156 | latent_loss: 4.984064102172852 | total_loss: 19.71622085571289\n",
      "Epoch: 853 | recon_loss: 34.85430908203125 | latent_loss: 4.980034351348877 | total_loss: 19.917171478271484\n",
      "Epoch: 854 | recon_loss: 35.8046760559082 | latent_loss: 4.975172519683838 | total_loss: 20.389925003051758\n",
      "Epoch: 855 | recon_loss: 35.43324279785156 | latent_loss: 4.9593024253845215 | total_loss: 20.196271896362305\n",
      "Epoch: 856 | recon_loss: 39.65395736694336 | latent_loss: 4.934144973754883 | total_loss: 22.294052124023438\n",
      "Epoch: 857 | recon_loss: 36.982479095458984 | latent_loss: 4.931326389312744 | total_loss: 20.9569034576416\n",
      "Epoch: 858 | recon_loss: 37.54579544067383 | latent_loss: 4.944129467010498 | total_loss: 21.244962692260742\n",
      "Epoch: 859 | recon_loss: 36.6912956237793 | latent_loss: 4.9371113777160645 | total_loss: 20.8142032623291\n",
      "Epoch: 860 | recon_loss: 34.18783187866211 | latent_loss: 4.948487758636475 | total_loss: 19.568159103393555\n",
      "Epoch: 861 | recon_loss: 34.59329605102539 | latent_loss: 4.981954574584961 | total_loss: 19.78762435913086\n",
      "Epoch: 862 | recon_loss: 36.57859420776367 | latent_loss: 4.988004684448242 | total_loss: 20.78329849243164\n",
      "Epoch: 863 | recon_loss: 34.62174987792969 | latent_loss: 4.981945991516113 | total_loss: 19.801847457885742\n",
      "Epoch: 864 | recon_loss: 34.637996673583984 | latent_loss: 4.97099494934082 | total_loss: 19.80449676513672\n",
      "Epoch: 865 | recon_loss: 35.5428352355957 | latent_loss: 4.965338706970215 | total_loss: 20.254087448120117\n",
      "Epoch: 866 | recon_loss: 36.04597473144531 | latent_loss: 4.932248115539551 | total_loss: 20.489110946655273\n",
      "Epoch: 867 | recon_loss: 34.26388168334961 | latent_loss: 4.933576583862305 | total_loss: 19.59872817993164\n",
      "Epoch: 868 | recon_loss: 35.58418273925781 | latent_loss: 4.947742938995361 | total_loss: 20.265962600708008\n",
      "Epoch: 869 | recon_loss: 38.7018928527832 | latent_loss: 4.943370819091797 | total_loss: 21.8226318359375\n",
      "Epoch: 870 | recon_loss: 38.03784942626953 | latent_loss: 4.964941501617432 | total_loss: 21.50139617919922\n",
      "Epoch: 871 | recon_loss: 34.225555419921875 | latent_loss: 4.982543468475342 | total_loss: 19.604049682617188\n",
      "Epoch: 872 | recon_loss: 34.44664001464844 | latent_loss: 4.960225582122803 | total_loss: 19.703432083129883\n",
      "Epoch: 873 | recon_loss: 38.26654815673828 | latent_loss: 4.9423909187316895 | total_loss: 21.604469299316406\n",
      "Epoch: 874 | recon_loss: 36.39591598510742 | latent_loss: 4.941830635070801 | total_loss: 20.668872833251953\n",
      "Epoch: 875 | recon_loss: 35.880130767822266 | latent_loss: 4.9293317794799805 | total_loss: 20.40473175048828\n",
      "Epoch: 876 | recon_loss: 36.90342712402344 | latent_loss: 4.911802768707275 | total_loss: 20.907615661621094\n",
      "Epoch: 877 | recon_loss: 35.84545135498047 | latent_loss: 4.907679557800293 | total_loss: 20.37656593322754\n",
      "Epoch: 878 | recon_loss: 34.55854797363281 | latent_loss: 4.925197601318359 | total_loss: 19.741872787475586\n",
      "Epoch: 879 | recon_loss: 35.64353942871094 | latent_loss: 4.927725315093994 | total_loss: 20.285633087158203\n",
      "Epoch: 880 | recon_loss: 37.47184753417969 | latent_loss: 4.948375225067139 | total_loss: 21.210111618041992\n",
      "Epoch: 881 | recon_loss: 35.5128059387207 | latent_loss: 4.9325127601623535 | total_loss: 20.222660064697266\n",
      "Epoch: 882 | recon_loss: 36.67207336425781 | latent_loss: 4.921634674072266 | total_loss: 20.79685401916504\n",
      "Epoch: 883 | recon_loss: 38.025421142578125 | latent_loss: 4.915933132171631 | total_loss: 21.47067642211914\n",
      "Epoch: 884 | recon_loss: 37.575462341308594 | latent_loss: 4.939562797546387 | total_loss: 21.25751304626465\n",
      "Epoch: 885 | recon_loss: 37.07393264770508 | latent_loss: 4.96548318862915 | total_loss: 21.01970863342285\n",
      "Epoch: 886 | recon_loss: 36.20692443847656 | latent_loss: 4.979074478149414 | total_loss: 20.592998504638672\n",
      "Epoch: 887 | recon_loss: 35.39552307128906 | latent_loss: 4.996006488800049 | total_loss: 20.195764541625977\n",
      "Epoch: 888 | recon_loss: 34.85935592651367 | latent_loss: 4.995306015014648 | total_loss: 19.927330017089844\n",
      "Epoch: 889 | recon_loss: 35.19911193847656 | latent_loss: 5.001598358154297 | total_loss: 20.10035514831543\n",
      "Epoch: 890 | recon_loss: 39.03730773925781 | latent_loss: 4.979475021362305 | total_loss: 22.008392333984375\n",
      "Epoch: 891 | recon_loss: 36.579952239990234 | latent_loss: 4.949788570404053 | total_loss: 20.764869689941406\n",
      "Epoch: 892 | recon_loss: 34.27583694458008 | latent_loss: 4.943093776702881 | total_loss: 19.609464645385742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 893 | recon_loss: 34.44292068481445 | latent_loss: 4.905273914337158 | total_loss: 19.674097061157227\n",
      "Epoch: 894 | recon_loss: 37.87752914428711 | latent_loss: 4.922568321228027 | total_loss: 21.400049209594727\n",
      "Epoch: 895 | recon_loss: 34.61641311645508 | latent_loss: 4.927711486816406 | total_loss: 19.772062301635742\n",
      "Epoch: 896 | recon_loss: 35.919376373291016 | latent_loss: 4.925336837768555 | total_loss: 20.42235565185547\n",
      "Epoch: 897 | recon_loss: 34.97077941894531 | latent_loss: 4.9267354011535645 | total_loss: 19.94875717163086\n",
      "Epoch: 898 | recon_loss: 37.142005920410156 | latent_loss: 4.9185285568237305 | total_loss: 21.0302677154541\n",
      "Epoch: 899 | recon_loss: 35.51011276245117 | latent_loss: 4.945797443389893 | total_loss: 20.227954864501953\n",
      "Epoch: 900 | recon_loss: 35.937110900878906 | latent_loss: 4.984086990356445 | total_loss: 20.46059799194336\n",
      "Epoch: 901 | recon_loss: 33.291622161865234 | latent_loss: 5.006772041320801 | total_loss: 19.14919662475586\n",
      "Epoch: 902 | recon_loss: 33.532073974609375 | latent_loss: 4.979288578033447 | total_loss: 19.25568199157715\n",
      "Epoch: 903 | recon_loss: 41.32097244262695 | latent_loss: 4.935087203979492 | total_loss: 23.128028869628906\n",
      "Epoch: 904 | recon_loss: 33.27958297729492 | latent_loss: 4.898782253265381 | total_loss: 19.089181900024414\n",
      "Epoch: 905 | recon_loss: 38.99003219604492 | latent_loss: 4.902466297149658 | total_loss: 21.94624900817871\n",
      "Epoch: 906 | recon_loss: 35.647674560546875 | latent_loss: 4.919560432434082 | total_loss: 20.28361701965332\n",
      "Epoch: 907 | recon_loss: 36.77183532714844 | latent_loss: 4.911447525024414 | total_loss: 20.84164047241211\n",
      "Epoch: 908 | recon_loss: 37.67723083496094 | latent_loss: 4.915313243865967 | total_loss: 21.29627227783203\n",
      "Epoch: 909 | recon_loss: 34.916805267333984 | latent_loss: 4.907682418823242 | total_loss: 19.912242889404297\n",
      "Epoch: 910 | recon_loss: 33.73768615722656 | latent_loss: 4.903395175933838 | total_loss: 19.320541381835938\n",
      "Epoch: 911 | recon_loss: 36.63629913330078 | latent_loss: 4.9191203117370605 | total_loss: 20.7777099609375\n",
      "Epoch: 912 | recon_loss: 35.86393737792969 | latent_loss: 4.946577072143555 | total_loss: 20.405258178710938\n",
      "Epoch: 913 | recon_loss: 33.41823196411133 | latent_loss: 4.961226463317871 | total_loss: 19.189729690551758\n",
      "Epoch: 914 | recon_loss: 34.98194122314453 | latent_loss: 4.962042808532715 | total_loss: 19.97199249267578\n",
      "Epoch: 915 | recon_loss: 38.132686614990234 | latent_loss: 4.967589855194092 | total_loss: 21.550138473510742\n",
      "Epoch: 916 | recon_loss: 37.58041763305664 | latent_loss: 4.940922260284424 | total_loss: 21.260669708251953\n",
      "Epoch: 917 | recon_loss: 34.41442108154297 | latent_loss: 4.927956581115723 | total_loss: 19.671188354492188\n",
      "Epoch: 918 | recon_loss: 33.34151077270508 | latent_loss: 4.905390739440918 | total_loss: 19.123451232910156\n",
      "Epoch: 919 | recon_loss: 37.0168342590332 | latent_loss: 4.886848449707031 | total_loss: 20.951841354370117\n",
      "Epoch: 920 | recon_loss: 38.79304504394531 | latent_loss: 4.882296562194824 | total_loss: 21.837671279907227\n",
      "Epoch: 921 | recon_loss: 36.97206497192383 | latent_loss: 4.886782646179199 | total_loss: 20.929424285888672\n",
      "Epoch: 922 | recon_loss: 35.80882263183594 | latent_loss: 4.898684024810791 | total_loss: 20.3537540435791\n",
      "Epoch: 923 | recon_loss: 36.11543655395508 | latent_loss: 4.9186224937438965 | total_loss: 20.51702880859375\n",
      "Epoch: 924 | recon_loss: 35.496917724609375 | latent_loss: 5.0128889083862305 | total_loss: 20.25490379333496\n",
      "Epoch: 925 | recon_loss: 33.070335388183594 | latent_loss: 5.031612396240234 | total_loss: 19.050973892211914\n",
      "Epoch: 926 | recon_loss: 35.83906173706055 | latent_loss: 4.960309982299805 | total_loss: 20.39968490600586\n",
      "Epoch: 927 | recon_loss: 35.722476959228516 | latent_loss: 4.9371337890625 | total_loss: 20.329805374145508\n",
      "Epoch: 928 | recon_loss: 33.47294616699219 | latent_loss: 4.9473137855529785 | total_loss: 19.21013069152832\n",
      "Epoch: 929 | recon_loss: 36.14651107788086 | latent_loss: 4.984565258026123 | total_loss: 20.56553840637207\n",
      "Epoch: 930 | recon_loss: 35.11818313598633 | latent_loss: 5.008600234985352 | total_loss: 20.063392639160156\n",
      "Epoch: 931 | recon_loss: 37.61783218383789 | latent_loss: 4.9597859382629395 | total_loss: 21.288808822631836\n",
      "Epoch: 932 | recon_loss: 33.75368881225586 | latent_loss: 4.95320987701416 | total_loss: 19.35344886779785\n",
      "Epoch: 933 | recon_loss: 37.74742126464844 | latent_loss: 4.981915473937988 | total_loss: 21.364667892456055\n",
      "Epoch: 934 | recon_loss: 36.8580322265625 | latent_loss: 4.99893856048584 | total_loss: 20.928485870361328\n",
      "Epoch: 935 | recon_loss: 37.923622131347656 | latent_loss: 4.996064186096191 | total_loss: 21.459842681884766\n",
      "Epoch: 936 | recon_loss: 37.63794708251953 | latent_loss: 5.073685169219971 | total_loss: 21.355815887451172\n",
      "Epoch: 937 | recon_loss: 35.39460754394531 | latent_loss: 5.073947429656982 | total_loss: 20.234277725219727\n",
      "Epoch: 938 | recon_loss: 36.39223098754883 | latent_loss: 5.04519510269165 | total_loss: 20.718713760375977\n",
      "Epoch: 939 | recon_loss: 36.38243103027344 | latent_loss: 4.920999050140381 | total_loss: 20.651714324951172\n",
      "Epoch: 940 | recon_loss: 34.82619857788086 | latent_loss: 4.909224987030029 | total_loss: 19.867712020874023\n",
      "Epoch: 941 | recon_loss: 34.74204635620117 | latent_loss: 4.933743476867676 | total_loss: 19.837894439697266\n",
      "Epoch: 942 | recon_loss: 37.25773620605469 | latent_loss: 4.899396896362305 | total_loss: 21.078567504882812\n",
      "Epoch: 943 | recon_loss: 40.46748733520508 | latent_loss: 4.941669464111328 | total_loss: 22.704578399658203\n",
      "Epoch: 944 | recon_loss: 35.55706024169922 | latent_loss: 4.962247848510742 | total_loss: 20.259654998779297\n",
      "Epoch: 945 | recon_loss: 37.648250579833984 | latent_loss: 4.9475932121276855 | total_loss: 21.297922134399414\n",
      "Epoch: 946 | recon_loss: 37.780128479003906 | latent_loss: 4.959869861602783 | total_loss: 21.369998931884766\n",
      "Epoch: 947 | recon_loss: 36.228919982910156 | latent_loss: 4.929532051086426 | total_loss: 20.579225540161133\n",
      "Epoch: 948 | recon_loss: 36.63916015625 | latent_loss: 4.938723087310791 | total_loss: 20.788942337036133\n",
      "Epoch: 949 | recon_loss: 33.5101318359375 | latent_loss: 4.9128923416137695 | total_loss: 19.211511611938477\n",
      "Epoch: 950 | recon_loss: 36.33406448364258 | latent_loss: 4.933938503265381 | total_loss: 20.634000778198242\n",
      "Epoch: 951 | recon_loss: 35.11323928833008 | latent_loss: 4.990239143371582 | total_loss: 20.051738739013672\n",
      "Epoch: 952 | recon_loss: 37.113189697265625 | latent_loss: 4.999395847320557 | total_loss: 21.056293487548828\n",
      "Epoch: 953 | recon_loss: 33.97694778442383 | latent_loss: 4.958120822906494 | total_loss: 19.4675350189209\n",
      "Epoch: 954 | recon_loss: 36.71434020996094 | latent_loss: 4.9151835441589355 | total_loss: 20.814762115478516\n",
      "Epoch: 955 | recon_loss: 39.47629928588867 | latent_loss: 4.906159400939941 | total_loss: 22.19122886657715\n",
      "Epoch: 956 | recon_loss: 34.18040084838867 | latent_loss: 4.9346537590026855 | total_loss: 19.557527542114258\n",
      "Epoch: 957 | recon_loss: 32.75822067260742 | latent_loss: 4.957585334777832 | total_loss: 18.85790252685547\n",
      "Epoch: 958 | recon_loss: 39.20246887207031 | latent_loss: 4.98982048034668 | total_loss: 22.096145629882812\n",
      "Epoch: 959 | recon_loss: 37.23876953125 | latent_loss: 4.9743852615356445 | total_loss: 21.106576919555664\n",
      "Epoch: 960 | recon_loss: 35.75886917114258 | latent_loss: 5.0107197761535645 | total_loss: 20.384794235229492\n",
      "Epoch: 961 | recon_loss: 38.38893127441406 | latent_loss: 4.963882923126221 | total_loss: 21.676406860351562\n",
      "Epoch: 962 | recon_loss: 37.265750885009766 | latent_loss: 4.955408573150635 | total_loss: 21.110580444335938\n",
      "Epoch: 963 | recon_loss: 36.39579772949219 | latent_loss: 4.973082065582275 | total_loss: 20.68444061279297\n",
      "Epoch: 964 | recon_loss: 35.639060974121094 | latent_loss: 4.9864583015441895 | total_loss: 20.312759399414062\n",
      "Epoch: 965 | recon_loss: 38.318790435791016 | latent_loss: 4.979842662811279 | total_loss: 21.649316787719727\n",
      "Epoch: 966 | recon_loss: 38.67766571044922 | latent_loss: 5.0426716804504395 | total_loss: 21.86016845703125\n",
      "Epoch: 967 | recon_loss: 36.82706069946289 | latent_loss: 4.97962760925293 | total_loss: 20.903343200683594\n",
      "Epoch: 968 | recon_loss: 37.42069625854492 | latent_loss: 4.975687026977539 | total_loss: 21.198192596435547\n",
      "Epoch: 969 | recon_loss: 37.37750244140625 | latent_loss: 5.007333755493164 | total_loss: 21.19241714477539\n",
      "Epoch: 970 | recon_loss: 37.98197937011719 | latent_loss: 5.0897417068481445 | total_loss: 21.535860061645508\n",
      "Epoch: 971 | recon_loss: 38.40079116821289 | latent_loss: 5.067815780639648 | total_loss: 21.734302520751953\n",
      "Epoch: 972 | recon_loss: 37.12327575683594 | latent_loss: 5.028515815734863 | total_loss: 21.075895309448242\n",
      "Epoch: 973 | recon_loss: 36.77223205566406 | latent_loss: 4.967855453491211 | total_loss: 20.870044708251953\n",
      "Epoch: 974 | recon_loss: 36.139244079589844 | latent_loss: 4.939639091491699 | total_loss: 20.53944206237793\n",
      "Epoch: 975 | recon_loss: 33.38684844970703 | latent_loss: 4.950215816497803 | total_loss: 19.16853141784668\n",
      "Epoch: 976 | recon_loss: 34.79294967651367 | latent_loss: 4.960535526275635 | total_loss: 19.87674331665039\n",
      "Epoch: 977 | recon_loss: 34.32643127441406 | latent_loss: 4.991570472717285 | total_loss: 19.659000396728516\n",
      "Epoch: 978 | recon_loss: 37.81908416748047 | latent_loss: 4.971100330352783 | total_loss: 21.395092010498047\n",
      "Epoch: 979 | recon_loss: 33.1697998046875 | latent_loss: 4.961761474609375 | total_loss: 19.065780639648438\n",
      "Epoch: 980 | recon_loss: 36.3842658996582 | latent_loss: 4.937525272369385 | total_loss: 20.66089630126953\n",
      "Epoch: 981 | recon_loss: 38.352935791015625 | latent_loss: 4.956231117248535 | total_loss: 21.654582977294922\n",
      "Epoch: 982 | recon_loss: 35.33183288574219 | latent_loss: 4.9997382164001465 | total_loss: 20.16578483581543\n",
      "Epoch: 983 | recon_loss: 36.92304992675781 | latent_loss: 5.028707504272461 | total_loss: 20.975879669189453\n",
      "Epoch: 984 | recon_loss: 37.62715148925781 | latent_loss: 5.036742687225342 | total_loss: 21.331947326660156\n",
      "Epoch: 985 | recon_loss: 37.50138473510742 | latent_loss: 4.977064609527588 | total_loss: 21.239225387573242\n",
      "Epoch: 986 | recon_loss: 34.043724060058594 | latent_loss: 4.958163261413574 | total_loss: 19.500944137573242\n",
      "Epoch: 987 | recon_loss: 32.850677490234375 | latent_loss: 4.965738773345947 | total_loss: 18.9082088470459\n",
      "Epoch: 988 | recon_loss: 36.72801971435547 | latent_loss: 5.017990589141846 | total_loss: 20.873004913330078\n",
      "Epoch: 989 | recon_loss: 36.256324768066406 | latent_loss: 5.075104236602783 | total_loss: 20.665714263916016\n",
      "Epoch: 990 | recon_loss: 36.11424255371094 | latent_loss: 5.039544582366943 | total_loss: 20.576892852783203\n",
      "Epoch: 991 | recon_loss: 36.2080192565918 | latent_loss: 5.025960922241211 | total_loss: 20.616989135742188\n",
      "Epoch: 992 | recon_loss: 39.58370590209961 | latent_loss: 5.010609149932861 | total_loss: 22.297157287597656\n",
      "Epoch: 993 | recon_loss: 35.237998962402344 | latent_loss: 4.952849864959717 | total_loss: 20.09542465209961\n",
      "Epoch: 994 | recon_loss: 35.478816986083984 | latent_loss: 4.883665561676025 | total_loss: 20.181241989135742\n",
      "Epoch: 995 | recon_loss: 37.12267303466797 | latent_loss: 4.8892035484313965 | total_loss: 21.005937576293945\n",
      "Epoch: 996 | recon_loss: 38.054325103759766 | latent_loss: 4.899878025054932 | total_loss: 21.477102279663086\n",
      "Epoch: 997 | recon_loss: 35.08817672729492 | latent_loss: 4.8983683586120605 | total_loss: 19.99327278137207\n",
      "Epoch: 998 | recon_loss: 34.31834411621094 | latent_loss: 4.980975151062012 | total_loss: 19.649660110473633\n",
      "Epoch: 999 | recon_loss: 38.20152282714844 | latent_loss: 4.9418864250183105 | total_loss: 21.571704864501953\n",
      "Epoch: 1000 | recon_loss: 36.51789855957031 | latent_loss: 4.927171230316162 | total_loss: 20.7225341796875\n",
      "Epoch: 1001 | recon_loss: 36.05466842651367 | latent_loss: 4.923383712768555 | total_loss: 20.489025115966797\n",
      "Epoch: 1002 | recon_loss: 36.52075958251953 | latent_loss: 4.986807346343994 | total_loss: 20.7537841796875\n",
      "Epoch: 1003 | recon_loss: 34.00355911254883 | latent_loss: 5.042203426361084 | total_loss: 19.52288055419922\n",
      "Epoch: 1004 | recon_loss: 34.51886749267578 | latent_loss: 4.994527339935303 | total_loss: 19.756696701049805\n",
      "Epoch: 1005 | recon_loss: 36.5738525390625 | latent_loss: 4.929194927215576 | total_loss: 20.751523971557617\n",
      "Epoch: 1006 | recon_loss: 35.751251220703125 | latent_loss: 4.92830753326416 | total_loss: 20.339778900146484\n",
      "Epoch: 1007 | recon_loss: 36.23735809326172 | latent_loss: 4.9515204429626465 | total_loss: 20.594438552856445\n",
      "Epoch: 1008 | recon_loss: 35.822113037109375 | latent_loss: 4.922122478485107 | total_loss: 20.37211799621582\n",
      "Epoch: 1009 | recon_loss: 36.19221878051758 | latent_loss: 4.8912434577941895 | total_loss: 20.541730880737305\n",
      "Epoch: 1010 | recon_loss: 34.50535583496094 | latent_loss: 4.889413833618164 | total_loss: 19.697383880615234\n",
      "Epoch: 1011 | recon_loss: 36.878597259521484 | latent_loss: 4.892805099487305 | total_loss: 20.885700225830078\n",
      "Epoch: 1012 | recon_loss: 34.812217712402344 | latent_loss: 4.9057135581970215 | total_loss: 19.858964920043945\n",
      "Epoch: 1013 | recon_loss: 34.864139556884766 | latent_loss: 4.916448593139648 | total_loss: 19.89029312133789\n",
      "Epoch: 1014 | recon_loss: 34.64362716674805 | latent_loss: 4.946523666381836 | total_loss: 19.795074462890625\n",
      "Epoch: 1015 | recon_loss: 35.90657424926758 | latent_loss: 4.937665939331055 | total_loss: 20.422119140625\n",
      "Epoch: 1016 | recon_loss: 37.42100143432617 | latent_loss: 4.954990386962891 | total_loss: 21.18799591064453\n",
      "Epoch: 1017 | recon_loss: 39.06190872192383 | latent_loss: 4.942842960357666 | total_loss: 22.002376556396484\n",
      "Epoch: 1018 | recon_loss: 35.368167877197266 | latent_loss: 4.934448719024658 | total_loss: 20.151308059692383\n",
      "Epoch: 1019 | recon_loss: 34.4785041809082 | latent_loss: 4.910325527191162 | total_loss: 19.694414138793945\n",
      "Epoch: 1020 | recon_loss: 38.38032531738281 | latent_loss: 4.878396511077881 | total_loss: 21.62936019897461\n",
      "Epoch: 1021 | recon_loss: 37.03468704223633 | latent_loss: 4.8808183670043945 | total_loss: 20.957752227783203\n",
      "Epoch: 1022 | recon_loss: 35.82279968261719 | latent_loss: 4.921657085418701 | total_loss: 20.372228622436523\n",
      "Epoch: 1023 | recon_loss: 36.43411636352539 | latent_loss: 4.948117733001709 | total_loss: 20.691116333007812\n",
      "Epoch: 1024 | recon_loss: 36.20161819458008 | latent_loss: 4.9654622077941895 | total_loss: 20.583539962768555\n",
      "Epoch: 1025 | recon_loss: 39.01513671875 | latent_loss: 4.973135471343994 | total_loss: 21.994136810302734\n",
      "Epoch: 1026 | recon_loss: 34.22394943237305 | latent_loss: 4.976197242736816 | total_loss: 19.600072860717773\n",
      "Epoch: 1027 | recon_loss: 37.03605651855469 | latent_loss: 4.985154151916504 | total_loss: 21.010604858398438\n",
      "Epoch: 1028 | recon_loss: 34.278106689453125 | latent_loss: 4.9540791511535645 | total_loss: 19.616092681884766\n",
      "Epoch: 1029 | recon_loss: 37.10743713378906 | latent_loss: 4.961030960083008 | total_loss: 21.03423309326172\n",
      "Epoch: 1030 | recon_loss: 33.80827331542969 | latent_loss: 4.991205215454102 | total_loss: 19.399738311767578\n",
      "Epoch: 1031 | recon_loss: 36.672698974609375 | latent_loss: 4.986336708068848 | total_loss: 20.829517364501953\n",
      "Epoch: 1032 | recon_loss: 34.66354751586914 | latent_loss: 4.979903697967529 | total_loss: 19.821725845336914\n",
      "Epoch: 1033 | recon_loss: 36.22795486450195 | latent_loss: 4.9825544357299805 | total_loss: 20.605255126953125\n",
      "Epoch: 1034 | recon_loss: 34.32058334350586 | latent_loss: 4.905138969421387 | total_loss: 19.61286163330078\n",
      "Epoch: 1035 | recon_loss: 37.61898422241211 | latent_loss: 4.885836601257324 | total_loss: 21.252410888671875\n",
      "Epoch: 1036 | recon_loss: 35.2803955078125 | latent_loss: 4.8940110206604 | total_loss: 20.087203979492188\n",
      "Epoch: 1037 | recon_loss: 36.6692008972168 | latent_loss: 4.914422035217285 | total_loss: 20.791810989379883\n",
      "Epoch: 1038 | recon_loss: 37.1159553527832 | latent_loss: 4.886749744415283 | total_loss: 21.001352310180664\n",
      "Epoch: 1039 | recon_loss: 40.3660774230957 | latent_loss: 4.926818370819092 | total_loss: 22.646448135375977\n",
      "Epoch: 1040 | recon_loss: 37.759822845458984 | latent_loss: 4.958127975463867 | total_loss: 21.35897445678711\n",
      "Epoch: 1041 | recon_loss: 40.02534484863281 | latent_loss: 4.948530197143555 | total_loss: 22.4869384765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1042 | recon_loss: 34.080589294433594 | latent_loss: 4.919166088104248 | total_loss: 19.4998779296875\n",
      "Epoch: 1043 | recon_loss: 36.5315055847168 | latent_loss: 4.89182710647583 | total_loss: 20.711666107177734\n",
      "Epoch: 1044 | recon_loss: 36.04915237426758 | latent_loss: 4.878923416137695 | total_loss: 20.464038848876953\n",
      "Epoch: 1045 | recon_loss: 35.72938537597656 | latent_loss: 4.916132926940918 | total_loss: 20.3227596282959\n",
      "Epoch: 1046 | recon_loss: 37.08552932739258 | latent_loss: 4.8939948081970215 | total_loss: 20.989761352539062\n",
      "Epoch: 1047 | recon_loss: 36.25204849243164 | latent_loss: 4.885395526885986 | total_loss: 20.568721771240234\n",
      "Epoch: 1048 | recon_loss: 35.94537353515625 | latent_loss: 4.943177223205566 | total_loss: 20.44427490234375\n",
      "Epoch: 1049 | recon_loss: 38.44001770019531 | latent_loss: 4.880741119384766 | total_loss: 21.66037940979004\n",
      "Epoch: 1050 | recon_loss: 36.61647415161133 | latent_loss: 4.874202251434326 | total_loss: 20.745338439941406\n",
      "Epoch: 1051 | recon_loss: 38.88829803466797 | latent_loss: 4.87364387512207 | total_loss: 21.880970001220703\n",
      "Epoch: 1052 | recon_loss: 36.04920196533203 | latent_loss: 4.927708625793457 | total_loss: 20.488454818725586\n",
      "Epoch: 1053 | recon_loss: 34.6369743347168 | latent_loss: 4.937727928161621 | total_loss: 19.787351608276367\n",
      "Epoch: 1054 | recon_loss: 36.393821716308594 | latent_loss: 4.970860958099365 | total_loss: 20.682340621948242\n",
      "Epoch: 1055 | recon_loss: 35.49244689941406 | latent_loss: 4.93237829208374 | total_loss: 20.212411880493164\n",
      "Epoch: 1056 | recon_loss: 39.29837417602539 | latent_loss: 4.904273986816406 | total_loss: 22.1013240814209\n",
      "Epoch: 1057 | recon_loss: 35.86132049560547 | latent_loss: 4.928243637084961 | total_loss: 20.39478302001953\n",
      "Epoch: 1058 | recon_loss: 36.88818359375 | latent_loss: 4.961795806884766 | total_loss: 20.924989700317383\n",
      "Epoch: 1059 | recon_loss: 36.995052337646484 | latent_loss: 5.013835430145264 | total_loss: 21.004444122314453\n",
      "Epoch: 1060 | recon_loss: 36.01822280883789 | latent_loss: 4.950891017913818 | total_loss: 20.484556198120117\n",
      "Epoch: 1061 | recon_loss: 39.09278869628906 | latent_loss: 4.889849662780762 | total_loss: 21.99131965637207\n",
      "Epoch: 1062 | recon_loss: 35.21640396118164 | latent_loss: 4.843186855316162 | total_loss: 20.029794692993164\n",
      "Epoch: 1063 | recon_loss: 36.61993408203125 | latent_loss: 4.839023590087891 | total_loss: 20.72947883605957\n",
      "Epoch: 1064 | recon_loss: 34.375694274902344 | latent_loss: 4.894898414611816 | total_loss: 19.635295867919922\n",
      "Epoch: 1065 | recon_loss: 35.89742660522461 | latent_loss: 4.944809436798096 | total_loss: 20.421117782592773\n",
      "Epoch: 1066 | recon_loss: 35.180259704589844 | latent_loss: 4.991695880889893 | total_loss: 20.08597755432129\n",
      "Epoch: 1067 | recon_loss: 37.80281066894531 | latent_loss: 4.987217426300049 | total_loss: 21.3950138092041\n",
      "Epoch: 1068 | recon_loss: 33.1195182800293 | latent_loss: 4.970944404602051 | total_loss: 19.045230865478516\n",
      "Epoch: 1069 | recon_loss: 33.212398529052734 | latent_loss: 4.91010046005249 | total_loss: 19.061248779296875\n",
      "Epoch: 1070 | recon_loss: 38.33472442626953 | latent_loss: 4.908383369445801 | total_loss: 21.621553421020508\n",
      "Epoch: 1071 | recon_loss: 35.49694061279297 | latent_loss: 4.903375625610352 | total_loss: 20.200157165527344\n",
      "Epoch: 1072 | recon_loss: 34.7846565246582 | latent_loss: 4.885870456695557 | total_loss: 19.835264205932617\n",
      "Epoch: 1073 | recon_loss: 35.88225173950195 | latent_loss: 4.887548446655273 | total_loss: 20.384899139404297\n",
      "Epoch: 1074 | recon_loss: 35.03016662597656 | latent_loss: 4.876593589782715 | total_loss: 19.953380584716797\n",
      "Epoch: 1075 | recon_loss: 35.88566589355469 | latent_loss: 4.87761116027832 | total_loss: 20.381637573242188\n",
      "Epoch: 1076 | recon_loss: 36.229942321777344 | latent_loss: 4.833009243011475 | total_loss: 20.531475067138672\n",
      "Epoch: 1077 | recon_loss: 35.82437515258789 | latent_loss: 4.841447830200195 | total_loss: 20.33291244506836\n",
      "Epoch: 1078 | recon_loss: 33.810508728027344 | latent_loss: 4.94030237197876 | total_loss: 19.37540626525879\n",
      "Epoch: 1079 | recon_loss: 34.10436248779297 | latent_loss: 4.925291061401367 | total_loss: 19.514827728271484\n",
      "Epoch: 1080 | recon_loss: 37.21788024902344 | latent_loss: 4.86067533493042 | total_loss: 21.039278030395508\n",
      "Epoch: 1081 | recon_loss: 35.98371887207031 | latent_loss: 4.8512959480285645 | total_loss: 20.41750717163086\n",
      "Epoch: 1082 | recon_loss: 37.647430419921875 | latent_loss: 4.820091247558594 | total_loss: 21.233760833740234\n",
      "Epoch: 1083 | recon_loss: 33.70758819580078 | latent_loss: 4.915042877197266 | total_loss: 19.311315536499023\n",
      "Epoch: 1084 | recon_loss: 36.43255615234375 | latent_loss: 4.919008255004883 | total_loss: 20.67578125\n",
      "Epoch: 1085 | recon_loss: 35.9446907043457 | latent_loss: 4.9282145500183105 | total_loss: 20.436452865600586\n",
      "Epoch: 1086 | recon_loss: 35.12754821777344 | latent_loss: 4.968752861022949 | total_loss: 20.04815101623535\n",
      "Epoch: 1087 | recon_loss: 34.831871032714844 | latent_loss: 4.882469654083252 | total_loss: 19.85717010498047\n",
      "Epoch: 1088 | recon_loss: 34.66741180419922 | latent_loss: 4.813812732696533 | total_loss: 19.740612030029297\n",
      "Epoch: 1089 | recon_loss: 36.7525749206543 | latent_loss: 4.790960311889648 | total_loss: 20.771766662597656\n",
      "Epoch: 1090 | recon_loss: 37.40437316894531 | latent_loss: 4.848284721374512 | total_loss: 21.12632942199707\n",
      "Epoch: 1091 | recon_loss: 34.99314498901367 | latent_loss: 4.887069225311279 | total_loss: 19.940107345581055\n",
      "Epoch: 1092 | recon_loss: 35.04092788696289 | latent_loss: 4.900045394897461 | total_loss: 19.97048568725586\n",
      "Epoch: 1093 | recon_loss: 36.50270080566406 | latent_loss: 4.885080337524414 | total_loss: 20.693889617919922\n",
      "Epoch: 1094 | recon_loss: 37.91688919067383 | latent_loss: 4.835120677947998 | total_loss: 21.376005172729492\n",
      "Epoch: 1095 | recon_loss: 36.06871032714844 | latent_loss: 4.835282325744629 | total_loss: 20.451995849609375\n",
      "Epoch: 1096 | recon_loss: 38.62755584716797 | latent_loss: 4.78785514831543 | total_loss: 21.707706451416016\n",
      "Epoch: 1097 | recon_loss: 37.70325469970703 | latent_loss: 4.782525539398193 | total_loss: 21.242889404296875\n",
      "Epoch: 1098 | recon_loss: 36.21381378173828 | latent_loss: 4.8361921310424805 | total_loss: 20.52500343322754\n",
      "Epoch: 1099 | recon_loss: 34.308074951171875 | latent_loss: 4.849996089935303 | total_loss: 19.57903480529785\n",
      "Epoch: 1100 | recon_loss: 35.48345947265625 | latent_loss: 4.834675312042236 | total_loss: 20.159067153930664\n",
      "Epoch: 1101 | recon_loss: 36.46339797973633 | latent_loss: 4.800429821014404 | total_loss: 20.631914138793945\n",
      "Epoch: 1102 | recon_loss: 36.30830764770508 | latent_loss: 4.813068866729736 | total_loss: 20.560688018798828\n",
      "Epoch: 1103 | recon_loss: 38.96704864501953 | latent_loss: 4.789849281311035 | total_loss: 21.878448486328125\n",
      "Epoch: 1104 | recon_loss: 32.19621276855469 | latent_loss: 4.811863422393799 | total_loss: 18.504037857055664\n",
      "Epoch: 1105 | recon_loss: 33.48448181152344 | latent_loss: 4.880183219909668 | total_loss: 19.18233299255371\n",
      "Epoch: 1106 | recon_loss: 36.42039108276367 | latent_loss: 4.878309726715088 | total_loss: 20.649351119995117\n",
      "Epoch: 1107 | recon_loss: 37.36131286621094 | latent_loss: 4.8703131675720215 | total_loss: 21.115812301635742\n",
      "Epoch: 1108 | recon_loss: 35.320640563964844 | latent_loss: 4.932807922363281 | total_loss: 20.126724243164062\n",
      "Epoch: 1109 | recon_loss: 33.55465316772461 | latent_loss: 4.855248928070068 | total_loss: 19.2049503326416\n",
      "Epoch: 1110 | recon_loss: 36.67774963378906 | latent_loss: 4.933094024658203 | total_loss: 20.805421829223633\n",
      "Epoch: 1111 | recon_loss: 37.45345687866211 | latent_loss: 4.966020107269287 | total_loss: 21.20973777770996\n",
      "Epoch: 1112 | recon_loss: 35.462318420410156 | latent_loss: 4.885121822357178 | total_loss: 20.17371940612793\n",
      "Epoch: 1113 | recon_loss: 37.939762115478516 | latent_loss: 4.904567718505859 | total_loss: 21.422164916992188\n",
      "Epoch: 1114 | recon_loss: 33.85261917114258 | latent_loss: 4.9184675216674805 | total_loss: 19.385543823242188\n",
      "Epoch: 1115 | recon_loss: 37.22492980957031 | latent_loss: 4.927583694458008 | total_loss: 21.076255798339844\n",
      "Epoch: 1116 | recon_loss: 33.686744689941406 | latent_loss: 4.925076007843018 | total_loss: 19.305910110473633\n",
      "Epoch: 1117 | recon_loss: 36.65727233886719 | latent_loss: 4.969700813293457 | total_loss: 20.813486099243164\n",
      "Epoch: 1118 | recon_loss: 35.54573059082031 | latent_loss: 4.9534196853637695 | total_loss: 20.249574661254883\n",
      "Epoch: 1119 | recon_loss: 32.75554275512695 | latent_loss: 4.938424587249756 | total_loss: 18.846982955932617\n",
      "Epoch: 1120 | recon_loss: 34.20554733276367 | latent_loss: 4.957186222076416 | total_loss: 19.58136749267578\n",
      "Epoch: 1121 | recon_loss: 33.043941497802734 | latent_loss: 4.872165679931641 | total_loss: 18.958053588867188\n",
      "Epoch: 1122 | recon_loss: 36.28248977661133 | latent_loss: 4.879029750823975 | total_loss: 20.580759048461914\n",
      "Epoch: 1123 | recon_loss: 33.40303039550781 | latent_loss: 4.848325252532959 | total_loss: 19.12567710876465\n",
      "Epoch: 1124 | recon_loss: 37.06857681274414 | latent_loss: 4.817243576049805 | total_loss: 20.942909240722656\n",
      "Epoch: 1125 | recon_loss: 34.83671188354492 | latent_loss: 4.8129682540893555 | total_loss: 19.824840545654297\n",
      "Epoch: 1126 | recon_loss: 37.41075897216797 | latent_loss: 4.806755542755127 | total_loss: 21.10875701904297\n",
      "Epoch: 1127 | recon_loss: 36.58064651489258 | latent_loss: 4.83897590637207 | total_loss: 20.70981216430664\n",
      "Epoch: 1128 | recon_loss: 32.89533996582031 | latent_loss: 4.810067653656006 | total_loss: 18.852703094482422\n",
      "Epoch: 1129 | recon_loss: 38.355125427246094 | latent_loss: 4.80026388168335 | total_loss: 21.577693939208984\n",
      "Epoch: 1130 | recon_loss: 35.83168029785156 | latent_loss: 4.7895989418029785 | total_loss: 20.310640335083008\n",
      "Epoch: 1131 | recon_loss: 35.802490234375 | latent_loss: 4.810754776000977 | total_loss: 20.306621551513672\n",
      "Epoch: 1132 | recon_loss: 34.72453308105469 | latent_loss: 4.8980183601379395 | total_loss: 19.811275482177734\n",
      "Epoch: 1133 | recon_loss: 35.28445053100586 | latent_loss: 4.879174709320068 | total_loss: 20.081811904907227\n",
      "Epoch: 1134 | recon_loss: 35.6569938659668 | latent_loss: 4.931835651397705 | total_loss: 20.294414520263672\n",
      "Epoch: 1135 | recon_loss: 35.64200973510742 | latent_loss: 4.898133277893066 | total_loss: 20.270071029663086\n",
      "Epoch: 1136 | recon_loss: 35.751678466796875 | latent_loss: 4.906002998352051 | total_loss: 20.328840255737305\n",
      "Epoch: 1137 | recon_loss: 35.94612121582031 | latent_loss: 4.910839557647705 | total_loss: 20.42848014831543\n",
      "Epoch: 1138 | recon_loss: 37.675533294677734 | latent_loss: 4.863271236419678 | total_loss: 21.26940155029297\n",
      "Epoch: 1139 | recon_loss: 36.61286163330078 | latent_loss: 4.846903324127197 | total_loss: 20.729883193969727\n",
      "Epoch: 1140 | recon_loss: 35.4499397277832 | latent_loss: 4.813939094543457 | total_loss: 20.131938934326172\n",
      "Epoch: 1141 | recon_loss: 34.32016372680664 | latent_loss: 4.818505764007568 | total_loss: 19.569334030151367\n",
      "Epoch: 1142 | recon_loss: 34.4102897644043 | latent_loss: 4.7807488441467285 | total_loss: 19.59552001953125\n",
      "Epoch: 1143 | recon_loss: 38.6568717956543 | latent_loss: 4.822103500366211 | total_loss: 21.739486694335938\n",
      "Epoch: 1144 | recon_loss: 34.92555236816406 | latent_loss: 4.927811622619629 | total_loss: 19.926681518554688\n",
      "Epoch: 1145 | recon_loss: 34.67055892944336 | latent_loss: 4.90457820892334 | total_loss: 19.787569046020508\n",
      "Epoch: 1146 | recon_loss: 35.030372619628906 | latent_loss: 4.875926494598389 | total_loss: 19.953149795532227\n",
      "Epoch: 1147 | recon_loss: 34.552581787109375 | latent_loss: 4.831165313720703 | total_loss: 19.69187355041504\n",
      "Epoch: 1148 | recon_loss: 34.21719741821289 | latent_loss: 4.778018951416016 | total_loss: 19.497608184814453\n",
      "Epoch: 1149 | recon_loss: 35.477874755859375 | latent_loss: 4.78063440322876 | total_loss: 20.129255294799805\n",
      "Epoch: 1150 | recon_loss: 32.97628402709961 | latent_loss: 4.852705955505371 | total_loss: 18.91449546813965\n",
      "Epoch: 1151 | recon_loss: 35.176025390625 | latent_loss: 4.861813068389893 | total_loss: 20.018918991088867\n",
      "Epoch: 1152 | recon_loss: 33.60612869262695 | latent_loss: 4.859428405761719 | total_loss: 19.232778549194336\n",
      "Epoch: 1153 | recon_loss: 34.20000076293945 | latent_loss: 4.853321075439453 | total_loss: 19.526660919189453\n",
      "Epoch: 1154 | recon_loss: 35.63465881347656 | latent_loss: 4.747880935668945 | total_loss: 20.191268920898438\n",
      "Epoch: 1155 | recon_loss: 37.36109924316406 | latent_loss: 4.708110809326172 | total_loss: 21.034605026245117\n",
      "Epoch: 1156 | recon_loss: 35.1068000793457 | latent_loss: 4.7179951667785645 | total_loss: 19.912397384643555\n",
      "Epoch: 1157 | recon_loss: 37.959590911865234 | latent_loss: 4.744062423706055 | total_loss: 21.351825714111328\n",
      "Epoch: 1158 | recon_loss: 32.79255676269531 | latent_loss: 4.731461048126221 | total_loss: 18.762008666992188\n",
      "Epoch: 1159 | recon_loss: 39.12590408325195 | latent_loss: 4.717184066772461 | total_loss: 21.92154312133789\n",
      "Epoch: 1160 | recon_loss: 32.82656478881836 | latent_loss: 4.6989898681640625 | total_loss: 18.76277732849121\n",
      "Epoch: 1161 | recon_loss: 38.99375915527344 | latent_loss: 4.7292704582214355 | total_loss: 21.861515045166016\n",
      "Epoch: 1162 | recon_loss: 36.52889633178711 | latent_loss: 4.6811676025390625 | total_loss: 20.605031967163086\n",
      "Epoch: 1163 | recon_loss: 35.926170349121094 | latent_loss: 4.670895576477051 | total_loss: 20.298532485961914\n",
      "Epoch: 1164 | recon_loss: 36.04799270629883 | latent_loss: 4.719073295593262 | total_loss: 20.383533477783203\n",
      "Epoch: 1165 | recon_loss: 35.122676849365234 | latent_loss: 4.8364386558532715 | total_loss: 19.979557037353516\n",
      "Epoch: 1166 | recon_loss: 37.13852310180664 | latent_loss: 4.833515644073486 | total_loss: 20.986019134521484\n",
      "Epoch: 1167 | recon_loss: 37.68586730957031 | latent_loss: 4.7979350090026855 | total_loss: 21.241901397705078\n",
      "Epoch: 1168 | recon_loss: 36.77721405029297 | latent_loss: 4.761436939239502 | total_loss: 20.769325256347656\n",
      "Epoch: 1169 | recon_loss: 37.05678939819336 | latent_loss: 4.796444416046143 | total_loss: 20.926616668701172\n",
      "Epoch: 1170 | recon_loss: 35.4626350402832 | latent_loss: 4.898599624633789 | total_loss: 20.180618286132812\n",
      "Epoch: 1171 | recon_loss: 34.15238571166992 | latent_loss: 4.91799259185791 | total_loss: 19.535188674926758\n",
      "Epoch: 1172 | recon_loss: 34.36321258544922 | latent_loss: 4.8297953605651855 | total_loss: 19.59650421142578\n",
      "Epoch: 1173 | recon_loss: 36.44501876831055 | latent_loss: 4.731660842895508 | total_loss: 20.588340759277344\n",
      "Epoch: 1174 | recon_loss: 39.60540771484375 | latent_loss: 4.684370517730713 | total_loss: 22.14488983154297\n",
      "Epoch: 1175 | recon_loss: 39.30731964111328 | latent_loss: 4.716668128967285 | total_loss: 22.011993408203125\n",
      "Epoch: 1176 | recon_loss: 36.689796447753906 | latent_loss: 4.748122692108154 | total_loss: 20.71895980834961\n",
      "Epoch: 1177 | recon_loss: 38.68549346923828 | latent_loss: 4.8068976402282715 | total_loss: 21.74619483947754\n",
      "Epoch: 1178 | recon_loss: 34.45567321777344 | latent_loss: 4.78981876373291 | total_loss: 19.622745513916016\n",
      "Epoch: 1179 | recon_loss: 39.04313659667969 | latent_loss: 4.774257183074951 | total_loss: 21.9086971282959\n",
      "Epoch: 1180 | recon_loss: 33.68882751464844 | latent_loss: 4.792651176452637 | total_loss: 19.240739822387695\n",
      "Epoch: 1181 | recon_loss: 35.33076477050781 | latent_loss: 4.7400641441345215 | total_loss: 20.03541374206543\n",
      "Epoch: 1182 | recon_loss: 37.99757766723633 | latent_loss: 4.731297492980957 | total_loss: 21.364437103271484\n",
      "Epoch: 1183 | recon_loss: 35.405242919921875 | latent_loss: 4.712905406951904 | total_loss: 20.05907440185547\n",
      "Epoch: 1184 | recon_loss: 36.6872444152832 | latent_loss: 4.735658168792725 | total_loss: 20.711450576782227\n",
      "Epoch: 1185 | recon_loss: 34.009307861328125 | latent_loss: 4.749275207519531 | total_loss: 19.379291534423828\n",
      "Epoch: 1186 | recon_loss: 33.93403625488281 | latent_loss: 4.6855149269104 | total_loss: 19.309776306152344\n",
      "Epoch: 1187 | recon_loss: 35.213069915771484 | latent_loss: 4.686080455780029 | total_loss: 19.949575424194336\n",
      "Epoch: 1188 | recon_loss: 36.76914978027344 | latent_loss: 4.759520530700684 | total_loss: 20.76433563232422\n",
      "Epoch: 1189 | recon_loss: 33.38240432739258 | latent_loss: 4.722081184387207 | total_loss: 19.052242279052734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1190 | recon_loss: 37.07209777832031 | latent_loss: 4.719696521759033 | total_loss: 20.895896911621094\n",
      "Epoch: 1191 | recon_loss: 34.91484451293945 | latent_loss: 4.736621856689453 | total_loss: 19.825733184814453\n",
      "Epoch: 1192 | recon_loss: 33.0937385559082 | latent_loss: 4.751407623291016 | total_loss: 18.92257308959961\n",
      "Epoch: 1193 | recon_loss: 34.407569885253906 | latent_loss: 4.789690017700195 | total_loss: 19.598628997802734\n",
      "Epoch: 1194 | recon_loss: 35.4616584777832 | latent_loss: 4.764329433441162 | total_loss: 20.112993240356445\n",
      "Epoch: 1195 | recon_loss: 32.9072380065918 | latent_loss: 4.7401628494262695 | total_loss: 18.823699951171875\n",
      "Epoch: 1196 | recon_loss: 34.504051208496094 | latent_loss: 4.711061000823975 | total_loss: 19.607555389404297\n",
      "Epoch: 1197 | recon_loss: 35.07851028442383 | latent_loss: 4.726526260375977 | total_loss: 19.90251922607422\n",
      "Epoch: 1198 | recon_loss: 36.2088508605957 | latent_loss: 4.725525379180908 | total_loss: 20.467187881469727\n",
      "The learning rate now is: <tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=0.000125>\n",
      "Epoch: 1199 | recon_loss: 40.29854202270508 | latent_loss: 4.723385334014893 | total_loss: 22.510963439941406\n",
      "Epoch: 1200 | recon_loss: 37.018253326416016 | latent_loss: 4.709576606750488 | total_loss: 20.863914489746094\n",
      "Epoch: 1201 | recon_loss: 32.08858108520508 | latent_loss: 4.74790096282959 | total_loss: 18.418241500854492\n",
      "Epoch: 1202 | recon_loss: 36.013912200927734 | latent_loss: 4.780928134918213 | total_loss: 20.39742088317871\n",
      "Epoch: 1203 | recon_loss: 35.09724426269531 | latent_loss: 4.803016185760498 | total_loss: 19.950130462646484\n",
      "Epoch: 1204 | recon_loss: 36.63751220703125 | latent_loss: 4.76868200302124 | total_loss: 20.703096389770508\n",
      "Epoch: 1205 | recon_loss: 36.27344512939453 | latent_loss: 4.717322826385498 | total_loss: 20.495384216308594\n",
      "Epoch: 1206 | recon_loss: 34.7835693359375 | latent_loss: 4.653346538543701 | total_loss: 19.71845817565918\n",
      "Epoch: 1207 | recon_loss: 36.17839050292969 | latent_loss: 4.651733875274658 | total_loss: 20.415061950683594\n",
      "Epoch: 1208 | recon_loss: 40.46068572998047 | latent_loss: 4.662172317504883 | total_loss: 22.56142807006836\n",
      "Epoch: 1209 | recon_loss: 35.71413803100586 | latent_loss: 4.714626312255859 | total_loss: 20.21438217163086\n",
      "Epoch: 1210 | recon_loss: 34.39362716674805 | latent_loss: 4.7357635498046875 | total_loss: 19.564695358276367\n",
      "Epoch: 1211 | recon_loss: 37.98617172241211 | latent_loss: 4.713249683380127 | total_loss: 21.34971046447754\n",
      "Epoch: 1212 | recon_loss: 33.92462921142578 | latent_loss: 4.752629280090332 | total_loss: 19.3386287689209\n",
      "Epoch: 1213 | recon_loss: 36.3909912109375 | latent_loss: 4.758241653442383 | total_loss: 20.574615478515625\n",
      "Epoch: 1214 | recon_loss: 36.81947708129883 | latent_loss: 4.707099437713623 | total_loss: 20.763288497924805\n",
      "Epoch: 1215 | recon_loss: 35.89103698730469 | latent_loss: 4.701475620269775 | total_loss: 20.29625701904297\n",
      "Epoch: 1216 | recon_loss: 35.610408782958984 | latent_loss: 4.739882946014404 | total_loss: 20.175146102905273\n",
      "Epoch: 1217 | recon_loss: 34.49463653564453 | latent_loss: 4.808231353759766 | total_loss: 19.65143394470215\n",
      "Epoch: 1218 | recon_loss: 34.41698455810547 | latent_loss: 4.825106620788574 | total_loss: 19.62104606628418\n",
      "Epoch: 1219 | recon_loss: 33.529869079589844 | latent_loss: 4.763366222381592 | total_loss: 19.146617889404297\n",
      "Epoch: 1220 | recon_loss: 34.00062561035156 | latent_loss: 4.750532150268555 | total_loss: 19.375579833984375\n",
      "Epoch: 1221 | recon_loss: 38.80577087402344 | latent_loss: 4.716197490692139 | total_loss: 21.760984420776367\n",
      "Epoch: 1222 | recon_loss: 35.19046401977539 | latent_loss: 4.697436332702637 | total_loss: 19.943950653076172\n",
      "Epoch: 1223 | recon_loss: 37.03935241699219 | latent_loss: 4.69308614730835 | total_loss: 20.86621856689453\n",
      "Epoch: 1224 | recon_loss: 33.98691940307617 | latent_loss: 4.687772274017334 | total_loss: 19.337345123291016\n",
      "Epoch: 1225 | recon_loss: 35.25840377807617 | latent_loss: 4.667794227600098 | total_loss: 19.963098526000977\n",
      "Epoch: 1226 | recon_loss: 36.209266662597656 | latent_loss: 4.68927001953125 | total_loss: 20.449268341064453\n",
      "Epoch: 1227 | recon_loss: 36.47466278076172 | latent_loss: 4.698190689086914 | total_loss: 20.58642578125\n",
      "Epoch: 1228 | recon_loss: 34.736541748046875 | latent_loss: 4.670311450958252 | total_loss: 19.703426361083984\n",
      "Epoch: 1229 | recon_loss: 33.31364059448242 | latent_loss: 4.655946254730225 | total_loss: 18.984792709350586\n",
      "Epoch: 1230 | recon_loss: 35.04148864746094 | latent_loss: 4.690765857696533 | total_loss: 19.866127014160156\n",
      "Epoch: 1231 | recon_loss: 34.62440872192383 | latent_loss: 4.698852062225342 | total_loss: 19.661630630493164\n",
      "Epoch: 1232 | recon_loss: 37.381492614746094 | latent_loss: 4.690892219543457 | total_loss: 21.036191940307617\n",
      "Epoch: 1233 | recon_loss: 35.366329193115234 | latent_loss: 4.681689262390137 | total_loss: 20.024009704589844\n",
      "Epoch: 1234 | recon_loss: 35.143585205078125 | latent_loss: 4.729018688201904 | total_loss: 19.936302185058594\n",
      "Epoch: 1235 | recon_loss: 34.573856353759766 | latent_loss: 4.745866298675537 | total_loss: 19.659860610961914\n",
      "Epoch: 1236 | recon_loss: 33.25478744506836 | latent_loss: 4.778198719024658 | total_loss: 19.01649284362793\n",
      "Epoch: 1237 | recon_loss: 40.8691291809082 | latent_loss: 4.723319053649902 | total_loss: 22.79622459411621\n",
      "Epoch: 1238 | recon_loss: 35.050559997558594 | latent_loss: 4.672410488128662 | total_loss: 19.86148452758789\n",
      "Epoch: 1239 | recon_loss: 35.570316314697266 | latent_loss: 4.688943862915039 | total_loss: 20.12963104248047\n",
      "Epoch: 1240 | recon_loss: 38.52264404296875 | latent_loss: 4.680567741394043 | total_loss: 21.601606369018555\n",
      "Epoch: 1241 | recon_loss: 35.28433609008789 | latent_loss: 4.718234062194824 | total_loss: 20.001285552978516\n",
      "Epoch: 1242 | recon_loss: 35.8058967590332 | latent_loss: 4.724735736846924 | total_loss: 20.265316009521484\n",
      "Epoch: 1243 | recon_loss: 35.972782135009766 | latent_loss: 4.675930023193359 | total_loss: 20.324356079101562\n",
      "Epoch: 1244 | recon_loss: 35.261474609375 | latent_loss: 4.631173610687256 | total_loss: 19.94632339477539\n",
      "Epoch: 1245 | recon_loss: 35.698726654052734 | latent_loss: 4.6157941818237305 | total_loss: 20.15726089477539\n",
      "Epoch: 1246 | recon_loss: 34.3994140625 | latent_loss: 4.634751796722412 | total_loss: 19.51708221435547\n",
      "Epoch: 1247 | recon_loss: 33.68247985839844 | latent_loss: 4.68924617767334 | total_loss: 19.185863494873047\n",
      "Epoch: 1248 | recon_loss: 35.33087158203125 | latent_loss: 4.691850662231445 | total_loss: 20.01136016845703\n",
      "Epoch: 1249 | recon_loss: 37.03321838378906 | latent_loss: 4.678753852844238 | total_loss: 20.855985641479492\n",
      "Epoch: 1250 | recon_loss: 36.01620864868164 | latent_loss: 4.68397331237793 | total_loss: 20.35009002685547\n",
      "Epoch: 1251 | recon_loss: 36.50804138183594 | latent_loss: 4.670680046081543 | total_loss: 20.5893611907959\n",
      "Epoch: 1252 | recon_loss: 36.71519088745117 | latent_loss: 4.663321018218994 | total_loss: 20.68925666809082\n",
      "Epoch: 1253 | recon_loss: 35.93818283081055 | latent_loss: 4.666774272918701 | total_loss: 20.302478790283203\n",
      "Epoch: 1254 | recon_loss: 36.55146408081055 | latent_loss: 4.713754653930664 | total_loss: 20.632610321044922\n",
      "Epoch: 1255 | recon_loss: 37.83097457885742 | latent_loss: 4.715701103210449 | total_loss: 21.273338317871094\n",
      "Epoch: 1256 | recon_loss: 33.891319274902344 | latent_loss: 4.733822822570801 | total_loss: 19.312570571899414\n",
      "Epoch: 1257 | recon_loss: 36.512474060058594 | latent_loss: 4.709216594696045 | total_loss: 20.6108455657959\n",
      "Epoch: 1258 | recon_loss: 35.42465591430664 | latent_loss: 4.713776111602783 | total_loss: 20.069215774536133\n",
      "Epoch: 1259 | recon_loss: 34.76853942871094 | latent_loss: 4.718178749084473 | total_loss: 19.743358612060547\n",
      "Epoch: 1260 | recon_loss: 37.40066909790039 | latent_loss: 4.749786376953125 | total_loss: 21.075227737426758\n",
      "Epoch: 1261 | recon_loss: 36.14888381958008 | latent_loss: 4.7156758308410645 | total_loss: 20.432279586791992\n",
      "Epoch: 1262 | recon_loss: 37.70188522338867 | latent_loss: 4.659212112426758 | total_loss: 21.18054962158203\n",
      "Epoch: 1263 | recon_loss: 40.67351150512695 | latent_loss: 4.626473903656006 | total_loss: 22.649991989135742\n",
      "Epoch: 1264 | recon_loss: 34.64930725097656 | latent_loss: 4.659935474395752 | total_loss: 19.654621124267578\n",
      "Epoch: 1265 | recon_loss: 33.233558654785156 | latent_loss: 4.6264872550964355 | total_loss: 18.930023193359375\n",
      "Epoch: 1266 | recon_loss: 35.42647933959961 | latent_loss: 4.605320930480957 | total_loss: 20.015899658203125\n",
      "Epoch: 1267 | recon_loss: 34.77583694458008 | latent_loss: 4.608954906463623 | total_loss: 19.69239616394043\n",
      "Epoch: 1268 | recon_loss: 37.584815979003906 | latent_loss: 4.6350417137146 | total_loss: 21.109928131103516\n",
      "Epoch: 1269 | recon_loss: 36.58089065551758 | latent_loss: 4.666470050811768 | total_loss: 20.623680114746094\n",
      "Epoch: 1270 | recon_loss: 35.02783966064453 | latent_loss: 4.719720363616943 | total_loss: 19.873779296875\n",
      "Epoch: 1271 | recon_loss: 35.793827056884766 | latent_loss: 4.723534107208252 | total_loss: 20.25868034362793\n",
      "Epoch: 1272 | recon_loss: 33.75108337402344 | latent_loss: 4.66534948348999 | total_loss: 19.208215713500977\n",
      "Epoch: 1273 | recon_loss: 37.54981231689453 | latent_loss: 4.661072254180908 | total_loss: 21.10544204711914\n",
      "Epoch: 1274 | recon_loss: 37.62950134277344 | latent_loss: 4.622511386871338 | total_loss: 21.126007080078125\n",
      "Epoch: 1275 | recon_loss: 33.86198425292969 | latent_loss: 4.648506164550781 | total_loss: 19.255245208740234\n",
      "Epoch: 1276 | recon_loss: 36.053951263427734 | latent_loss: 4.6475419998168945 | total_loss: 20.350746154785156\n",
      "Epoch: 1277 | recon_loss: 35.496368408203125 | latent_loss: 4.638574123382568 | total_loss: 20.06747055053711\n",
      "Epoch: 1278 | recon_loss: 38.16014099121094 | latent_loss: 4.62583589553833 | total_loss: 21.392988204956055\n",
      "Epoch: 1279 | recon_loss: 34.096824645996094 | latent_loss: 4.61631965637207 | total_loss: 19.356571197509766\n",
      "Epoch: 1280 | recon_loss: 33.34151840209961 | latent_loss: 4.6269354820251465 | total_loss: 18.98422622680664\n",
      "Epoch: 1281 | recon_loss: 32.671180725097656 | latent_loss: 4.642249584197998 | total_loss: 18.656715393066406\n",
      "Epoch: 1282 | recon_loss: 35.597740173339844 | latent_loss: 4.629390239715576 | total_loss: 20.11356544494629\n",
      "Epoch: 1283 | recon_loss: 35.81617736816406 | latent_loss: 4.613152980804443 | total_loss: 20.214664459228516\n",
      "Epoch: 1284 | recon_loss: 35.03413009643555 | latent_loss: 4.584882736206055 | total_loss: 19.809505462646484\n",
      "Epoch: 1285 | recon_loss: 37.97451400756836 | latent_loss: 4.564879894256592 | total_loss: 21.269697189331055\n",
      "Epoch: 1286 | recon_loss: 33.98215103149414 | latent_loss: 4.569082260131836 | total_loss: 19.275615692138672\n",
      "Epoch: 1287 | recon_loss: 34.929439544677734 | latent_loss: 4.642278671264648 | total_loss: 19.785858154296875\n",
      "Epoch: 1288 | recon_loss: 36.77566146850586 | latent_loss: 4.670441150665283 | total_loss: 20.723051071166992\n",
      "Epoch: 1289 | recon_loss: 35.51753234863281 | latent_loss: 4.694115161895752 | total_loss: 20.105823516845703\n",
      "Epoch: 1290 | recon_loss: 36.584678649902344 | latent_loss: 4.712614059448242 | total_loss: 20.64864730834961\n",
      "Epoch: 1291 | recon_loss: 39.97138977050781 | latent_loss: 4.687671661376953 | total_loss: 22.329530715942383\n",
      "Epoch: 1292 | recon_loss: 31.643510818481445 | latent_loss: 4.665750026702881 | total_loss: 18.154630661010742\n",
      "Epoch: 1293 | recon_loss: 35.47751998901367 | latent_loss: 4.676284313201904 | total_loss: 20.076902389526367\n",
      "Epoch: 1294 | recon_loss: 38.48774719238281 | latent_loss: 4.671034812927246 | total_loss: 21.579391479492188\n",
      "Epoch: 1295 | recon_loss: 34.78684997558594 | latent_loss: 4.709651470184326 | total_loss: 19.74825096130371\n",
      "Epoch: 1296 | recon_loss: 35.46632385253906 | latent_loss: 4.733296871185303 | total_loss: 20.099809646606445\n",
      "Epoch: 1297 | recon_loss: 35.95644760131836 | latent_loss: 4.714710712432861 | total_loss: 20.33557891845703\n",
      "Epoch: 1298 | recon_loss: 34.443603515625 | latent_loss: 4.711085796356201 | total_loss: 19.57734489440918\n",
      "Epoch: 1299 | recon_loss: 34.10960006713867 | latent_loss: 4.671530723571777 | total_loss: 19.390565872192383\n",
      "Epoch: 1300 | recon_loss: 35.146484375 | latent_loss: 4.640133857727051 | total_loss: 19.893308639526367\n",
      "Epoch: 1301 | recon_loss: 35.00981903076172 | latent_loss: 4.669027805328369 | total_loss: 19.83942413330078\n",
      "Epoch: 1302 | recon_loss: 36.5423583984375 | latent_loss: 4.685129165649414 | total_loss: 20.61374282836914\n",
      "Epoch: 1303 | recon_loss: 34.75233459472656 | latent_loss: 4.6743483543396 | total_loss: 19.713340759277344\n",
      "Epoch: 1304 | recon_loss: 34.06683349609375 | latent_loss: 4.676159858703613 | total_loss: 19.371496200561523\n",
      "Epoch: 1305 | recon_loss: 34.20643997192383 | latent_loss: 4.6517558097839355 | total_loss: 19.42909812927246\n",
      "Epoch: 1306 | recon_loss: 36.81684875488281 | latent_loss: 4.635119438171387 | total_loss: 20.725984573364258\n",
      "Epoch: 1307 | recon_loss: 32.73102951049805 | latent_loss: 4.618859767913818 | total_loss: 18.674943923950195\n",
      "Epoch: 1308 | recon_loss: 35.81940841674805 | latent_loss: 4.595404148101807 | total_loss: 20.207406997680664\n",
      "Epoch: 1309 | recon_loss: 36.089012145996094 | latent_loss: 4.593645095825195 | total_loss: 20.341327667236328\n",
      "Epoch: 1310 | recon_loss: 34.9853401184082 | latent_loss: 4.601208686828613 | total_loss: 19.79327392578125\n",
      "Epoch: 1311 | recon_loss: 34.49732971191406 | latent_loss: 4.60471773147583 | total_loss: 19.551023483276367\n",
      "Epoch: 1312 | recon_loss: 34.828914642333984 | latent_loss: 4.617284297943115 | total_loss: 19.723098754882812\n",
      "Epoch: 1313 | recon_loss: 32.79029846191406 | latent_loss: 4.652629852294922 | total_loss: 18.721464157104492\n",
      "Epoch: 1314 | recon_loss: 35.464683532714844 | latent_loss: 4.653736114501953 | total_loss: 20.0592098236084\n",
      "Epoch: 1315 | recon_loss: 32.90586471557617 | latent_loss: 4.655134677886963 | total_loss: 18.780500411987305\n",
      "Epoch: 1316 | recon_loss: 38.538761138916016 | latent_loss: 4.641408920288086 | total_loss: 21.590084075927734\n",
      "Epoch: 1317 | recon_loss: 35.97646713256836 | latent_loss: 4.686456680297852 | total_loss: 20.331462860107422\n",
      "Epoch: 1318 | recon_loss: 37.20498275756836 | latent_loss: 4.653445243835449 | total_loss: 20.929214477539062\n",
      "Epoch: 1319 | recon_loss: 37.14240264892578 | latent_loss: 4.680363655090332 | total_loss: 20.9113826751709\n",
      "Epoch: 1320 | recon_loss: 32.9530029296875 | latent_loss: 4.645532608032227 | total_loss: 18.799266815185547\n",
      "Epoch: 1321 | recon_loss: 37.696475982666016 | latent_loss: 4.636954307556152 | total_loss: 21.166715621948242\n",
      "Epoch: 1322 | recon_loss: 33.21898651123047 | latent_loss: 4.619550704956055 | total_loss: 18.919269561767578\n",
      "Epoch: 1323 | recon_loss: 34.24049758911133 | latent_loss: 4.683730602264404 | total_loss: 19.462114334106445\n",
      "Epoch: 1324 | recon_loss: 34.25547790527344 | latent_loss: 4.628414154052734 | total_loss: 19.441946029663086\n",
      "Epoch: 1325 | recon_loss: 35.13552474975586 | latent_loss: 4.587515354156494 | total_loss: 19.861520767211914\n",
      "Epoch: 1326 | recon_loss: 33.63519287109375 | latent_loss: 4.5769758224487305 | total_loss: 19.1060848236084\n",
      "Epoch: 1327 | recon_loss: 38.79507827758789 | latent_loss: 4.60195779800415 | total_loss: 21.698518753051758\n",
      "Epoch: 1328 | recon_loss: 32.916927337646484 | latent_loss: 4.660109996795654 | total_loss: 18.78851890563965\n",
      "Epoch: 1329 | recon_loss: 36.38613510131836 | latent_loss: 4.62188720703125 | total_loss: 20.504011154174805\n",
      "Epoch: 1330 | recon_loss: 34.786861419677734 | latent_loss: 4.635067939758301 | total_loss: 19.71096420288086\n",
      "Epoch: 1331 | recon_loss: 34.92692565917969 | latent_loss: 4.6640825271606445 | total_loss: 19.795503616333008\n",
      "Epoch: 1332 | recon_loss: 37.469627380371094 | latent_loss: 4.6919755935668945 | total_loss: 21.080801010131836\n",
      "Epoch: 1333 | recon_loss: 38.7050666809082 | latent_loss: 4.634532451629639 | total_loss: 21.6697998046875\n",
      "Epoch: 1334 | recon_loss: 36.13443374633789 | latent_loss: 4.592499256134033 | total_loss: 20.363466262817383\n",
      "Epoch: 1335 | recon_loss: 36.571632385253906 | latent_loss: 4.475442886352539 | total_loss: 20.523536682128906\n",
      "Epoch: 1336 | recon_loss: 35.8659553527832 | latent_loss: 4.456406116485596 | total_loss: 20.16118049621582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1337 | recon_loss: 36.62559509277344 | latent_loss: 4.573198318481445 | total_loss: 20.599395751953125\n",
      "Epoch: 1338 | recon_loss: 32.563804626464844 | latent_loss: 4.668681621551514 | total_loss: 18.616243362426758\n",
      "Epoch: 1339 | recon_loss: 35.48129653930664 | latent_loss: 4.773203372955322 | total_loss: 20.12725067138672\n",
      "Epoch: 1340 | recon_loss: 34.99110794067383 | latent_loss: 4.770096302032471 | total_loss: 19.88060188293457\n",
      "Epoch: 1341 | recon_loss: 35.40678024291992 | latent_loss: 4.715961456298828 | total_loss: 20.061370849609375\n",
      "Epoch: 1342 | recon_loss: 36.958892822265625 | latent_loss: 4.697457313537598 | total_loss: 20.828174591064453\n",
      "Epoch: 1343 | recon_loss: 38.12636184692383 | latent_loss: 4.648176670074463 | total_loss: 21.387269973754883\n",
      "Epoch: 1344 | recon_loss: 38.871124267578125 | latent_loss: 4.608450412750244 | total_loss: 21.739788055419922\n",
      "Epoch: 1345 | recon_loss: 36.50068664550781 | latent_loss: 4.584913730621338 | total_loss: 20.542800903320312\n",
      "Epoch: 1346 | recon_loss: 35.0250129699707 | latent_loss: 4.627574920654297 | total_loss: 19.8262939453125\n",
      "Epoch: 1347 | recon_loss: 34.70149230957031 | latent_loss: 4.701806545257568 | total_loss: 19.701648712158203\n",
      "Epoch: 1348 | recon_loss: 32.538421630859375 | latent_loss: 4.679304122924805 | total_loss: 18.608863830566406\n",
      "Epoch: 1349 | recon_loss: 36.80232238769531 | latent_loss: 4.644773960113525 | total_loss: 20.723548889160156\n",
      "Epoch: 1350 | recon_loss: 36.014827728271484 | latent_loss: 4.615555286407471 | total_loss: 20.3151912689209\n",
      "Epoch: 1351 | recon_loss: 34.434444427490234 | latent_loss: 4.513026714324951 | total_loss: 19.473735809326172\n",
      "Epoch: 1352 | recon_loss: 33.85142517089844 | latent_loss: 4.531851291656494 | total_loss: 19.191638946533203\n",
      "Epoch: 1353 | recon_loss: 34.67024612426758 | latent_loss: 4.632938861846924 | total_loss: 19.651592254638672\n",
      "Epoch: 1354 | recon_loss: 33.64384460449219 | latent_loss: 4.613749980926514 | total_loss: 19.12879753112793\n",
      "Epoch: 1355 | recon_loss: 36.898826599121094 | latent_loss: 4.63587760925293 | total_loss: 20.767353057861328\n",
      "Epoch: 1356 | recon_loss: 32.62897491455078 | latent_loss: 4.6405439376831055 | total_loss: 18.6347599029541\n",
      "Epoch: 1357 | recon_loss: 34.19232940673828 | latent_loss: 4.663407802581787 | total_loss: 19.427867889404297\n",
      "Epoch: 1358 | recon_loss: 35.39683151245117 | latent_loss: 4.641377925872803 | total_loss: 20.01910400390625\n",
      "Epoch: 1359 | recon_loss: 37.33441925048828 | latent_loss: 4.629266262054443 | total_loss: 20.981842041015625\n",
      "Epoch: 1360 | recon_loss: 35.82719421386719 | latent_loss: 4.685993194580078 | total_loss: 20.256593704223633\n",
      "Epoch: 1361 | recon_loss: 36.31403350830078 | latent_loss: 4.694035530090332 | total_loss: 20.5040340423584\n",
      "Epoch: 1362 | recon_loss: 34.98587417602539 | latent_loss: 4.582527160644531 | total_loss: 19.78420066833496\n",
      "Epoch: 1363 | recon_loss: 39.3828010559082 | latent_loss: 4.574592113494873 | total_loss: 21.978696823120117\n",
      "Epoch: 1364 | recon_loss: 33.60738754272461 | latent_loss: 4.668124198913574 | total_loss: 19.13775634765625\n",
      "Epoch: 1365 | recon_loss: 35.66240310668945 | latent_loss: 4.632140636444092 | total_loss: 20.14727210998535\n",
      "Epoch: 1366 | recon_loss: 35.21754455566406 | latent_loss: 4.702582836151123 | total_loss: 19.960063934326172\n",
      "Epoch: 1367 | recon_loss: 31.998661041259766 | latent_loss: 4.761807441711426 | total_loss: 18.380233764648438\n",
      "Epoch: 1368 | recon_loss: 33.63614273071289 | latent_loss: 4.740907669067383 | total_loss: 19.188526153564453\n",
      "Epoch: 1369 | recon_loss: 37.50676727294922 | latent_loss: 4.733356475830078 | total_loss: 21.12006187438965\n",
      "Epoch: 1370 | recon_loss: 36.86064910888672 | latent_loss: 4.6196393966674805 | total_loss: 20.740144729614258\n",
      "Epoch: 1371 | recon_loss: 36.07313537597656 | latent_loss: 4.584306240081787 | total_loss: 20.328720092773438\n",
      "Epoch: 1372 | recon_loss: 35.940792083740234 | latent_loss: 4.555983543395996 | total_loss: 20.248388290405273\n",
      "Epoch: 1373 | recon_loss: 33.83324432373047 | latent_loss: 4.569552421569824 | total_loss: 19.201398849487305\n",
      "Epoch: 1374 | recon_loss: 33.51043701171875 | latent_loss: 4.567975997924805 | total_loss: 19.039207458496094\n",
      "Epoch: 1375 | recon_loss: 34.61501693725586 | latent_loss: 4.606428623199463 | total_loss: 19.6107234954834\n",
      "Epoch: 1376 | recon_loss: 34.140106201171875 | latent_loss: 4.6210222244262695 | total_loss: 19.380563735961914\n",
      "Epoch: 1377 | recon_loss: 35.9873161315918 | latent_loss: 4.613046646118164 | total_loss: 20.300182342529297\n",
      "Epoch: 1378 | recon_loss: 33.56653594970703 | latent_loss: 4.6066083908081055 | total_loss: 19.086572647094727\n",
      "Epoch: 1379 | recon_loss: 37.44932174682617 | latent_loss: 4.627956390380859 | total_loss: 21.038639068603516\n",
      "Epoch: 1380 | recon_loss: 36.76210021972656 | latent_loss: 4.670807361602783 | total_loss: 20.716453552246094\n",
      "Epoch: 1381 | recon_loss: 33.502479553222656 | latent_loss: 4.631129741668701 | total_loss: 19.066804885864258\n",
      "Epoch: 1382 | recon_loss: 33.75779724121094 | latent_loss: 4.560473442077637 | total_loss: 19.159135818481445\n",
      "Epoch: 1383 | recon_loss: 37.04606246948242 | latent_loss: 4.564272880554199 | total_loss: 20.80516815185547\n",
      "Epoch: 1384 | recon_loss: 36.79344177246094 | latent_loss: 4.551947116851807 | total_loss: 20.67269515991211\n",
      "Epoch: 1385 | recon_loss: 34.35325622558594 | latent_loss: 4.585926532745361 | total_loss: 19.46959114074707\n",
      "Epoch: 1386 | recon_loss: 35.152549743652344 | latent_loss: 4.61770486831665 | total_loss: 19.885128021240234\n",
      "Epoch: 1387 | recon_loss: 37.02455520629883 | latent_loss: 4.646769046783447 | total_loss: 20.835662841796875\n",
      "Epoch: 1388 | recon_loss: 35.076515197753906 | latent_loss: 4.64417028427124 | total_loss: 19.860342025756836\n",
      "Epoch: 1389 | recon_loss: 32.817962646484375 | latent_loss: 4.611495018005371 | total_loss: 18.71472930908203\n",
      "Epoch: 1390 | recon_loss: 33.947288513183594 | latent_loss: 4.5965895652771 | total_loss: 19.27193832397461\n",
      "Epoch: 1391 | recon_loss: 35.84352111816406 | latent_loss: 4.63389778137207 | total_loss: 20.23870849609375\n",
      "Epoch: 1392 | recon_loss: 37.31394958496094 | latent_loss: 4.618801593780518 | total_loss: 20.96637535095215\n",
      "Epoch: 1393 | recon_loss: 34.007041931152344 | latent_loss: 4.649334907531738 | total_loss: 19.328187942504883\n",
      "Epoch: 1394 | recon_loss: 34.064910888671875 | latent_loss: 4.643021106719971 | total_loss: 19.353965759277344\n",
      "Epoch: 1395 | recon_loss: 34.16956329345703 | latent_loss: 4.669278144836426 | total_loss: 19.41942024230957\n",
      "Epoch: 1396 | recon_loss: 36.23722839355469 | latent_loss: 4.667085647583008 | total_loss: 20.45215606689453\n",
      "Epoch: 1397 | recon_loss: 36.850284576416016 | latent_loss: 4.622005939483643 | total_loss: 20.73614501953125\n",
      "Epoch: 1398 | recon_loss: 34.52812194824219 | latent_loss: 4.665502548217773 | total_loss: 19.596813201904297\n",
      "Epoch: 1399 | recon_loss: 33.20985412597656 | latent_loss: 4.656315326690674 | total_loss: 18.93308448791504\n",
      "Epoch: 1400 | recon_loss: 39.47150421142578 | latent_loss: 4.6550517082214355 | total_loss: 22.063278198242188\n",
      "Epoch: 1401 | recon_loss: 32.89680099487305 | latent_loss: 4.656151294708252 | total_loss: 18.77647590637207\n",
      "Epoch: 1402 | recon_loss: 31.703725814819336 | latent_loss: 4.677992820739746 | total_loss: 18.190858840942383\n",
      "Epoch: 1403 | recon_loss: 34.08290100097656 | latent_loss: 4.6797943115234375 | total_loss: 19.38134765625\n",
      "Epoch: 1404 | recon_loss: 35.538875579833984 | latent_loss: 4.645837783813477 | total_loss: 20.092357635498047\n",
      "Epoch: 1405 | recon_loss: 35.07891845703125 | latent_loss: 4.640439510345459 | total_loss: 19.859678268432617\n",
      "Epoch: 1406 | recon_loss: 33.953125 | latent_loss: 4.650941371917725 | total_loss: 19.302032470703125\n",
      "Epoch: 1407 | recon_loss: 37.00762939453125 | latent_loss: 4.676264762878418 | total_loss: 20.841947555541992\n",
      "Epoch: 1408 | recon_loss: 36.75525665283203 | latent_loss: 4.689176082611084 | total_loss: 20.72221565246582\n",
      "Epoch: 1409 | recon_loss: 35.71443557739258 | latent_loss: 4.695202350616455 | total_loss: 20.204818725585938\n",
      "Epoch: 1410 | recon_loss: 35.189002990722656 | latent_loss: 4.611279487609863 | total_loss: 19.9001407623291\n",
      "Epoch: 1411 | recon_loss: 35.93135452270508 | latent_loss: 4.600063323974609 | total_loss: 20.265708923339844\n",
      "Epoch: 1412 | recon_loss: 32.0344352722168 | latent_loss: 4.6415886878967285 | total_loss: 18.3380126953125\n",
      "Epoch: 1413 | recon_loss: 40.94469451904297 | latent_loss: 4.6616530418396 | total_loss: 22.803173065185547\n",
      "Epoch: 1414 | recon_loss: 34.19036102294922 | latent_loss: 4.60513162612915 | total_loss: 19.397747039794922\n",
      "Epoch: 1415 | recon_loss: 35.092498779296875 | latent_loss: 4.608072757720947 | total_loss: 19.85028648376465\n",
      "Epoch: 1416 | recon_loss: 36.114158630371094 | latent_loss: 4.5975446701049805 | total_loss: 20.355852127075195\n",
      "Epoch: 1417 | recon_loss: 38.97076416015625 | latent_loss: 4.596441745758057 | total_loss: 21.78360366821289\n",
      "Epoch: 1418 | recon_loss: 35.856346130371094 | latent_loss: 4.601877212524414 | total_loss: 20.229110717773438\n",
      "Epoch: 1419 | recon_loss: 32.574275970458984 | latent_loss: 4.6782636642456055 | total_loss: 18.626270294189453\n",
      "Epoch: 1420 | recon_loss: 33.3006477355957 | latent_loss: 4.71171236038208 | total_loss: 19.006179809570312\n",
      "Epoch: 1421 | recon_loss: 32.889259338378906 | latent_loss: 4.691080093383789 | total_loss: 18.79016876220703\n",
      "Epoch: 1422 | recon_loss: 36.42140197753906 | latent_loss: 4.642760753631592 | total_loss: 20.532081604003906\n",
      "Epoch: 1423 | recon_loss: 36.873985290527344 | latent_loss: 4.60792350769043 | total_loss: 20.740955352783203\n",
      "Epoch: 1424 | recon_loss: 40.109127044677734 | latent_loss: 4.574606895446777 | total_loss: 22.341867446899414\n",
      "Epoch: 1425 | recon_loss: 37.20594024658203 | latent_loss: 4.548259735107422 | total_loss: 20.877099990844727\n",
      "Epoch: 1426 | recon_loss: 36.35401916503906 | latent_loss: 4.560068130493164 | total_loss: 20.457042694091797\n",
      "Epoch: 1427 | recon_loss: 39.77577590942383 | latent_loss: 4.588258743286133 | total_loss: 22.182018280029297\n",
      "Epoch: 1428 | recon_loss: 36.214805603027344 | latent_loss: 4.587966442108154 | total_loss: 20.401386260986328\n",
      "Epoch: 1429 | recon_loss: 33.381813049316406 | latent_loss: 4.573856830596924 | total_loss: 18.977834701538086\n",
      "Epoch: 1430 | recon_loss: 37.74541091918945 | latent_loss: 4.583610534667969 | total_loss: 21.16451072692871\n",
      "Epoch: 1431 | recon_loss: 35.05760192871094 | latent_loss: 4.6010823249816895 | total_loss: 19.829341888427734\n",
      "Epoch: 1432 | recon_loss: 32.82038116455078 | latent_loss: 4.603274345397949 | total_loss: 18.711828231811523\n",
      "Epoch: 1433 | recon_loss: 33.12699508666992 | latent_loss: 4.616597652435303 | total_loss: 18.871795654296875\n",
      "Epoch: 1434 | recon_loss: 36.78168487548828 | latent_loss: 4.617427825927734 | total_loss: 20.699556350708008\n",
      "Epoch: 1435 | recon_loss: 35.01303482055664 | latent_loss: 4.569845676422119 | total_loss: 19.791440963745117\n",
      "Epoch: 1436 | recon_loss: 35.38772201538086 | latent_loss: 4.55064582824707 | total_loss: 19.96918487548828\n",
      "Epoch: 1437 | recon_loss: 33.33893966674805 | latent_loss: 4.6145758628845215 | total_loss: 18.976757049560547\n",
      "Epoch: 1438 | recon_loss: 34.8808708190918 | latent_loss: 4.631741523742676 | total_loss: 19.756305694580078\n",
      "Epoch: 1439 | recon_loss: 33.564205169677734 | latent_loss: 4.623051166534424 | total_loss: 19.0936279296875\n",
      "Epoch: 1440 | recon_loss: 35.60200119018555 | latent_loss: 4.535362720489502 | total_loss: 20.068681716918945\n",
      "Epoch: 1441 | recon_loss: 35.538394927978516 | latent_loss: 4.569420337677002 | total_loss: 20.05390739440918\n",
      "Epoch: 1442 | recon_loss: 34.034080505371094 | latent_loss: 4.606632709503174 | total_loss: 19.320356369018555\n",
      "Epoch: 1443 | recon_loss: 33.0206184387207 | latent_loss: 4.702910423278809 | total_loss: 18.861764907836914\n",
      "Epoch: 1444 | recon_loss: 33.49659729003906 | latent_loss: 4.709822654724121 | total_loss: 19.10321044921875\n",
      "Epoch: 1445 | recon_loss: 36.91160583496094 | latent_loss: 4.772237777709961 | total_loss: 20.841922760009766\n",
      "Epoch: 1446 | recon_loss: 34.16781234741211 | latent_loss: 4.631582260131836 | total_loss: 19.399696350097656\n",
      "Epoch: 1447 | recon_loss: 34.8248405456543 | latent_loss: 4.657496452331543 | total_loss: 19.741168975830078\n",
      "Epoch: 1448 | recon_loss: 36.80363464355469 | latent_loss: 4.667873382568359 | total_loss: 20.735754013061523\n",
      "Epoch: 1449 | recon_loss: 37.48409652709961 | latent_loss: 4.686915397644043 | total_loss: 21.085506439208984\n",
      "Epoch: 1450 | recon_loss: 32.976966857910156 | latent_loss: 4.606593608856201 | total_loss: 18.791780471801758\n",
      "Epoch: 1451 | recon_loss: 35.37744903564453 | latent_loss: 4.554778575897217 | total_loss: 19.966114044189453\n",
      "Epoch: 1452 | recon_loss: 35.72141647338867 | latent_loss: 4.572495937347412 | total_loss: 20.146955490112305\n",
      "Epoch: 1453 | recon_loss: 35.7741584777832 | latent_loss: 4.578133583068848 | total_loss: 20.176145553588867\n",
      "Epoch: 1454 | recon_loss: 34.91950988769531 | latent_loss: 4.6097612380981445 | total_loss: 19.76463508605957\n",
      "Epoch: 1455 | recon_loss: 35.70381164550781 | latent_loss: 4.662863254547119 | total_loss: 20.183338165283203\n",
      "Epoch: 1456 | recon_loss: 34.17764663696289 | latent_loss: 4.6742424964904785 | total_loss: 19.425945281982422\n",
      "Epoch: 1457 | recon_loss: 36.06233215332031 | latent_loss: 4.608983039855957 | total_loss: 20.335657119750977\n",
      "Epoch: 1458 | recon_loss: 35.42953872680664 | latent_loss: 4.575632095336914 | total_loss: 20.002586364746094\n",
      "Epoch: 1459 | recon_loss: 38.43205261230469 | latent_loss: 4.648611068725586 | total_loss: 21.540332794189453\n",
      "Epoch: 1460 | recon_loss: 33.85926818847656 | latent_loss: 4.6408233642578125 | total_loss: 19.250045776367188\n",
      "Epoch: 1461 | recon_loss: 39.24943542480469 | latent_loss: 4.587306976318359 | total_loss: 21.918371200561523\n",
      "Epoch: 1462 | recon_loss: 35.28004455566406 | latent_loss: 4.588724613189697 | total_loss: 19.934385299682617\n",
      "Epoch: 1463 | recon_loss: 35.07977294921875 | latent_loss: 4.576460361480713 | total_loss: 19.82811737060547\n",
      "Epoch: 1464 | recon_loss: 35.93907165527344 | latent_loss: 4.595020771026611 | total_loss: 20.267045974731445\n",
      "Epoch: 1465 | recon_loss: 33.310874938964844 | latent_loss: 4.6192426681518555 | total_loss: 18.965059280395508\n",
      "Epoch: 1466 | recon_loss: 38.24070739746094 | latent_loss: 4.659621715545654 | total_loss: 21.450164794921875\n",
      "Epoch: 1467 | recon_loss: 36.38026809692383 | latent_loss: 4.612765312194824 | total_loss: 20.496517181396484\n",
      "Epoch: 1468 | recon_loss: 34.23440170288086 | latent_loss: 4.5521769523620605 | total_loss: 19.39328956604004\n",
      "Epoch: 1469 | recon_loss: 36.45958709716797 | latent_loss: 4.507322788238525 | total_loss: 20.483455657958984\n",
      "Epoch: 1470 | recon_loss: 34.73225402832031 | latent_loss: 4.615838527679443 | total_loss: 19.67404556274414\n",
      "Epoch: 1471 | recon_loss: 36.940834045410156 | latent_loss: 4.615936756134033 | total_loss: 20.778385162353516\n",
      "Epoch: 1472 | recon_loss: 33.188941955566406 | latent_loss: 4.617185592651367 | total_loss: 18.903064727783203\n",
      "Epoch: 1473 | recon_loss: 34.46010971069336 | latent_loss: 4.5719475746154785 | total_loss: 19.516029357910156\n",
      "Epoch: 1474 | recon_loss: 36.68484115600586 | latent_loss: 4.5059309005737305 | total_loss: 20.595386505126953\n",
      "Epoch: 1475 | recon_loss: 37.15131759643555 | latent_loss: 4.458726406097412 | total_loss: 20.805021286010742\n",
      "Epoch: 1476 | recon_loss: 36.22904968261719 | latent_loss: 4.549930572509766 | total_loss: 20.389490127563477\n",
      "Epoch: 1477 | recon_loss: 36.53530502319336 | latent_loss: 4.559910297393799 | total_loss: 20.547607421875\n",
      "Epoch: 1478 | recon_loss: 36.75785827636719 | latent_loss: 4.558708667755127 | total_loss: 20.658283233642578\n",
      "Epoch: 1479 | recon_loss: 34.73700714111328 | latent_loss: 4.595778942108154 | total_loss: 19.666393280029297\n",
      "Epoch: 1480 | recon_loss: 32.94306182861328 | latent_loss: 4.6415815353393555 | total_loss: 18.792322158813477\n",
      "Epoch: 1481 | recon_loss: 34.23719024658203 | latent_loss: 4.592813014984131 | total_loss: 19.415000915527344\n",
      "Epoch: 1482 | recon_loss: 35.0002555847168 | latent_loss: 4.580756664276123 | total_loss: 19.79050636291504\n",
      "Epoch: 1483 | recon_loss: 37.27045440673828 | latent_loss: 4.66647481918335 | total_loss: 20.968463897705078\n",
      "Epoch: 1484 | recon_loss: 35.958194732666016 | latent_loss: 4.560571193695068 | total_loss: 20.259382247924805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1485 | recon_loss: 34.93385696411133 | latent_loss: 4.499505043029785 | total_loss: 19.7166805267334\n",
      "Epoch: 1486 | recon_loss: 36.34128189086914 | latent_loss: 4.487203598022461 | total_loss: 20.414241790771484\n",
      "Epoch: 1487 | recon_loss: 37.1698112487793 | latent_loss: 4.4817399978637695 | total_loss: 20.825775146484375\n",
      "Epoch: 1488 | recon_loss: 34.819950103759766 | latent_loss: 4.479227066040039 | total_loss: 19.64958953857422\n",
      "Epoch: 1489 | recon_loss: 34.69108963012695 | latent_loss: 4.5096964836120605 | total_loss: 19.600393295288086\n",
      "Epoch: 1490 | recon_loss: 34.70750427246094 | latent_loss: 4.55980110168457 | total_loss: 19.633651733398438\n",
      "Epoch: 1491 | recon_loss: 33.6994743347168 | latent_loss: 4.557763576507568 | total_loss: 19.128618240356445\n",
      "Epoch: 1492 | recon_loss: 37.140769958496094 | latent_loss: 4.581179141998291 | total_loss: 20.86097526550293\n",
      "Epoch: 1493 | recon_loss: 33.967445373535156 | latent_loss: 4.561911106109619 | total_loss: 19.264678955078125\n",
      "Epoch: 1494 | recon_loss: 37.128631591796875 | latent_loss: 4.607662200927734 | total_loss: 20.868146896362305\n",
      "Epoch: 1495 | recon_loss: 34.41318893432617 | latent_loss: 4.638010025024414 | total_loss: 19.52560043334961\n",
      "Epoch: 1496 | recon_loss: 35.88410949707031 | latent_loss: 4.610822677612305 | total_loss: 20.247467041015625\n",
      "Epoch: 1497 | recon_loss: 33.67530059814453 | latent_loss: 4.6016764640808105 | total_loss: 19.13848876953125\n",
      "Epoch: 1498 | recon_loss: 34.49545669555664 | latent_loss: 4.579283714294434 | total_loss: 19.537370681762695\n",
      "Epoch: 1499 | recon_loss: 37.15769958496094 | latent_loss: 4.507686138153076 | total_loss: 20.832693099975586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1500\n",
    "\n",
    "# gradually reduce the learning rate\n",
    "steps_before_reduce_learning_rate = 400\n",
    "reduce_factor = 2.0\n",
    "\n",
    "N_TRAIN_BATCHES = y_train.sum() // BATCH_SIZE\n",
    "N_TEST_BATCHES = y_test.sum() // BATCH_SIZE\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "losses = pd.DataFrame(columns = ['latent_loss', 'recon_loss', 'loss'])\n",
    "prev_loss, early_stop_round = np.inf, 0\n",
    "\n",
    "model = VAE(1e-3, hidden_size=6, share_hidden=False)\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), total=n_epochs):\n",
    "    # reduce learning rate\n",
    "    if (epoch + 1) >= steps_before_reduce_learning_rate and (epoch + 1) % steps_before_reduce_learning_rate == 0:\n",
    "        model.reduce_learning_rate(reduce_factor)\n",
    "        print('The learning rate now is: {}'.format(model.learning_rate))\n",
    "\n",
    "    # train\n",
    "    for batch, train_x in zip(range(N_TRAIN_BATCHES), train_pos_dataset):\n",
    "        model.train(train_x)\n",
    "        \n",
    "    # test on holdout\n",
    "    loss = []\n",
    "    for batch, test_x in zip(range(N_TEST_BATCHES), test_pos_dataset):\n",
    "        loss.append(model.compute_loss(test_x))\n",
    "    losses.loc[len(losses)] = np.mean(loss, axis=0)\n",
    "    \n",
    "    # early stopping\n",
    "    round_loss = (losses.recon_loss.values[-1] + losses.latent_loss.values[-1]) / 2\n",
    "    if round_loss < prev_loss:\n",
    "        early_stop_round = 0\n",
    "    else:\n",
    "        early_stop_round += 1\n",
    "        if early_stop_round == EARLY_STOPPING_ROUNDS:\n",
    "            break\n",
    "    prev_loss = round_loss\n",
    "    \n",
    "    print(\n",
    "        \"Epoch: {} | recon_loss: {} | latent_loss: {} | total_loss: {}\".format(\n",
    "           epoch, losses.recon_loss.values[-1], losses.latent_loss.values[-1], round_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x50db6a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss throughout the training\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(losses.index, losses.recon_loss, label='recon_loss')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(losses.index, losses.latent_loss, label='latent_loss', color='orange')\n",
    "ax.figure.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save_weights('./vae_1_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model perfomance with no lable at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "p_z = ds.MultivariateNormalDiag(loc=[0.] * model.hidden_size, scale_diag=[1.] * model.hidden_size)\n",
    "\n",
    "test_tensor = tf.constant(X_test.values, dtype='float32')\n",
    "# mu, sigma = tf.split(model.encoder(test_tensor), 2, 1)\n",
    "encoded = model.encoder(test_tensor)\n",
    "mu, sigma = model.dense_mean(encoded), model.dense_std(encoded)\n",
    "\n",
    "q_z = ds.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
    "test_pred = ds.kl_divergence(p_z, q_z)\n",
    "# test_pred = tf.sigmoid(test_pred)\n",
    "\n",
    "test_pred = test_pred.numpy()\n",
    "test_pred = test_pred / (test_pred.max() - test_pred.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7206637173549073"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use labels to generate oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to generate 3627 positive samples.\n"
     ]
    }
   ],
   "source": [
    "# number of positive samples to generate so the minority class has the same amount as majority clas\n",
    "n = (y_train == 0).sum() - (y_train==1).sum()\n",
    "print('Need to generate {} positive samples.'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3933, 36])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pos = X_train[y_train==1]\n",
    "\n",
    "_, _, ds_pos = model.encode(X_train_pos.values)\n",
    "sample_pos = ds_pos.sample(sample_shape=(n // y_train.sum()) + 1, seed=1024)\n",
    "sample_pos = tf.reshape(sample_pos, [-1, model.hidden_size])\n",
    "sample_pos = model.decode(sample_pos)\n",
    "sample_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append new samples to original samples\n",
    "X_ = np.r_[X_train.values, sample_pos.numpy()[:n, ...]]\n",
    "y_ = np.append(y_train, [1] * n)\n",
    "\n",
    "# shuffle it \n",
    "from sklearn.utils import shuffle\n",
    "X_, y_ = shuffle(X_, y_, random_state=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9428321336478179\n",
      "Recall: 0.5957446808510638\n",
      "Precision: 0.7671232876712328\n",
      "F1: 0.6706586826347305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_, y_)\n",
    "\n",
    "evaluate(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Environment (conda_dl)",
   "language": "python",
   "name": "conda_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
