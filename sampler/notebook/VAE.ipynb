{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will be using satimage data as in the Borderline-SMOTE paper\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "data = fetch_openml(name='satimage')\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# treat 4 as the target class\n",
    "y = (y=='4.').astype(int)\n",
    "y.mean()\n",
    "\n",
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the baseline model does without any over-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7,\n",
    "                                                    stratify=y, shuffle=True, random_state=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf):\n",
    "    pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "    pred = clf.predict(X_test)\n",
    "    metric = 'AUC: {}\\nRecall: {}\\nPrecision: {}\\nF1: {}\\n'.format(roc_auc_score(y_test, pred_proba),\n",
    "                                                              recall_score(y_test, pred),\n",
    "                                                              precision_score(y_test, pred),\n",
    "                                                              f1_score(y_test, pred))\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9455910029696799\n",
      "Recall: 0.4946808510638298\n",
      "Precision: 0.8378378378378378\n",
      "F1: 0.6220735785953178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=1024)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "evaluate(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9566677258117736\n",
      "Recall: 0.6595744680851063\n",
      "Precision: 0.6458333333333334\n",
      "F1: 0.6526315789473683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "clf.fit(*SMOTE().fit_resample(X_train, y_train))\n",
    "\n",
    "evaluate(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T19:17:29.886938Z",
     "start_time": "2019-05-10T19:17:26.656978Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuzhehao\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl\\lib\\site-packages\\tqdm\\autonotebook\\__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from tensorflow import keras\n",
    "\n",
    "# the nightly build of tensorflow_probability is required as of the time of writing this \n",
    "import tensorflow_probability as tfp\n",
    "ds = tfp.distributions\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    mean_, var_ = tf.nn.moments(x, axes=[0], keepdims=True)\n",
    "    return (x - mean_) / tf.sqrt(var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEATURE = X_train.shape[1]\n",
    "BATCH_SIZE = 124\n",
    "\n",
    "train_dataset =tf.data.Dataset.from_tensor_slices(X_train.values.astype('float32')) \\\n",
    "    .shuffle(10000).batch(BATCH_SIZE).map(standardize)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(X_test.values.astype('float32')) \\\n",
    "    .shuffle(10000).batch(10000).map(standardize)\n",
    "\n",
    "# all positive samples\n",
    "train_pos_dataset = tf.data.Dataset.from_tensor_slices(X_train[y_train==1].values.astype('float32')) \\\n",
    "    .shuffle(10000).batch(BATCH_SIZE).map(standardize)\n",
    "test_pos_dataset = tf.data.Dataset.from_tensor_slices(X_test[y_test==1].values.astype('float32')) \\\n",
    "    .shuffle(10000).batch(100).map(standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([124, 36])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_pos_dataset))\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encoder_decoder(hidden_size=4, share_hidden=False):\n",
    "    \"\"\" If share hidden is set to False, then the actual hidden_size will be 2 * hidden_size\n",
    "        and it will split in half into the mean and std vector\n",
    "    \"\"\"\n",
    "    if share_hidden is False:\n",
    "        hidden_size = hidden_size * 2\n",
    "        \n",
    "    encoder = tf.keras.models.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(N_FEATURE,)),\n",
    "        keras.layers.Dense(36, activation=None),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(18, activation=None),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(hidden_size)\n",
    "    ])\n",
    "\n",
    "    decoder = tf.keras.models.Sequential([\n",
    "        keras.layers.Dense(18, activation=None),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(36, activation=None),\n",
    "        keras.layers.LeakyReLU(alpha=0.2),\n",
    "        keras.layers.Dense(N_FEATURE)\n",
    "    ])\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 18)                666       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 8)                 152       \n",
      "=================================================================\n",
      "Total params: 2,150\n",
      "Trainable params: 2,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# take a look at the encoder and decoders\n",
    "e_, d_ = make_encoder_decoder()\n",
    "e_.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(gradients):\n",
    "    \"\"\" Handy function to check the gradients, in case we get nans from gradient explotion \"\"\"\n",
    "    grad = [i.numpy() for i in gradients]\n",
    "    if all([np.isfinite(g).all() for g in grad]):\n",
    "        avg_grad = [np.mean(g) for g in grad]\n",
    "        mean_, std_ = np.mean(avg_grad), np.std(avg_grad)\n",
    "        print('Gradients stats: mean={}, std={}'.format(mean_, std_))\n",
    "    else:\n",
    "        print('Gradient exploded: {}'.format(grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-3, hidden_size=8, share_hidden=False, recon_loss_div=10.0):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.share_hidden = share_hidden\n",
    "        self.encoder, self.decoder = make_encoder_decoder(hidden_size, share_hidden)\n",
    "        self.recon_loss_div = recon_loss_div\n",
    "        \n",
    "        # only used when share hidden is True\n",
    "        self.dense_mean = keras.layers.Dense(hidden_size)\n",
    "        self.dense_std = keras.layers.Dense(hidden_size)\n",
    "        \n",
    "        # use for the weighted sum of latent loss and reconstruction loss\n",
    "        self.latent_weight = tf.Variable(0.5, dtype=tf.float32, name='recon_loss_weight')\n",
    "        \n",
    "        self.learning_rate = tf.Variable(learning_rate, dtype=tf.float32, trainable=False)\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "    def reduce_learning_rate(self, factor=2):\n",
    "        self.learning_rate = self.learning_rate.assign(self.learning_rate / factor)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        if self.share_hidden:\n",
    "            mu, sigma = self.dense_mean(encoded), self.dense_std(encoded)\n",
    "        else:\n",
    "            mu, sigma = tf.split(encoded, num_or_size_splits=2, axis=1)\n",
    "        return mu, sigma, ds.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def compute_loss(self, x):\n",
    "        mu, sigma, q_z = self.encode(x)\n",
    "#         z = sigma * q_z.sample() + mu\n",
    "        z = q_z.sample()\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        # standard normal distribution\n",
    "        p_z = ds.MultivariateNormalDiag(loc=[0.] * z.shape[-1], scale_diag=[1.] * z.shape[-1])\n",
    "        kl_div = ds.kl_divergence(q_z, p_z)\n",
    "        latent_loss = tf.reduce_mean(tf.maximum(kl_div, 0))\n",
    "        recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x - x_recon), axis=0))\n",
    "#         return latent_loss, recon_loss, recon_loss / self.recon_loss_div + latent_loss\n",
    "\n",
    "        latent_weight = tf.clip_by_value(self.latent_weight, 0.1, 0.9)\n",
    "        recon_weight = 1 - latent_weight\n",
    "        return latent_loss, recon_loss, recon_weight * recon_loss + latent_weight * latent_loss\n",
    "    \n",
    "    def compute_gradients(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            latent_loss, recon_loss, loss = self.compute_loss(x)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        return gradients\n",
    "#         grad = [g.numpy() for g in gradients]\n",
    "#         if all([np.isfinite(g).all() for g in grad]):\n",
    "#             return gradients\n",
    "#         else:\n",
    "#             print('Gradient exploded.')\n",
    "#             return None\n",
    "        \n",
    "    @tf.function\n",
    "    def train(self, train_x):\n",
    "        gradients = self.compute_gradients(train_x)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b81d3f2735a42ee8f3c8e02b76df774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | recon_loss: 101.55613708496094 | latent_loss: 5.612210273742676 | total_loss: 53.58417510986328\n",
      "Epoch: 1 | recon_loss: 99.94841766357422 | latent_loss: 5.239713668823242 | total_loss: 52.59406661987305\n",
      "Epoch: 2 | recon_loss: 98.8521499633789 | latent_loss: 5.099998950958252 | total_loss: 51.97607421875\n",
      "Epoch: 3 | recon_loss: 97.87281799316406 | latent_loss: 5.136470794677734 | total_loss: 51.50464630126953\n",
      "Epoch: 4 | recon_loss: 95.87464141845703 | latent_loss: 5.178112983703613 | total_loss: 50.5263786315918\n",
      "Epoch: 5 | recon_loss: 94.69163513183594 | latent_loss: 5.199558258056641 | total_loss: 49.945594787597656\n",
      "Epoch: 6 | recon_loss: 92.33568572998047 | latent_loss: 5.2553229331970215 | total_loss: 48.79550552368164\n",
      "Epoch: 7 | recon_loss: 90.65685272216797 | latent_loss: 5.448147773742676 | total_loss: 48.0525016784668\n",
      "Epoch: 8 | recon_loss: 87.29712677001953 | latent_loss: 5.506939888000488 | total_loss: 46.402034759521484\n",
      "Epoch: 9 | recon_loss: 84.53145599365234 | latent_loss: 5.544269561767578 | total_loss: 45.037864685058594\n",
      "Epoch: 10 | recon_loss: 79.75546264648438 | latent_loss: 5.582155227661133 | total_loss: 42.66880798339844\n",
      "Epoch: 11 | recon_loss: 76.98802185058594 | latent_loss: 5.78514289855957 | total_loss: 41.38658142089844\n",
      "Epoch: 12 | recon_loss: 71.88855743408203 | latent_loss: 6.226800918579102 | total_loss: 39.05767822265625\n",
      "Epoch: 13 | recon_loss: 65.63007354736328 | latent_loss: 6.638747692108154 | total_loss: 36.1344108581543\n",
      "Epoch: 14 | recon_loss: 60.69120788574219 | latent_loss: 7.17545223236084 | total_loss: 33.93333053588867\n",
      "Epoch: 15 | recon_loss: 57.51728820800781 | latent_loss: 7.570818901062012 | total_loss: 32.54405212402344\n",
      "Epoch: 16 | recon_loss: 55.37948989868164 | latent_loss: 7.953922271728516 | total_loss: 31.666706085205078\n",
      "Epoch: 17 | recon_loss: 53.47134780883789 | latent_loss: 8.184667587280273 | total_loss: 30.828006744384766\n",
      "Epoch: 18 | recon_loss: 52.87019348144531 | latent_loss: 8.18380355834961 | total_loss: 30.52699851989746\n",
      "Epoch: 19 | recon_loss: 51.34951400756836 | latent_loss: 7.9305830001831055 | total_loss: 29.64004898071289\n",
      "Epoch: 20 | recon_loss: 51.54784393310547 | latent_loss: 7.78782844543457 | total_loss: 29.667835235595703\n",
      "Epoch: 21 | recon_loss: 50.92029571533203 | latent_loss: 7.73106575012207 | total_loss: 29.325679779052734\n",
      "Epoch: 22 | recon_loss: 50.69866943359375 | latent_loss: 7.700797080993652 | total_loss: 29.19973373413086\n",
      "Epoch: 23 | recon_loss: 49.11607360839844 | latent_loss: 7.708888053894043 | total_loss: 28.4124813079834\n",
      "Epoch: 24 | recon_loss: 48.93023681640625 | latent_loss: 7.621628284454346 | total_loss: 28.27593231201172\n",
      "Epoch: 25 | recon_loss: 49.871891021728516 | latent_loss: 7.539337158203125 | total_loss: 28.70561408996582\n",
      "Epoch: 26 | recon_loss: 49.48448944091797 | latent_loss: 7.575253963470459 | total_loss: 28.529870986938477\n",
      "Epoch: 27 | recon_loss: 47.419307708740234 | latent_loss: 7.707273006439209 | total_loss: 27.563289642333984\n",
      "Epoch: 28 | recon_loss: 47.94192123413086 | latent_loss: 7.693246364593506 | total_loss: 27.817583084106445\n",
      "Epoch: 29 | recon_loss: 46.77184295654297 | latent_loss: 7.628902435302734 | total_loss: 27.20037269592285\n",
      "Epoch: 30 | recon_loss: 48.87110137939453 | latent_loss: 7.5782952308654785 | total_loss: 28.224699020385742\n",
      "Epoch: 31 | recon_loss: 46.72337341308594 | latent_loss: 7.425349235534668 | total_loss: 27.07436180114746\n",
      "Epoch: 32 | recon_loss: 47.15387725830078 | latent_loss: 7.324162483215332 | total_loss: 27.2390193939209\n",
      "Epoch: 33 | recon_loss: 46.543216705322266 | latent_loss: 7.259645938873291 | total_loss: 26.901432037353516\n",
      "Epoch: 34 | recon_loss: 47.95336151123047 | latent_loss: 7.177718639373779 | total_loss: 27.565540313720703\n",
      "Epoch: 35 | recon_loss: 48.813926696777344 | latent_loss: 6.648133754730225 | total_loss: 27.731029510498047\n",
      "Epoch: 36 | recon_loss: 50.53799057006836 | latent_loss: 6.441138744354248 | total_loss: 28.489564895629883\n",
      "Epoch: 37 | recon_loss: 50.28306579589844 | latent_loss: 6.3019914627075195 | total_loss: 28.29252815246582\n",
      "Epoch: 38 | recon_loss: 48.17353820800781 | latent_loss: 6.183018207550049 | total_loss: 27.17827796936035\n",
      "Epoch: 39 | recon_loss: 49.08683776855469 | latent_loss: 6.2524237632751465 | total_loss: 27.66963005065918\n",
      "Epoch: 40 | recon_loss: 47.948326110839844 | latent_loss: 6.184148788452148 | total_loss: 27.066238403320312\n",
      "Epoch: 41 | recon_loss: 47.79866027832031 | latent_loss: 6.2167205810546875 | total_loss: 27.0076904296875\n",
      "Epoch: 42 | recon_loss: 46.01523971557617 | latent_loss: 6.362884998321533 | total_loss: 26.189062118530273\n",
      "Epoch: 43 | recon_loss: 45.50884246826172 | latent_loss: 6.569330215454102 | total_loss: 26.039085388183594\n",
      "Epoch: 44 | recon_loss: 46.49140548706055 | latent_loss: 6.711392879486084 | total_loss: 26.601398468017578\n",
      "Epoch: 45 | recon_loss: 45.486026763916016 | latent_loss: 6.888095855712891 | total_loss: 26.187061309814453\n",
      "Epoch: 46 | recon_loss: 45.58952713012695 | latent_loss: 6.8939714431762695 | total_loss: 26.241748809814453\n",
      "Epoch: 47 | recon_loss: 44.87806701660156 | latent_loss: 6.902851581573486 | total_loss: 25.890459060668945\n",
      "Epoch: 48 | recon_loss: 44.59767532348633 | latent_loss: 6.884703159332275 | total_loss: 25.74118995666504\n",
      "Epoch: 49 | recon_loss: 45.50836944580078 | latent_loss: 6.904106616973877 | total_loss: 26.20623779296875\n",
      "Epoch: 50 | recon_loss: 45.066951751708984 | latent_loss: 6.795225620269775 | total_loss: 25.931089401245117\n",
      "Epoch: 51 | recon_loss: 44.48550033569336 | latent_loss: 6.764690399169922 | total_loss: 25.62509536743164\n",
      "Epoch: 52 | recon_loss: 43.963287353515625 | latent_loss: 6.8031325340271 | total_loss: 25.383209228515625\n",
      "Epoch: 53 | recon_loss: 44.352561950683594 | latent_loss: 6.786168098449707 | total_loss: 25.569364547729492\n",
      "Epoch: 54 | recon_loss: 45.342529296875 | latent_loss: 6.876278877258301 | total_loss: 26.109403610229492\n",
      "Epoch: 55 | recon_loss: 45.31971740722656 | latent_loss: 7.006654739379883 | total_loss: 26.163185119628906\n",
      "Epoch: 56 | recon_loss: 43.46080017089844 | latent_loss: 7.116342544555664 | total_loss: 25.288570404052734\n",
      "Epoch: 57 | recon_loss: 43.25239944458008 | latent_loss: 7.209862232208252 | total_loss: 25.231130599975586\n",
      "Epoch: 58 | recon_loss: 44.07371520996094 | latent_loss: 7.313174247741699 | total_loss: 25.693445205688477\n",
      "Epoch: 59 | recon_loss: 42.534305572509766 | latent_loss: 7.3808135986328125 | total_loss: 24.95755958557129\n",
      "Epoch: 60 | recon_loss: 42.84873962402344 | latent_loss: 7.2727813720703125 | total_loss: 25.060760498046875\n",
      "Epoch: 61 | recon_loss: 42.56449890136719 | latent_loss: 7.308786392211914 | total_loss: 24.936641693115234\n",
      "Epoch: 62 | recon_loss: 42.418495178222656 | latent_loss: 7.243687152862549 | total_loss: 24.831090927124023\n",
      "Epoch: 63 | recon_loss: 42.56421661376953 | latent_loss: 7.137502670288086 | total_loss: 24.850860595703125\n",
      "Epoch: 64 | recon_loss: 43.278621673583984 | latent_loss: 7.003233432769775 | total_loss: 25.140928268432617\n",
      "Epoch: 65 | recon_loss: 43.62887954711914 | latent_loss: 6.89547061920166 | total_loss: 25.262174606323242\n",
      "Epoch: 66 | recon_loss: 41.61324691772461 | latent_loss: 6.853300094604492 | total_loss: 24.233272552490234\n",
      "Epoch: 67 | recon_loss: 40.46397399902344 | latent_loss: 6.819843292236328 | total_loss: 23.641908645629883\n",
      "Epoch: 68 | recon_loss: 43.24787139892578 | latent_loss: 6.8275909423828125 | total_loss: 25.037731170654297\n",
      "Epoch: 69 | recon_loss: 41.14132308959961 | latent_loss: 6.992038726806641 | total_loss: 24.066680908203125\n",
      "Epoch: 70 | recon_loss: 42.745025634765625 | latent_loss: 7.177616119384766 | total_loss: 24.961320877075195\n",
      "Epoch: 71 | recon_loss: 45.350318908691406 | latent_loss: 7.139045238494873 | total_loss: 26.24468231201172\n",
      "Epoch: 72 | recon_loss: 40.76066589355469 | latent_loss: 7.007523059844971 | total_loss: 23.88409423828125\n",
      "Epoch: 73 | recon_loss: 39.49433135986328 | latent_loss: 7.010484218597412 | total_loss: 23.25240707397461\n",
      "Epoch: 74 | recon_loss: 41.256744384765625 | latent_loss: 7.009605884552002 | total_loss: 24.133174896240234\n",
      "Epoch: 75 | recon_loss: 44.15424346923828 | latent_loss: 6.867326259613037 | total_loss: 25.510784149169922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 | recon_loss: 41.79694366455078 | latent_loss: 6.810222148895264 | total_loss: 24.3035831451416\n",
      "Epoch: 77 | recon_loss: 41.92538833618164 | latent_loss: 6.742252826690674 | total_loss: 24.333820343017578\n",
      "Epoch: 78 | recon_loss: 42.68800735473633 | latent_loss: 6.655840873718262 | total_loss: 24.671924591064453\n",
      "Epoch: 79 | recon_loss: 44.956878662109375 | latent_loss: 6.4778666496276855 | total_loss: 25.71737289428711\n",
      "Epoch: 80 | recon_loss: 43.20393753051758 | latent_loss: 6.2868733406066895 | total_loss: 24.745405197143555\n",
      "Epoch: 81 | recon_loss: 43.93116760253906 | latent_loss: 6.2483229637146 | total_loss: 25.089744567871094\n",
      "Epoch: 82 | recon_loss: 41.65568161010742 | latent_loss: 6.310086250305176 | total_loss: 23.98288345336914\n",
      "Epoch: 83 | recon_loss: 42.35512924194336 | latent_loss: 6.344081401824951 | total_loss: 24.349605560302734\n",
      "Epoch: 84 | recon_loss: 43.054447174072266 | latent_loss: 6.2410430908203125 | total_loss: 24.64774513244629\n",
      "Epoch: 85 | recon_loss: 40.92353439331055 | latent_loss: 6.185678482055664 | total_loss: 23.554607391357422\n",
      "Epoch: 86 | recon_loss: 41.17584991455078 | latent_loss: 6.152456760406494 | total_loss: 23.664154052734375\n",
      "Epoch: 87 | recon_loss: 41.442806243896484 | latent_loss: 6.164175510406494 | total_loss: 23.803491592407227\n",
      "Epoch: 88 | recon_loss: 40.74563217163086 | latent_loss: 6.1972784996032715 | total_loss: 23.471454620361328\n",
      "Epoch: 89 | recon_loss: 43.62928771972656 | latent_loss: 6.238120079040527 | total_loss: 24.933704376220703\n",
      "Epoch: 90 | recon_loss: 40.4666748046875 | latent_loss: 6.283198833465576 | total_loss: 23.374937057495117\n",
      "Epoch: 91 | recon_loss: 40.233402252197266 | latent_loss: 6.296713352203369 | total_loss: 23.265058517456055\n",
      "Epoch: 92 | recon_loss: 40.5085334777832 | latent_loss: 6.319314479827881 | total_loss: 23.413923263549805\n",
      "Epoch: 93 | recon_loss: 39.62413024902344 | latent_loss: 6.327887535095215 | total_loss: 22.976009368896484\n",
      "Epoch: 94 | recon_loss: 39.631813049316406 | latent_loss: 6.317018508911133 | total_loss: 22.974414825439453\n",
      "Epoch: 95 | recon_loss: 39.66344451904297 | latent_loss: 6.301486015319824 | total_loss: 22.982465744018555\n",
      "Epoch: 96 | recon_loss: 40.15106964111328 | latent_loss: 6.304635524749756 | total_loss: 23.22785186767578\n",
      "Epoch: 97 | recon_loss: 39.47614288330078 | latent_loss: 6.315944194793701 | total_loss: 22.89604377746582\n",
      "Epoch: 98 | recon_loss: 39.58334732055664 | latent_loss: 6.321998119354248 | total_loss: 22.952672958374023\n",
      "Epoch: 99 | recon_loss: 38.91181945800781 | latent_loss: 6.330204486846924 | total_loss: 22.62101173400879\n",
      "Epoch: 100 | recon_loss: 40.46601867675781 | latent_loss: 6.359218120574951 | total_loss: 23.41261863708496\n",
      "Epoch: 101 | recon_loss: 40.16201400756836 | latent_loss: 6.320517539978027 | total_loss: 23.24126625061035\n",
      "Epoch: 102 | recon_loss: 40.363853454589844 | latent_loss: 6.318850040435791 | total_loss: 23.341352462768555\n",
      "Epoch: 103 | recon_loss: 38.75862121582031 | latent_loss: 6.30666446685791 | total_loss: 22.532642364501953\n",
      "Epoch: 104 | recon_loss: 40.64021682739258 | latent_loss: 6.314687728881836 | total_loss: 23.47745132446289\n",
      "Epoch: 105 | recon_loss: 39.165836334228516 | latent_loss: 6.333129405975342 | total_loss: 22.749483108520508\n",
      "Epoch: 106 | recon_loss: 40.22567367553711 | latent_loss: 6.335608959197998 | total_loss: 23.280641555786133\n",
      "Epoch: 107 | recon_loss: 39.92496109008789 | latent_loss: 6.337561130523682 | total_loss: 23.131261825561523\n",
      "Epoch: 108 | recon_loss: 38.719696044921875 | latent_loss: 6.3409857749938965 | total_loss: 22.53034019470215\n",
      "Epoch: 109 | recon_loss: 40.071990966796875 | latent_loss: 6.335687160491943 | total_loss: 23.203838348388672\n",
      "Epoch: 110 | recon_loss: 40.28080749511719 | latent_loss: 6.329339504241943 | total_loss: 23.305072784423828\n",
      "Epoch: 111 | recon_loss: 38.76055908203125 | latent_loss: 6.2905473709106445 | total_loss: 22.52555274963379\n",
      "Epoch: 112 | recon_loss: 39.024662017822266 | latent_loss: 6.275157928466797 | total_loss: 22.64990997314453\n",
      "Epoch: 113 | recon_loss: 39.78277587890625 | latent_loss: 6.26170539855957 | total_loss: 23.022239685058594\n",
      "Epoch: 114 | recon_loss: 39.454105377197266 | latent_loss: 6.240450859069824 | total_loss: 22.847278594970703\n",
      "Epoch: 115 | recon_loss: 38.243675231933594 | latent_loss: 6.226442813873291 | total_loss: 22.23505973815918\n",
      "Epoch: 116 | recon_loss: 38.09978103637695 | latent_loss: 6.219681262969971 | total_loss: 22.159730911254883\n",
      "Epoch: 117 | recon_loss: 38.18687057495117 | latent_loss: 6.227178573608398 | total_loss: 22.20702362060547\n",
      "Epoch: 118 | recon_loss: 38.41995620727539 | latent_loss: 6.240601062774658 | total_loss: 22.330278396606445\n",
      "Epoch: 119 | recon_loss: 38.5832633972168 | latent_loss: 6.253145694732666 | total_loss: 22.41820526123047\n",
      "Epoch: 120 | recon_loss: 38.07957458496094 | latent_loss: 6.2595367431640625 | total_loss: 22.1695556640625\n",
      "Epoch: 121 | recon_loss: 38.60789489746094 | latent_loss: 6.2350993156433105 | total_loss: 22.421497344970703\n",
      "Epoch: 122 | recon_loss: 37.45138931274414 | latent_loss: 6.2192702293396 | total_loss: 21.835329055786133\n",
      "Epoch: 123 | recon_loss: 40.385799407958984 | latent_loss: 6.213379383087158 | total_loss: 23.299589157104492\n",
      "Epoch: 124 | recon_loss: 38.74210739135742 | latent_loss: 6.223513126373291 | total_loss: 22.482810974121094\n",
      "Epoch: 125 | recon_loss: 37.99089050292969 | latent_loss: 6.217053413391113 | total_loss: 22.103971481323242\n",
      "Epoch: 126 | recon_loss: 38.666847229003906 | latent_loss: 6.180637359619141 | total_loss: 22.423742294311523\n",
      "Epoch: 127 | recon_loss: 39.80792236328125 | latent_loss: 6.15178108215332 | total_loss: 22.97985076904297\n",
      "Epoch: 128 | recon_loss: 38.184906005859375 | latent_loss: 6.147317409515381 | total_loss: 22.16611099243164\n",
      "Epoch: 129 | recon_loss: 37.97200012207031 | latent_loss: 6.163598537445068 | total_loss: 22.067798614501953\n",
      "Epoch: 130 | recon_loss: 37.58835983276367 | latent_loss: 6.163569927215576 | total_loss: 21.875965118408203\n",
      "Epoch: 131 | recon_loss: 37.65190505981445 | latent_loss: 6.185586452484131 | total_loss: 21.918745040893555\n",
      "Epoch: 132 | recon_loss: 39.19599151611328 | latent_loss: 6.1951494216918945 | total_loss: 22.69556999206543\n",
      "Epoch: 133 | recon_loss: 37.8555793762207 | latent_loss: 6.175406455993652 | total_loss: 22.015493392944336\n",
      "Epoch: 134 | recon_loss: 38.11070251464844 | latent_loss: 6.1729207038879395 | total_loss: 22.14181137084961\n",
      "Epoch: 135 | recon_loss: 37.579368591308594 | latent_loss: 6.188559055328369 | total_loss: 21.88396453857422\n",
      "Epoch: 136 | recon_loss: 39.40238571166992 | latent_loss: 6.122159481048584 | total_loss: 22.762271881103516\n",
      "Epoch: 137 | recon_loss: 39.67761993408203 | latent_loss: 6.093211650848389 | total_loss: 22.88541603088379\n",
      "Epoch: 138 | recon_loss: 38.54579544067383 | latent_loss: 6.0832390785217285 | total_loss: 22.314517974853516\n",
      "Epoch: 139 | recon_loss: 39.01722717285156 | latent_loss: 6.090181350708008 | total_loss: 22.55370330810547\n",
      "Epoch: 140 | recon_loss: 37.445308685302734 | latent_loss: 6.106794357299805 | total_loss: 21.776050567626953\n",
      "Epoch: 141 | recon_loss: 39.07160949707031 | latent_loss: 6.144803524017334 | total_loss: 22.608205795288086\n",
      "Epoch: 142 | recon_loss: 36.58763885498047 | latent_loss: 6.115386486053467 | total_loss: 21.351512908935547\n",
      "Epoch: 143 | recon_loss: 38.67139434814453 | latent_loss: 6.131828784942627 | total_loss: 22.401611328125\n",
      "Epoch: 144 | recon_loss: 36.985633850097656 | latent_loss: 6.166353225708008 | total_loss: 21.575992584228516\n",
      "Epoch: 145 | recon_loss: 36.59783935546875 | latent_loss: 6.0986104011535645 | total_loss: 21.348224639892578\n",
      "Epoch: 146 | recon_loss: 36.72486877441406 | latent_loss: 6.099354267120361 | total_loss: 21.412111282348633\n",
      "Epoch: 147 | recon_loss: 39.25965881347656 | latent_loss: 6.087889194488525 | total_loss: 22.67377471923828\n",
      "Epoch: 148 | recon_loss: 39.17145919799805 | latent_loss: 6.068525314331055 | total_loss: 22.619991302490234\n",
      "Epoch: 149 | recon_loss: 36.39385986328125 | latent_loss: 6.040704727172852 | total_loss: 21.217281341552734\n",
      "Epoch: 150 | recon_loss: 38.04216003417969 | latent_loss: 6.05173397064209 | total_loss: 22.046947479248047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 151 | recon_loss: 39.453182220458984 | latent_loss: 6.030349254608154 | total_loss: 22.74176597595215\n",
      "Epoch: 152 | recon_loss: 36.53891372680664 | latent_loss: 6.031999588012695 | total_loss: 21.285457611083984\n",
      "Epoch: 153 | recon_loss: 38.28887939453125 | latent_loss: 6.051328659057617 | total_loss: 22.17010498046875\n",
      "Epoch: 154 | recon_loss: 37.73033905029297 | latent_loss: 6.017341136932373 | total_loss: 21.87384033203125\n",
      "Epoch: 155 | recon_loss: 36.05161666870117 | latent_loss: 5.983509540557861 | total_loss: 21.017562866210938\n",
      "Epoch: 156 | recon_loss: 37.26445770263672 | latent_loss: 5.985456466674805 | total_loss: 21.624958038330078\n",
      "Epoch: 157 | recon_loss: 40.74662399291992 | latent_loss: 6.01609992980957 | total_loss: 23.381362915039062\n",
      "Epoch: 158 | recon_loss: 38.41356658935547 | latent_loss: 5.971153736114502 | total_loss: 22.192359924316406\n",
      "Epoch: 159 | recon_loss: 39.58557891845703 | latent_loss: 5.983389377593994 | total_loss: 22.78448486328125\n",
      "Epoch: 160 | recon_loss: 36.73750305175781 | latent_loss: 5.971739292144775 | total_loss: 21.35462188720703\n",
      "Epoch: 161 | recon_loss: 37.16333770751953 | latent_loss: 5.976401329040527 | total_loss: 21.569869995117188\n",
      "Epoch: 162 | recon_loss: 36.32988357543945 | latent_loss: 5.938604354858398 | total_loss: 21.13424301147461\n",
      "Epoch: 163 | recon_loss: 37.18830871582031 | latent_loss: 5.89827823638916 | total_loss: 21.543292999267578\n",
      "Epoch: 164 | recon_loss: 36.74066162109375 | latent_loss: 5.85062313079834 | total_loss: 21.295642852783203\n",
      "Epoch: 165 | recon_loss: 36.92087173461914 | latent_loss: 5.819242000579834 | total_loss: 21.37005615234375\n",
      "Epoch: 166 | recon_loss: 37.35713577270508 | latent_loss: 5.803572177886963 | total_loss: 21.580354690551758\n",
      "Epoch: 167 | recon_loss: 37.72751235961914 | latent_loss: 5.824726581573486 | total_loss: 21.776119232177734\n",
      "Epoch: 168 | recon_loss: 36.72623825073242 | latent_loss: 5.803107738494873 | total_loss: 21.264673233032227\n",
      "Epoch: 169 | recon_loss: 39.99876022338867 | latent_loss: 5.77458381652832 | total_loss: 22.886672973632812\n",
      "Epoch: 170 | recon_loss: 40.1842155456543 | latent_loss: 5.746404647827148 | total_loss: 22.965309143066406\n",
      "Epoch: 171 | recon_loss: 38.1316032409668 | latent_loss: 5.741563320159912 | total_loss: 21.936582565307617\n",
      "Epoch: 172 | recon_loss: 35.596221923828125 | latent_loss: 5.698392868041992 | total_loss: 20.647308349609375\n",
      "Epoch: 173 | recon_loss: 37.89870834350586 | latent_loss: 5.6556549072265625 | total_loss: 21.77718162536621\n",
      "Epoch: 174 | recon_loss: 40.46290588378906 | latent_loss: 5.63120174407959 | total_loss: 23.047054290771484\n",
      "Epoch: 175 | recon_loss: 37.43534851074219 | latent_loss: 5.602505683898926 | total_loss: 21.5189266204834\n",
      "Epoch: 176 | recon_loss: 37.741641998291016 | latent_loss: 5.578251361846924 | total_loss: 21.65994644165039\n",
      "Epoch: 177 | recon_loss: 39.847042083740234 | latent_loss: 5.556354999542236 | total_loss: 22.701698303222656\n",
      "Epoch: 178 | recon_loss: 40.32810974121094 | latent_loss: 5.581019878387451 | total_loss: 22.954565048217773\n",
      "Epoch: 179 | recon_loss: 38.54951477050781 | latent_loss: 5.5336527824401855 | total_loss: 22.041584014892578\n",
      "Epoch: 180 | recon_loss: 40.609893798828125 | latent_loss: 5.525501728057861 | total_loss: 23.067697525024414\n",
      "Epoch: 181 | recon_loss: 39.596561431884766 | latent_loss: 5.509061813354492 | total_loss: 22.552810668945312\n",
      "Epoch: 182 | recon_loss: 36.812347412109375 | latent_loss: 5.5032267570495605 | total_loss: 21.157787322998047\n",
      "Epoch: 183 | recon_loss: 41.15090560913086 | latent_loss: 5.50937557220459 | total_loss: 23.330141067504883\n",
      "Epoch: 184 | recon_loss: 37.152610778808594 | latent_loss: 5.447475433349609 | total_loss: 21.3000431060791\n",
      "Epoch: 185 | recon_loss: 38.70690155029297 | latent_loss: 5.419257640838623 | total_loss: 22.063079833984375\n",
      "Epoch: 186 | recon_loss: 38.01530838012695 | latent_loss: 5.4035234451293945 | total_loss: 21.709415435791016\n",
      "Epoch: 187 | recon_loss: 38.120784759521484 | latent_loss: 5.400381088256836 | total_loss: 21.760581970214844\n",
      "Epoch: 188 | recon_loss: 38.08665466308594 | latent_loss: 5.402930736541748 | total_loss: 21.744792938232422\n",
      "Epoch: 189 | recon_loss: 39.06349182128906 | latent_loss: 5.404985427856445 | total_loss: 22.234237670898438\n",
      "Epoch: 190 | recon_loss: 40.68736267089844 | latent_loss: 5.407651901245117 | total_loss: 23.047508239746094\n",
      "Epoch: 191 | recon_loss: 37.53279495239258 | latent_loss: 5.379209518432617 | total_loss: 21.45600128173828\n",
      "Epoch: 192 | recon_loss: 40.07419204711914 | latent_loss: 5.346816539764404 | total_loss: 22.71050453186035\n",
      "Epoch: 193 | recon_loss: 39.12987518310547 | latent_loss: 5.323995590209961 | total_loss: 22.22693634033203\n",
      "Epoch: 194 | recon_loss: 41.524871826171875 | latent_loss: 5.304213047027588 | total_loss: 23.41454315185547\n",
      "Epoch: 195 | recon_loss: 39.03986740112305 | latent_loss: 5.293554306030273 | total_loss: 22.166709899902344\n",
      "Epoch: 196 | recon_loss: 37.4804801940918 | latent_loss: 5.2852067947387695 | total_loss: 21.382843017578125\n",
      "Epoch: 197 | recon_loss: 37.836090087890625 | latent_loss: 5.279930591583252 | total_loss: 21.55801010131836\n",
      "Epoch: 198 | recon_loss: 39.96417236328125 | latent_loss: 5.277172088623047 | total_loss: 22.62067222595215\n",
      "Epoch: 199 | recon_loss: 41.784751892089844 | latent_loss: 5.271002292633057 | total_loss: 23.527877807617188\n",
      "Epoch: 200 | recon_loss: 40.41117858886719 | latent_loss: 5.270882606506348 | total_loss: 22.84103012084961\n",
      "Epoch: 201 | recon_loss: 39.8882942199707 | latent_loss: 5.291563510894775 | total_loss: 22.589929580688477\n",
      "Epoch: 202 | recon_loss: 37.32427215576172 | latent_loss: 5.3007097244262695 | total_loss: 21.312490463256836\n",
      "Epoch: 203 | recon_loss: 37.5656852722168 | latent_loss: 5.307941913604736 | total_loss: 21.436813354492188\n",
      "Epoch: 204 | recon_loss: 40.13194274902344 | latent_loss: 5.320990085601807 | total_loss: 22.72646713256836\n",
      "Epoch: 205 | recon_loss: 39.277774810791016 | latent_loss: 5.304135322570801 | total_loss: 22.29095458984375\n",
      "Epoch: 206 | recon_loss: 39.54086685180664 | latent_loss: 5.290319442749023 | total_loss: 22.415592193603516\n",
      "Epoch: 207 | recon_loss: 38.628944396972656 | latent_loss: 5.304293632507324 | total_loss: 21.96661949157715\n",
      "Epoch: 208 | recon_loss: 38.93983840942383 | latent_loss: 5.301363468170166 | total_loss: 22.120601654052734\n",
      "Epoch: 209 | recon_loss: 38.96183776855469 | latent_loss: 5.303962230682373 | total_loss: 22.13290023803711\n",
      "Epoch: 210 | recon_loss: 38.96097946166992 | latent_loss: 5.300179481506348 | total_loss: 22.130578994750977\n",
      "Epoch: 211 | recon_loss: 40.46516799926758 | latent_loss: 5.313568115234375 | total_loss: 22.889368057250977\n",
      "Epoch: 212 | recon_loss: 39.09114074707031 | latent_loss: 5.343667984008789 | total_loss: 22.217403411865234\n",
      "Epoch: 213 | recon_loss: 38.43903350830078 | latent_loss: 5.302289962768555 | total_loss: 21.870662689208984\n",
      "Epoch: 214 | recon_loss: 43.128662109375 | latent_loss: 5.293483734130859 | total_loss: 24.21107292175293\n",
      "Epoch: 215 | recon_loss: 38.74449157714844 | latent_loss: 5.2518086433410645 | total_loss: 21.998149871826172\n",
      "Epoch: 216 | recon_loss: 39.01124572753906 | latent_loss: 5.237344741821289 | total_loss: 22.12429428100586\n",
      "Epoch: 217 | recon_loss: 38.3480339050293 | latent_loss: 5.2261481285095215 | total_loss: 21.787090301513672\n",
      "Epoch: 218 | recon_loss: 36.87744140625 | latent_loss: 5.238667011260986 | total_loss: 21.058053970336914\n",
      "Epoch: 219 | recon_loss: 37.058319091796875 | latent_loss: 5.237853527069092 | total_loss: 21.148086547851562\n",
      "Epoch: 220 | recon_loss: 43.142486572265625 | latent_loss: 5.2408037185668945 | total_loss: 24.1916446685791\n",
      "Epoch: 221 | recon_loss: 39.081050872802734 | latent_loss: 5.245919704437256 | total_loss: 22.163484573364258\n",
      "Epoch: 222 | recon_loss: 42.78092575073242 | latent_loss: 5.2658305168151855 | total_loss: 24.023378372192383\n",
      "Epoch: 223 | recon_loss: 40.04804611206055 | latent_loss: 5.271939754486084 | total_loss: 22.659992218017578\n",
      "Epoch: 224 | recon_loss: 40.11749267578125 | latent_loss: 5.267092227935791 | total_loss: 22.692293167114258\n",
      "Epoch: 225 | recon_loss: 39.311641693115234 | latent_loss: 5.275558948516846 | total_loss: 22.29360008239746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 226 | recon_loss: 41.22336196899414 | latent_loss: 5.261407852172852 | total_loss: 23.242385864257812\n",
      "Epoch: 227 | recon_loss: 40.95165252685547 | latent_loss: 5.276694297790527 | total_loss: 23.114173889160156\n",
      "Epoch: 228 | recon_loss: 37.77178955078125 | latent_loss: 5.225625991821289 | total_loss: 21.498706817626953\n",
      "Epoch: 229 | recon_loss: 39.11382293701172 | latent_loss: 5.214836597442627 | total_loss: 22.164329528808594\n",
      "Epoch: 230 | recon_loss: 36.41205978393555 | latent_loss: 5.224729061126709 | total_loss: 20.81839370727539\n",
      "Epoch: 231 | recon_loss: 38.049190521240234 | latent_loss: 5.208677768707275 | total_loss: 21.628934860229492\n",
      "Epoch: 232 | recon_loss: 37.766483306884766 | latent_loss: 5.194202423095703 | total_loss: 21.480342864990234\n",
      "Epoch: 233 | recon_loss: 39.924129486083984 | latent_loss: 5.198178768157959 | total_loss: 22.561153411865234\n",
      "Epoch: 234 | recon_loss: 38.912147521972656 | latent_loss: 5.178580284118652 | total_loss: 22.045364379882812\n",
      "Epoch: 235 | recon_loss: 40.553401947021484 | latent_loss: 5.184367656707764 | total_loss: 22.868885040283203\n",
      "Epoch: 236 | recon_loss: 38.92192077636719 | latent_loss: 5.17257022857666 | total_loss: 22.047245025634766\n",
      "Epoch: 237 | recon_loss: 38.29652404785156 | latent_loss: 5.140843391418457 | total_loss: 21.71868324279785\n",
      "Epoch: 238 | recon_loss: 39.30767822265625 | latent_loss: 5.132631778717041 | total_loss: 22.220155715942383\n",
      "Epoch: 239 | recon_loss: 40.585975646972656 | latent_loss: 5.117609024047852 | total_loss: 22.851791381835938\n",
      "Epoch: 240 | recon_loss: 39.601768493652344 | latent_loss: 5.100250244140625 | total_loss: 22.351009368896484\n",
      "Epoch: 241 | recon_loss: 39.95065689086914 | latent_loss: 5.089950084686279 | total_loss: 22.52030372619629\n",
      "Epoch: 242 | recon_loss: 39.89251708984375 | latent_loss: 5.091157913208008 | total_loss: 22.491836547851562\n",
      "Epoch: 243 | recon_loss: 39.4910888671875 | latent_loss: 5.101637363433838 | total_loss: 22.296363830566406\n",
      "Epoch: 244 | recon_loss: 37.11386489868164 | latent_loss: 5.125710487365723 | total_loss: 21.119787216186523\n",
      "Epoch: 245 | recon_loss: 43.59031677246094 | latent_loss: 5.173872470855713 | total_loss: 24.382095336914062\n",
      "Epoch: 246 | recon_loss: 39.42435836791992 | latent_loss: 5.155831813812256 | total_loss: 22.29009437561035\n",
      "Epoch: 247 | recon_loss: 39.627750396728516 | latent_loss: 5.164071559906006 | total_loss: 22.395910263061523\n",
      "Epoch: 248 | recon_loss: 36.78245162963867 | latent_loss: 5.129785537719727 | total_loss: 20.956119537353516\n",
      "Epoch: 249 | recon_loss: 39.864952087402344 | latent_loss: 5.121591567993164 | total_loss: 22.493270874023438\n",
      "Epoch: 250 | recon_loss: 37.47728729248047 | latent_loss: 5.124950408935547 | total_loss: 21.301118850708008\n",
      "Epoch: 251 | recon_loss: 38.56120300292969 | latent_loss: 5.123786449432373 | total_loss: 21.84249496459961\n",
      "Epoch: 252 | recon_loss: 40.496002197265625 | latent_loss: 5.122714042663574 | total_loss: 22.809358596801758\n",
      "Epoch: 253 | recon_loss: 42.49592208862305 | latent_loss: 5.125551700592041 | total_loss: 23.81073760986328\n",
      "Epoch: 254 | recon_loss: 41.27903747558594 | latent_loss: 5.106534004211426 | total_loss: 23.192785263061523\n",
      "Epoch: 255 | recon_loss: 39.00450897216797 | latent_loss: 5.099243640899658 | total_loss: 22.051876068115234\n",
      "Epoch: 256 | recon_loss: 40.21357345581055 | latent_loss: 5.100550651550293 | total_loss: 22.657062530517578\n",
      "Epoch: 257 | recon_loss: 42.22633361816406 | latent_loss: 5.085026741027832 | total_loss: 23.65567970275879\n",
      "Epoch: 258 | recon_loss: 36.81428527832031 | latent_loss: 5.091724872589111 | total_loss: 20.953004837036133\n",
      "Epoch: 259 | recon_loss: 39.57305145263672 | latent_loss: 5.108081340789795 | total_loss: 22.340566635131836\n",
      "Epoch: 260 | recon_loss: 37.686283111572266 | latent_loss: 5.079485893249512 | total_loss: 21.382884979248047\n",
      "Epoch: 261 | recon_loss: 37.85857009887695 | latent_loss: 5.079754829406738 | total_loss: 21.469161987304688\n",
      "Epoch: 262 | recon_loss: 38.59910583496094 | latent_loss: 5.083300590515137 | total_loss: 21.841203689575195\n",
      "Epoch: 263 | recon_loss: 40.46860885620117 | latent_loss: 5.093137741088867 | total_loss: 22.780872344970703\n",
      "Epoch: 264 | recon_loss: 38.8406867980957 | latent_loss: 5.114144802093506 | total_loss: 21.977415084838867\n",
      "Epoch: 265 | recon_loss: 38.12930679321289 | latent_loss: 5.10336446762085 | total_loss: 21.616334915161133\n",
      "Epoch: 266 | recon_loss: 41.50021743774414 | latent_loss: 5.103277206420898 | total_loss: 23.301746368408203\n",
      "Epoch: 267 | recon_loss: 38.01048278808594 | latent_loss: 5.0802202224731445 | total_loss: 21.545351028442383\n",
      "Epoch: 268 | recon_loss: 36.846885681152344 | latent_loss: 5.070931434631348 | total_loss: 20.958908081054688\n",
      "Epoch: 269 | recon_loss: 40.596168518066406 | latent_loss: 5.083599090576172 | total_loss: 22.83988380432129\n",
      "Epoch: 270 | recon_loss: 39.69010925292969 | latent_loss: 5.0910964012146 | total_loss: 22.390602111816406\n",
      "Epoch: 271 | recon_loss: 38.91037368774414 | latent_loss: 5.087599754333496 | total_loss: 21.998987197875977\n",
      "Epoch: 272 | recon_loss: 37.25442123413086 | latent_loss: 5.073487281799316 | total_loss: 21.16395378112793\n",
      "Epoch: 273 | recon_loss: 39.31621551513672 | latent_loss: 5.069244384765625 | total_loss: 22.192729949951172\n",
      "Epoch: 274 | recon_loss: 38.041996002197266 | latent_loss: 5.073622703552246 | total_loss: 21.557809829711914\n",
      "Epoch: 275 | recon_loss: 38.84402847290039 | latent_loss: 5.072744846343994 | total_loss: 21.95838737487793\n",
      "Epoch: 276 | recon_loss: 37.99628448486328 | latent_loss: 5.0849480628967285 | total_loss: 21.540616989135742\n",
      "Epoch: 277 | recon_loss: 37.543617248535156 | latent_loss: 5.1298322677612305 | total_loss: 21.33672523498535\n",
      "Epoch: 278 | recon_loss: 35.24238586425781 | latent_loss: 5.095527648925781 | total_loss: 20.168956756591797\n",
      "Epoch: 279 | recon_loss: 39.21328353881836 | latent_loss: 5.071559906005859 | total_loss: 22.14242172241211\n",
      "Epoch: 280 | recon_loss: 39.76594924926758 | latent_loss: 5.053192615509033 | total_loss: 22.409570693969727\n",
      "Epoch: 281 | recon_loss: 37.43229675292969 | latent_loss: 5.037382125854492 | total_loss: 21.234840393066406\n",
      "Epoch: 282 | recon_loss: 41.3681755065918 | latent_loss: 5.026845455169678 | total_loss: 23.197509765625\n",
      "Epoch: 283 | recon_loss: 41.14898681640625 | latent_loss: 5.032531261444092 | total_loss: 23.09075927734375\n",
      "Epoch: 284 | recon_loss: 39.52967071533203 | latent_loss: 5.037547588348389 | total_loss: 22.28360939025879\n",
      "Epoch: 285 | recon_loss: 37.82783508300781 | latent_loss: 5.031685829162598 | total_loss: 21.429759979248047\n",
      "Epoch: 286 | recon_loss: 37.71330261230469 | latent_loss: 5.020946502685547 | total_loss: 21.367124557495117\n",
      "Epoch: 287 | recon_loss: 39.00126647949219 | latent_loss: 5.014946937561035 | total_loss: 22.008106231689453\n",
      "Epoch: 288 | recon_loss: 37.276004791259766 | latent_loss: 5.016157627105713 | total_loss: 21.146081924438477\n",
      "Epoch: 289 | recon_loss: 40.42491912841797 | latent_loss: 5.025424957275391 | total_loss: 22.72517204284668\n",
      "Epoch: 290 | recon_loss: 40.549049377441406 | latent_loss: 5.021404266357422 | total_loss: 22.785226821899414\n",
      "Epoch: 291 | recon_loss: 34.942962646484375 | latent_loss: 5.01553201675415 | total_loss: 19.979248046875\n",
      "Epoch: 292 | recon_loss: 38.723052978515625 | latent_loss: 5.012335300445557 | total_loss: 21.867694854736328\n",
      "Epoch: 293 | recon_loss: 37.08660888671875 | latent_loss: 4.9985737800598145 | total_loss: 21.042591094970703\n",
      "Epoch: 294 | recon_loss: 38.0465087890625 | latent_loss: 4.984609127044678 | total_loss: 21.51555824279785\n",
      "Epoch: 295 | recon_loss: 41.53755569458008 | latent_loss: 4.978732585906982 | total_loss: 23.25814437866211\n",
      "Epoch: 296 | recon_loss: 39.0526237487793 | latent_loss: 4.9738240242004395 | total_loss: 22.01322364807129\n",
      "Epoch: 297 | recon_loss: 37.75349426269531 | latent_loss: 4.978734970092773 | total_loss: 21.36611557006836\n",
      "Epoch: 298 | recon_loss: 37.85964584350586 | latent_loss: 4.984644889831543 | total_loss: 21.42214584350586\n",
      "The learning rate now is: <tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=0.0005>\n",
      "Epoch: 299 | recon_loss: 36.188594818115234 | latent_loss: 4.992616653442383 | total_loss: 20.590606689453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300 | recon_loss: 39.68938446044922 | latent_loss: 4.998510360717773 | total_loss: 22.343948364257812\n",
      "Epoch: 301 | recon_loss: 39.15959167480469 | latent_loss: 5.003299236297607 | total_loss: 22.081445693969727\n",
      "Epoch: 302 | recon_loss: 38.74342727661133 | latent_loss: 5.000336647033691 | total_loss: 21.87188148498535\n",
      "Epoch: 303 | recon_loss: 39.37421798706055 | latent_loss: 4.993684768676758 | total_loss: 22.18395233154297\n",
      "Epoch: 304 | recon_loss: 38.88850402832031 | latent_loss: 5.002788066864014 | total_loss: 21.945646286010742\n",
      "Epoch: 305 | recon_loss: 38.28532409667969 | latent_loss: 5.041144847869873 | total_loss: 21.66323471069336\n",
      "Epoch: 306 | recon_loss: 39.66169738769531 | latent_loss: 5.007611274719238 | total_loss: 22.334653854370117\n",
      "Epoch: 307 | recon_loss: 39.5867919921875 | latent_loss: 5.011460781097412 | total_loss: 22.29912567138672\n",
      "Epoch: 308 | recon_loss: 37.71796798706055 | latent_loss: 5.026521682739258 | total_loss: 21.37224578857422\n",
      "Epoch: 309 | recon_loss: 36.29862976074219 | latent_loss: 5.039881706237793 | total_loss: 20.66925621032715\n",
      "Epoch: 310 | recon_loss: 40.061065673828125 | latent_loss: 5.045979976654053 | total_loss: 22.55352210998535\n",
      "Epoch: 311 | recon_loss: 37.67554473876953 | latent_loss: 5.030301570892334 | total_loss: 21.352922439575195\n",
      "Epoch: 312 | recon_loss: 37.48503494262695 | latent_loss: 5.036707401275635 | total_loss: 21.26087188720703\n",
      "Epoch: 313 | recon_loss: 41.67068862915039 | latent_loss: 5.038473606109619 | total_loss: 23.354581832885742\n",
      "Epoch: 314 | recon_loss: 41.82151412963867 | latent_loss: 5.020077705383301 | total_loss: 23.420795440673828\n",
      "Epoch: 315 | recon_loss: 37.49281311035156 | latent_loss: 5.007335662841797 | total_loss: 21.25007438659668\n",
      "Epoch: 316 | recon_loss: 41.08298873901367 | latent_loss: 5.019105911254883 | total_loss: 23.051048278808594\n",
      "Epoch: 317 | recon_loss: 42.36865997314453 | latent_loss: 5.002300262451172 | total_loss: 23.68548011779785\n",
      "Epoch: 318 | recon_loss: 38.56098937988281 | latent_loss: 4.996696949005127 | total_loss: 21.77884292602539\n",
      "Epoch: 319 | recon_loss: 38.22270965576172 | latent_loss: 5.000444412231445 | total_loss: 21.611576080322266\n",
      "Epoch: 320 | recon_loss: 40.58293914794922 | latent_loss: 5.001903057098389 | total_loss: 22.792421340942383\n",
      "Epoch: 321 | recon_loss: 36.11162567138672 | latent_loss: 4.988276958465576 | total_loss: 20.549951553344727\n",
      "Epoch: 322 | recon_loss: 37.427146911621094 | latent_loss: 5.004755020141602 | total_loss: 21.21595001220703\n",
      "Epoch: 323 | recon_loss: 38.27375793457031 | latent_loss: 5.040584087371826 | total_loss: 21.65717124938965\n",
      "Epoch: 324 | recon_loss: 36.99772644042969 | latent_loss: 5.025592803955078 | total_loss: 21.011659622192383\n",
      "Epoch: 325 | recon_loss: 39.18442153930664 | latent_loss: 5.022955417633057 | total_loss: 22.103689193725586\n",
      "Epoch: 326 | recon_loss: 35.38325500488281 | latent_loss: 5.000352382659912 | total_loss: 20.191802978515625\n",
      "Epoch: 327 | recon_loss: 36.980934143066406 | latent_loss: 4.987724781036377 | total_loss: 20.984329223632812\n",
      "Epoch: 328 | recon_loss: 38.889610290527344 | latent_loss: 4.9750776290893555 | total_loss: 21.932344436645508\n",
      "Epoch: 329 | recon_loss: 36.949867248535156 | latent_loss: 4.960343360900879 | total_loss: 20.95510482788086\n",
      "Epoch: 330 | recon_loss: 37.894981384277344 | latent_loss: 4.946659564971924 | total_loss: 21.420820236206055\n",
      "Epoch: 331 | recon_loss: 36.852272033691406 | latent_loss: 4.937875747680664 | total_loss: 20.89507293701172\n",
      "Epoch: 332 | recon_loss: 37.06587219238281 | latent_loss: 4.9203782081604 | total_loss: 20.993125915527344\n",
      "Epoch: 333 | recon_loss: 33.95619583129883 | latent_loss: 4.909173488616943 | total_loss: 19.43268394470215\n",
      "Epoch: 334 | recon_loss: 36.92799377441406 | latent_loss: 4.907851219177246 | total_loss: 20.917922973632812\n",
      "Epoch: 335 | recon_loss: 38.10956954956055 | latent_loss: 4.918166637420654 | total_loss: 21.51386833190918\n",
      "Epoch: 336 | recon_loss: 38.68227767944336 | latent_loss: 4.928155899047852 | total_loss: 21.805217742919922\n",
      "Epoch: 337 | recon_loss: 39.19416427612305 | latent_loss: 4.934137344360352 | total_loss: 22.064151763916016\n",
      "Epoch: 338 | recon_loss: 38.22026062011719 | latent_loss: 4.936648368835449 | total_loss: 21.578454971313477\n",
      "Epoch: 339 | recon_loss: 39.9382209777832 | latent_loss: 4.941714286804199 | total_loss: 22.43996810913086\n",
      "Epoch: 340 | recon_loss: 37.21802520751953 | latent_loss: 4.947015285491943 | total_loss: 21.08251953125\n",
      "Epoch: 341 | recon_loss: 38.59272384643555 | latent_loss: 4.96241569519043 | total_loss: 21.777568817138672\n",
      "Epoch: 342 | recon_loss: 37.43999481201172 | latent_loss: 5.0039753913879395 | total_loss: 21.22198486328125\n",
      "Epoch: 343 | recon_loss: 37.66227722167969 | latent_loss: 4.97864294052124 | total_loss: 21.320459365844727\n",
      "Epoch: 344 | recon_loss: 35.45060729980469 | latent_loss: 4.966057777404785 | total_loss: 20.208332061767578\n",
      "Epoch: 345 | recon_loss: 37.513938903808594 | latent_loss: 4.954709053039551 | total_loss: 21.234323501586914\n",
      "Epoch: 346 | recon_loss: 38.88007354736328 | latent_loss: 4.941344261169434 | total_loss: 21.910709381103516\n",
      "Epoch: 347 | recon_loss: 38.72854995727539 | latent_loss: 4.938028812408447 | total_loss: 21.833290100097656\n",
      "Epoch: 348 | recon_loss: 35.6967658996582 | latent_loss: 4.926123142242432 | total_loss: 20.311445236206055\n",
      "Epoch: 349 | recon_loss: 37.47771453857422 | latent_loss: 4.902184009552002 | total_loss: 21.18994903564453\n",
      "Epoch: 350 | recon_loss: 38.37470626831055 | latent_loss: 4.892685890197754 | total_loss: 21.633695602416992\n",
      "Epoch: 351 | recon_loss: 35.670169830322266 | latent_loss: 4.886528491973877 | total_loss: 20.278348922729492\n",
      "Epoch: 352 | recon_loss: 37.123077392578125 | latent_loss: 4.905346870422363 | total_loss: 21.014211654663086\n",
      "Epoch: 353 | recon_loss: 38.662227630615234 | latent_loss: 4.897542953491211 | total_loss: 21.779884338378906\n",
      "Epoch: 354 | recon_loss: 39.12577819824219 | latent_loss: 4.886283874511719 | total_loss: 22.006031036376953\n",
      "Epoch: 355 | recon_loss: 38.040958404541016 | latent_loss: 4.883771896362305 | total_loss: 21.462364196777344\n",
      "Epoch: 356 | recon_loss: 36.02641296386719 | latent_loss: 4.879332065582275 | total_loss: 20.45287322998047\n",
      "Epoch: 357 | recon_loss: 36.95098876953125 | latent_loss: 4.878433704376221 | total_loss: 20.914710998535156\n",
      "Epoch: 358 | recon_loss: 37.39442443847656 | latent_loss: 4.8919453620910645 | total_loss: 21.143184661865234\n",
      "Epoch: 359 | recon_loss: 37.56891632080078 | latent_loss: 4.889222621917725 | total_loss: 21.229068756103516\n",
      "Epoch: 360 | recon_loss: 38.58210372924805 | latent_loss: 4.883196830749512 | total_loss: 21.732650756835938\n",
      "Epoch: 361 | recon_loss: 40.848533630371094 | latent_loss: 4.881237030029297 | total_loss: 22.864885330200195\n",
      "Epoch: 362 | recon_loss: 41.98637390136719 | latent_loss: 4.903827667236328 | total_loss: 23.445100784301758\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1500\n",
    "\n",
    "# gradually reduce the learning rate\n",
    "steps_before_reduce_learning_rate = 300\n",
    "reduce_factor = 2.0\n",
    "\n",
    "N_TRAIN_BATCHES = y_train.sum() // BATCH_SIZE\n",
    "N_TEST_BATCHES = y_test.sum() // BATCH_SIZE\n",
    "EARLY_STOPPING_ROUNDS = 50\n",
    "\n",
    "losses = pd.DataFrame(columns = ['latent_loss', 'recon_loss', 'loss'])\n",
    "prev_loss, early_stop_round = np.inf, 0\n",
    "\n",
    "model = VAE(1e-3, hidden_size=4, share_hidden=False)\n",
    "\n",
    "for epoch in tqdm(range(n_epochs), total=n_epochs):\n",
    "    # reduce learning rate\n",
    "    if (epoch + 1) >= steps_before_reduce_learning_rate and (epoch + 1) % steps_before_reduce_learning_rate == 0:\n",
    "        model.reduce_learning_rate(reduce_factor)\n",
    "        print('The learning rate now is: {}'.format(model.learning_rate))\n",
    "\n",
    "    # train\n",
    "    for batch, train_x in zip(range(N_TRAIN_BATCHES), train_pos_dataset):\n",
    "        model.train(train_x)\n",
    "        \n",
    "    # test on holdout\n",
    "    loss = []\n",
    "    for batch, test_x in zip(range(N_TEST_BATCHES), test_pos_dataset):\n",
    "        loss.append(model.compute_loss(test_x))\n",
    "    losses.loc[len(losses)] = np.mean(loss, axis=0)\n",
    "    \n",
    "    # early stopping\n",
    "    round_loss = (losses.recon_loss.values[-1] + losses.latent_loss.values[-1]) / 2\n",
    "    if round_loss < prev_loss:\n",
    "        early_stop_round = 0\n",
    "    else:\n",
    "        early_stop_round += 1\n",
    "        if early_stop_round == EARLY_STOPPING_ROUNDS:\n",
    "            break\n",
    "    prev_loss = round_loss\n",
    "    \n",
    "    print(\n",
    "        \"Epoch: {} | recon_loss: {} | latent_loss: {} | total_loss: {}\".format(\n",
    "           epoch, losses.recon_loss.values[-1], losses.latent_loss.values[-1], round_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = tf.Variable(learning_rate, dtype=tf.float32, trainable=False)\n",
    "learning_rate.assign(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x4bcc7630>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss throughout the training\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.plot(losses.index, losses.recon_loss, label='recon_loss')\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(losses.index, losses.latent_loss, label='latent_loss', color='orange')\n",
    "ax.figure.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save_weights('./vae_1_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model perfomance with no lable at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "p_z = ds.MultivariateNormalDiag(loc=[0.] * model.hidden_size, scale_diag=[1.] * model.hidden_size)\n",
    "\n",
    "test_tensor = tf.constant(X_test.values, dtype='float32')\n",
    "# mu, sigma = tf.split(model.encoder(test_tensor), 2, 1)\n",
    "encoded = model.encoder(test_tensor)\n",
    "mu, sigma = model.dense_mean(encoded), model.dense_std(encoded)\n",
    "\n",
    "q_z = ds.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
    "test_pred = ds.kl_divergence(p_z, q_z)\n",
    "# test_pred = tf.sigmoid(test_pred)\n",
    "\n",
    "test_pred = test_pred.numpy()\n",
    "test_pred = test_pred / (test_pred.max() - test_pred.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7206637173549073"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use labels to generate oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to generate 3627 positive samples.\n"
     ]
    }
   ],
   "source": [
    "# number of positive samples to generate so the minority class has the same amount as majority clas\n",
    "n = (y_train == 0).sum() - (y_train==1).sum()\n",
    "print('Need to generate {} positive samples.'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3933, 36])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pos = X_train[y_train==1]\n",
    "\n",
    "_, _, ds_pos = model.encode(X_train_pos.values)\n",
    "sample_pos = ds_pos.sample(sample_shape=(n // y_train.sum()) + 1, seed=1024)\n",
    "sample_pos = tf.reshape(sample_pos, [-1, model.hidden_size])\n",
    "sample_pos = model.decode(sample_pos)\n",
    "sample_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append new samples to original samples\n",
    "X_ = np.r_[X_train.values, sample_pos.numpy()[:n, ...]]\n",
    "y_ = np.append(y_train, [1] * n)\n",
    "\n",
    "# shuffle it \n",
    "from sklearn.utils import shuffle\n",
    "X_, y_ = shuffle(X_, y_, random_state=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9438128612805063\n",
      "Recall: 0.5691489361702128\n",
      "Precision: 0.7588652482269503\n",
      "F1: 0.6504559270516718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_, y_)\n",
    "\n",
    "evaluate(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Environment (conda_dl)",
   "language": "python",
   "name": "conda_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
