{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuzhehao\\AppData\\Local\\Continuum\\anaconda3\\envs\\dl\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n",
      "I1113 16:21:38.630255 61452 file_utils.py:32] TensorFlow version 2.0.0 available.\n",
      "I1113 16:21:39.234316 61452 file_utils.py:39] PyTorch version 1.0.0 available.\n",
      "I1113 16:21:40.394432 61452 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:21:47.195111 61452 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at C:\\Users\\liuzhehao\\.cache\\torch\\transformers\\8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "# try out bert\n",
    "model_class, tokenizer_class, pretrained_weights = \\\n",
    "    TFBertForSequenceClassification, BertTokenizer, 'bert-base-chinese'\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1113 16:21:51.274519 61452 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at C:\\Users\\liuzhehao\\.cache\\torch\\transformers\\8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.0c16faba8be66db3f02805c912e4cf94d3c9cffc1f12fa1a39906f9270f76d33\n",
      "I1113 16:21:51.283520 61452 configuration_utils.py:168] Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I1113 16:21:52.382630 61452 modeling_tf_utils.py:258] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-tf_model.h5 from cache at C:\\Users\\liuzhehao\\.cache\\torch\\transformers\\987cd265ea1aa9cd7e884caf8dd86c2e764e5114ee9a14a67686c1fe05f7a26c.e6b974f59b54219496a89fd32be7afb020374df0976a796e5ccd3a1733d31537.h5\n"
     ]
    }
   ],
   "source": [
    "model = model_class.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/ChnSentiCorp_htl_all/ChnSentiCorp_htl_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for label, text in zip(df['label'], df['review']):\n",
    "    try:\n",
    "        X.append(tokenizer.encode(text, max_length=500))\n",
    "        y.append(label)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "X = tf.ragged.constant(X)\n",
    "X = X.to_tensor()\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.numpy(), y, train_size=0.8, stratify=y, random_state=1024)\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset.batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning\n",
    "from tensorflow import keras\n",
    "\n",
    "for layer in model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "model.layers[-1].trainable = True\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.CategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6212 samples\n",
      "6212/6212 [==============================] - ETA: 8:51:08 - loss: 1.43 - ETA: 7:18:18 - loss: 1.71 - ETA: 6:54:06 - loss: 2.64 - ETA: 6:38:22 - loss: 2.32 - ETA: 6:28:40 - loss: 1.99 - ETA: 6:17:53 - loss: 1.83 - ETA: 6:11:49 - loss: 1.66 - ETA: 6:07:24 - loss: 1.59 - ETA: 6:02:11 - loss: 1.49 - ETA: 5:57:11 - loss: 1.40 - ETA: 5:52:35 - loss: 1.33 - ETA: 5:49:07 - loss: 1.27 - ETA: 5:45:08 - loss: 1.23 - ETA: 5:41:36 - loss: 1.19 - ETA: 5:38:28 - loss: 1.15 - ETA: 5:35:16 - loss: 1.12 - ETA: 5:32:11 - loss: 1.09 - ETA: 5:29:40 - loss: 1.07 - ETA: 5:26:56 - loss: 1.05 - ETA: 5:24:20 - loss: 1.03 - ETA: 5:21:01 - loss: 1.01 - ETA: 5:18:26 - loss: 0.99 - ETA: 5:15:43 - loss: 0.97 - ETA: 5:12:44 - loss: 0.96 - ETA: 5:10:04 - loss: 0.94 - ETA: 5:07:27 - loss: 0.93 - ETA: 5:05:18 - loss: 0.92 - ETA: 5:03:26 - loss: 0.91 - ETA: 5:02:01 - loss: 0.91 - ETA: 5:00:37 - loss: 0.90 - ETA: 4:58:48 - loss: 0.89 - ETA: 4:56:41 - loss: 0.88 - ETA: 4:54:54 - loss: 0.87 - ETA: 4:53:14 - loss: 0.86 - ETA: 4:51:24 - loss: 0.85 - ETA: 4:49:32 - loss: 0.84 - ETA: 4:47:46 - loss: 0.84 - ETA: 4:45:56 - loss: 0.83 - ETA: 4:44:06 - loss: 0.82 - ETA: 4:42:18 - loss: 0.82 - ETA: 4:40:29 - loss: 0.81 - ETA: 4:38:59 - loss: 0.81 - ETA: 4:36:56 - loss: 0.80 - ETA: 4:34:20 - loss: 0.79 - ETA: 4:31:54 - loss: 0.79 - ETA: 4:29:19 - loss: 0.78 - ETA: 4:27:19 - loss: 0.79 - ETA: 4:24:50 - loss: 0.79 - ETA: 4:22:29 - loss: 0.78 - ETA: 4:20:04 - loss: 0.79 - ETA: 4:17:50 - loss: 0.79 - ETA: 4:15:44 - loss: 0.78 - ETA: 4:13:36 - loss: 0.79 - ETA: 4:11:39 - loss: 0.79 - ETA: 4:09:35 - loss: 0.79 - ETA: 4:07:25 - loss: 0.79 - ETA: 4:05:11 - loss: 0.78 - ETA: 4:03:05 - loss: 0.78 - ETA: 4:00:53 - loss: 0.78 - ETA: 3:58:43 - loss: 0.77 - ETA: 3:56:36 - loss: 0.77 - ETA: 3:54:29 - loss: 0.77 - ETA: 3:52:26 - loss: 0.76 - ETA: 3:50:17 - loss: 0.76 - ETA: 3:48:16 - loss: 0.76 - ETA: 3:46:10 - loss: 0.76 - ETA: 3:44:11 - loss: 0.75 - ETA: 3:42:09 - loss: 0.75 - ETA: 3:40:11 - loss: 0.75 - ETA: 3:38:10 - loss: 0.75 - ETA: 3:36:15 - loss: 0.75 - ETA: 3:34:20 - loss: 0.74 - ETA: 3:32:18 - loss: 0.74 - ETA: 3:30:22 - loss: 0.74 - ETA: 3:28:22 - loss: 0.74 - ETA: 3:26:27 - loss: 0.74 - ETA: 3:24:29 - loss: 0.73 - ETA: 3:22:37 - loss: 0.73 - ETA: 3:20:38 - loss: 0.73 - ETA: 3:18:46 - loss: 0.73 - ETA: 3:16:47 - loss: 0.73 - ETA: 3:14:53 - loss: 0.73 - ETA: 3:12:58 - loss: 0.72 - ETA: 3:11:06 - loss: 0.72 - ETA: 3:09:15 - loss: 0.72 - ETA: 3:07:19 - loss: 0.72 - ETA: 3:05:28 - loss: 0.72 - ETA: 3:03:34 - loss: 0.72 - ETA: 3:01:44 - loss: 0.71 - ETA: 2:59:51 - loss: 0.71 - ETA: 2:58:01 - loss: 0.71 - ETA: 2:56:08 - loss: 0.71 - ETA: 2:54:19 - loss: 0.71 - ETA: 2:52:29 - loss: 0.71 - ETA: 2:50:41 - loss: 0.71 - ETA: 2:48:49 - loss: 0.71 - ETA: 2:46:59 - loss: 0.70 - ETA: 2:45:10 - loss: 0.70 - ETA: 2:43:19 - loss: 0.70 - ETA: 2:41:30 - loss: 0.70 - ETA: 2:39:41 - loss: 0.70 - ETA: 2:37:53 - loss: 0.70 - ETA: 2:36:05 - loss: 0.70 - ETA: 2:34:17 - loss: 0.70 - ETA: 2:32:27 - loss: 0.69 - ETA: 2:30:39 - loss: 0.69 - ETA: 2:28:49 - loss: 0.69 - ETA: 2:27:02 - loss: 0.69 - ETA: 2:25:14 - loss: 0.69 - ETA: 2:23:29 - loss: 0.69 - ETA: 2:21:42 - loss: 0.69 - ETA: 2:19:55 - loss: 0.69 - ETA: 2:18:09 - loss: 0.69 - ETA: 2:16:23 - loss: 0.69 - ETA: 2:14:38 - loss: 0.68 - ETA: 2:12:51 - loss: 0.68 - ETA: 2:11:06 - loss: 0.68 - ETA: 2:09:19 - loss: 0.68 - ETA: 2:07:34 - loss: 0.68 - ETA: 2:05:48 - loss: 0.68 - ETA: 2:04:05 - loss: 0.68 - ETA: 2:02:19 - loss: 0.68 - ETA: 2:00:35 - loss: 0.68 - ETA: 1:58:50 - loss: 0.68 - ETA: 1:57:06 - loss: 0.68 - ETA: 1:55:23 - loss: 0.68 - ETA: 1:53:38 - loss: 0.67 - ETA: 1:51:54 - loss: 0.67 - ETA: 1:50:10 - loss: 0.67 - ETA: 1:48:26 - loss: 0.67 - ETA: 1:46:41 - loss: 0.67 - ETA: 1:44:57 - loss: 0.67 - ETA: 1:43:12 - loss: 0.67 - ETA: 1:41:29 - loss: 0.67 - ETA: 1:39:44 - loss: 0.67 - ETA: 1:38:01 - loss: 0.67 - ETA: 1:36:17 - loss: 0.67 - ETA: 1:34:34 - loss: 0.67 - ETA: 1:32:52 - loss: 0.67 - ETA: 1:31:10 - loss: 0.66 - ETA: 1:29:28 - loss: 0.66 - ETA: 1:27:45 - loss: 0.66 - ETA: 1:26:03 - loss: 0.66 - ETA: 1:24:19 - loss: 0.66 - ETA: 1:22:37 - loss: 0.66 - ETA: 1:20:55 - loss: 0.66 - ETA: 1:19:13 - loss: 0.66 - ETA: 1:17:30 - loss: 0.66 - ETA: 1:15:49 - loss: 0.66 - ETA: 1:14:06 - loss: 0.66 - ETA: 1:12:25 - loss: 0.66 - ETA: 1:10:43 - loss: 0.66 - ETA: 1:09:01 - loss: 0.66 - ETA: 1:07:19 - loss: 0.65 - ETA: 1:05:37 - loss: 0.65 - ETA: 1:03:56 - loss: 0.65 - ETA: 1:02:14 - loss: 0.66 - ETA: 1:00:33 - loss: 0.66 - ETA: 58:52 - loss: 0.6599 - ETA: 57:10 - loss: 0.65 - ETA: 55:28 - loss: 0.65 - ETA: 53:47 - loss: 0.65 - ETA: 52:06 - loss: 0.65 - ETA: 50:25 - loss: 0.65 - ETA: 48:43 - loss: 0.65 - ETA: 47:02 - loss: 0.65 - ETA: 45:21 - loss: 0.65 - ETA: 43:40 - loss: 0.65 - ETA: 42:00 - loss: 0.65 - ETA: 40:19 - loss: 0.65 - ETA: 38:38 - loss: 0.65 - ETA: 36:57 - loss: 0.65 - ETA: 35:17 - loss: 0.65 - ETA: 33:36 - loss: 0.65 - ETA: 31:56 - loss: 0.65 - ETA: 30:15 - loss: 0.65 - ETA: 28:35 - loss: 0.65 - ETA: 26:54 - loss: 0.65 - ETA: 25:14 - loss: 0.65 - ETA: 23:34 - loss: 0.65 - ETA: 21:54 - loss: 0.65 - ETA: 20:13 - loss: 0.65 - ETA: 18:33 - loss: 0.65 - ETA: 16:53 - loss: 0.64 - ETA: 15:13 - loss: 0.64 - ETA: 13:32 - loss: 0.64 - ETA: 11:52 - loss: 0.64 - ETA: 10:12 - loss: 0.64 - ETA: 8:32 - loss: 0.6465 - ETA: 6:52 - loss: 0.645 - ETA: 5:12 - loss: 0.645 - ETA: 3:32 - loss: 0.645 - ETA: 1:52 - loss: 0.644 - ETA: 12s - loss: 0.644 - 19404s 3s/sample - loss: 0.6439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x274541d0>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = 1 / (1 + np.exp(pred_test[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7038916309177853"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "roc_auc_score(np.argmin(y_test, axis=1), pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_dl)",
   "language": "python",
   "name": "conda_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
